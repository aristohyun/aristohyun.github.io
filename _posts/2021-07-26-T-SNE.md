---
layout: post
title: "기계학습, T-SNE"
description: "T-SNE, T-분산 확률근접배치"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, T-SNE, T - Distributed Stochastic Neighbor Embedding]
use_math: true
redirect_from:
  - /2021/07/26/
---

* Kramdown table of contents
{:toc .toc}      


[SNE, T-SNE 1](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/){:target = "_ blank"}
[SNE, T-SNE] 2(https://dos-tacos.github.io/paper%20review/TSNE/){:target = "_ blank"}

# 고차원의 시각화

> 많은 기법이 제시되었지만 이는       
> 사람의 해석을 필요로 하며, 2차원 이상의 차원으로 매핑하는 단점이 있었다           
> PCA등의 차원감소 방법은 효과적인 성능을 보이지만 시각화에 한계가 있음              


# SNE (Stochastic Neighbor Embedding)

> 고차원 공간에 존재하는 데이터 $x$의 이웃간의 거리를 최대한 보존하는 저차원의 $y$를 학습하는 방법           
> Stochastic : 거리 정보를 확률적으로 나타내기 때문

> SNE의 목적은 p와 q의 분포 차이가 최대한 작게끔 하고자 함             
> 차원 축소가 제대로 이루어 졌다면, 고차원에서 이웃으로 뽑힐 확률과, 저차원에서 선택될 확률이 비슷하기 때문

@
p_ {j|i} = \frac {\frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}{\sum \limits_ {k \neq i}^{} \frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}
@

@
q_ {j|i} = \frac {e^(-||y_ i - y_ j||^2)}{\sum \limits_ {k \neq i}^{} e^(-||y_ i - y_ k||^2)}
@
[^denominator][^normal]


p는 고차원 공간에 존재하는 $i$번째 개체 $x_ i$가 주어졌을 때, $j$번째 이웃인 $x_ j$가 선택될 확률            
q는 저차원에 임베딩 된 $i$번째 개체 $y_ i$가 주어졌을 때, $j$번째 이웃인 $y_ j$가 선택될 확률         

> $x_ i$와 $x_ j$가 가까이 있으면 분자의 값이 1에 근사하게 될것이고,      
> $x_ i$와 $x_ j$가 멀리 있으면 분자의 지수 값이 굉장히 작아지기에, 분자값은 0에 근사하게 된다


### 목표

> 원래 차원에서 i를 기준으로 j가 선택될 확률과, 저차원에서의 i를 기준으로 j가 선택될 확률이 동일하도록 임베딩 하는 것
> 분포만 일치하면 됨


## 비용 함수

> `Kullback-Leibler divergence`            
> 두 확률분포가 얼마나 비슷한지 측정하는 지표          
> 완전히 다르면 1, 동일하면 0의 값을 가짐        
> 즉 이 코스트 값의 총 합이 최대한 작도록 y[^]를 찾아야 함       

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } 
\end{align\*}
@

#### Gradient[^gradient]

@
\frac {d C}{d y_ i} = 2 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {j|i} - q_ {j|i} + p_ {i|j} - q_ {i|j} )} 
@

SNE 연구진은 계산 속도를 높이기 위해 몇 가지 학습 트릭을 도입.         
$\sigma_ i$는 각 개체마다 데이터 밀도가 달라서 이웃으로 뽑힐 확률이 왜곡되는 현상을 방지하기 위한 값인데,         
반복 실험 결과 p를 계산할 때 쓰는 $\sigma_ i$는 고정된 값을 써도 성능에 큰 차이를 보이지 않았다고 함.     
따라서 $\sigma_ i$ 계산을 생략하게 됨       
또한 i에서 j가 이웃을 뽑힐 확률과 j에서 i가 뽑힐 확률이 동일하다고 생각해도 됨         
따라서          

@
p_ {ij} = \frac {p_ {j|i} + p_ {i|j}}{2} , q_ {ij} = \frac {q_ {j|i} + q_ {i|j}}{2}
@
<br/>
@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } \\\ 
\frac {d C}{d y_ i} &= 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})} 
\end{align\*}
@

 
최종적으로 구하고자 하는 미지수는 저차원에 임베딩된 좌표값 $y_ i$           
SNE는 그래디언트 디센트(gradient descent)[^1] 방식으로 $y_ i$ 들을 업데이트한다.           
즉, 처음에 $y_ i$ 를 랜덤으로 초기화해놓고 위에서 구한 그래디언트의 반대방향으로 조금씩 $y_ i$ 들을 갱신해 나가는 것          


# T-SNE (T - Distributed Stochastic Neighbor Embedding)

> SNE가 전제하는 확률분포는 가우시안 분포        
> 가우시안 분포는 꼬리가 두텁지 않아서 적당히 떨어져있는 데이터와, 아주 멀리 떨어져있는 데이터가 선택될 확률이 크게나지 않음                  
> 가우시안 분포보다 꼬리가 두터운 t-분포[^2]를 쓴 것이 t-SNE                
> $q_ {ij}$에만 t분포를 적용하고, $p_ {ij}$는 SNE와 같음                 

@
q_ {ij} = \frac {(1 + |y_ i - y_ j|^2)^{-1}}{\sum \limits_ {k \neq l}^{}(1 + |y_ k - y_ l|^2)^{-1}}
@

@
p_ {j|i} = \frac {exp(-|x_ i - x_ j|^2 / 2\sigma_ i ^2)}{\sum \limits_ {k \neq i}^{} exp(-|x_ i - x_ k|^2 / 2\sigma_ i ^2)}
@

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } \\\ 
\frac {d C}{d y_ i} &= 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})(1 + |y_ j - y_ i|^2)} 
\end{align\*}
@

## Gradient Interpretation



# Practice

[IRIS : T-SNE](https://www.machinelearningman.com/post/dimensionality-reduction-using-t-sne)

~~~ python
x = np.array(iris[['sepal_length', 'sepal_width','petal_length','petal_width']])
y = np.array(iris['species'])

x_embedded = TSNE(n_components=2, perplexity=30, n_iter=4000).fit_transform(x)

sns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y)
~~~
![image](https://user-images.githubusercontent.com/32366711/127210356-9787adc6-2ed3-4129-b4a7-b60569408a68.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210363-166f76ad-553c-42ac-ac9b-bc2e389b1194.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210374-a8e9820f-41d7-4aa1-8e52-a7a749511d46.png){: width="250"}


# Algorithm

![image](https://user-images.githubusercontent.com/32366711/127319224-e222fe7b-686f-4bde-9cf3-503d80d0fda6.png)


[^1]: 경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.
[^2]: ![image](https://user-images.githubusercontent.com/32366711/127208561-f9974db7-41ec-4916-b02d-8bb9d5eae6aa.png){:width="300"}




#
1차원 기준
1. 각 데아터들을 랜덤한 위치에 배치시킴
2. 티스니는 이 랜덤위치에 있는 데이터들이 클러스터링될떄까지 움직이는 것 
3. 각 데이터들을 어느쪽으로 움직일지 결정하는데, 움직이는 조건은 같은 종류의 데이터끼리 붙으려는 성질이 있으며, 서로다른 종류의 데이터끼리는 멀어지려는 성질이 있다. 이때문에 같은 데이터가 많은쪼긍로 움직인다

1. 모든 데이터에 대해 유사성을 계산한다
2. 이때 유사성이란, A데이터와 다른 데이터간의 거리를 재고, A 데이터를 정규분포의 가운데에 두었을때, 해당거리만큼 떨어진 곳의 값이 다른 데이터들의 유사성이 된다.



sne
두 쌍의 거리가 로컬인 것을 확률적으로 결정하겠다.

하나의 데이터 포인터가 다른 데이터 포인터를 이웃으로 선택할 확률
원래 차원 : High
변환된 저차원 : Low



# Gradient of the Cost Function [^gradient]

[youtube link](https://www.youtube.com/watch?v=INHwh8k4XhM)

![image](https://user-images.githubusercontent.com/32366711/128055067-baa290b5-e4e3-4b53-b233-49fbbcc864c4.png)

![image](https://user-images.githubusercontent.com/32366711/128055701-befbc0ec-e92e-4a9f-bfae-bbcd370dc961.png)  

![image](https://user-images.githubusercontent.com/32366711/128056271-f06a0f36-ec0c-466e-a475-04b494ed1850.png)

![image](https://user-images.githubusercontent.com/32366711/128059265-c958ee28-50de-4362-a7e4-51ef658edbca.png)

![image](https://user-images.githubusercontent.com/32366711/128059315-f083ae2d-e5bf-4b9b-8083-2a6e161cef6d.png)

![image](https://user-images.githubusercontent.com/32366711/128059430-aa8a66f7-5fd1-41ab-a588-56e9d68840c0.png)

![image](https://user-images.githubusercontent.com/32366711/128059499-105103ce-abb0-4d77-9b44-5338ed163d6a.png)

![image](https://user-images.githubusercontent.com/32366711/128059574-352b3051-1ca8-49e5-8ea2-a7214e94657c.png)





[^denominator]: 전체 확률이 1이 되도록 강제하기 위해 분모를 전체 의 합으로 만듬
[^normal]: $\frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}$는 정규 분포 식, 여기에 x값을 대입함으로써 확률 밀도 값을 구할 수 있는데, 이 값을 바탕으로 선택될 확률을 계산하는 것
[^y]: 저차원에서의 좌표 시스템
[^gradient]: 위 그림 참조
