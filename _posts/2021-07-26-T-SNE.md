---
layout: post
title: "기계학습, T-SNE"
description: "T-SNE, T-분산 확률근접배치"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, T-SNE, T - Distributed Stochastic Neighbor Embedding]
use_math: true
redirect_from:
  - /2021/07/26/
---

* Kramdown table of contents
{:toc .toc}      


[SNE, T-SNE 1](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/){:target = "_ blank"}
[SNE, T-SNE 2](https://dos-tacos.github.io/paper%20review/TSNE/){:target = "_ blank"}


# 고차원의 시각화

> 많은 기법이 제시되었지만 이는       
> 사람의 해석을 필요로 하며, 2차원 이상의 차원으로 매핑하는 단점이 있었다           
> PCA등의 차원감소 방법은 효과적인 성능을 보이지만 시각화에 한계가 있음              


# SNE (Stochastic Neighbor Embedding)

> 고차원 공간에 존재하는 데이터 $x$의 이웃간의 거리를 최대한 보존하는 저차원의 $y$를 학습하는 방법           
> Stochastic : 거리 정보를 확률적으로 나타내기 때문

> SNE의 목적은 p와 q의 분포 차이가 최대한 작게끔 하고자 함             
> 차원 축소가 제대로 이루어 졌다면, 고차원에서 이웃으로 뽑힐 확률과, 저차원에서 선택될 확률이 비슷하기 때문

![image](https://user-images.githubusercontent.com/32366711/128156055-b6a2123a-8309-49ac-bfab-40e86a603fdd.png){: width="350"}{: .aligncenter}


@
p_ {j|i} = \frac {\frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}{\sum \limits_ {k \neq i}^{} \frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}
@

@
q_ {j|i} = \frac {e^{(-||y_ i - y_ j||^2)}}{\sum \limits_ {k \neq i}^{} e^({-||y_ i - y_ k||^2)}}
@
[^denominator] [^normal]


p는 고차원 공간에 존재하는 $i$번째 개체 $x_ i$가 주어졌을 때, $j$번째 이웃인 $x_ j$가 선택될 확률            
q는 저차원에 임베딩 된 $i$번째 개체 $y_ i$가 주어졌을 때, $j$번째 이웃인 $y_ j$가 선택될 확률         

> $x_ i$와 $x_ j$가 가까이 있으면 분자의 값이 1에 근사하게 될것이고,      
> $x_ i$와 $x_ j$가 멀리 있으면 분자의 지수 값이 굉장히 작아지기에, 분자값은 0에 근사하게 된다


### 목표

> 원래 차원에서 i를 기준으로 j가 선택될 확률과, 저차원에서의 i를 기준으로 j가 선택될 확률이 동일하도록 임베딩 하는 것
> 분포만 일치하면 됨


## 비용 함수

> `Kullback-Leibler divergence`            
> 두 확률분포가 얼마나 비슷한지 측정하는 지표          
> 완전히 다르면 1, 동일하면 0의 값을 가짐        
> 즉 이 코스트 값의 총 합이 최대한 작도록 y[^]를 찾아야 함       

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } 
\end{align\*}
@

#### Gradient[^gradient]

@
\frac {d C}{d y_ i} = 2 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {j|i} - q_ {j|i} + p_ {i|j} - q_ {i|j} )} 
@

#### Gradient update with momentum term

> Cost 함수를 최소화 할때 y좌표를 업데이트 하는 방법

n : 학습 횟수, $a(t)$ : 변화 

@
Y^{(t+1)} = Y^{(t)} + n \frac{\partial C}{\partial Y} + \alpha (t) (Y^{(t)} - Y^{(t-1)})
@


## Symmetric SNE

> 기존에는 $P_ {i|j}$와 $P_ {j|i}$의 값이 달랐음, 
> Symmetric SNE에서는 서로가 뽑힐 확률 똑같이 만들어 줬기에 계산이 간결해짐
> 기존 SNE의 경우 분산을 각각 계산해주었기에 이상치가 있어도 괜찮았지만,
> Symmetric하게 하기위해 분산을 하나로 통일한 지금은 이상치에 민감하게 반응할 수 있음
> 따라서 p를 다음과 같이 재정의해줌

@
p_ {ij} = \frac {p_ {j|i} + p_ {i|j}}{2n} \;\;\;\;  \sum \limits_ {j}^{} > \frac{1}{2n}
@

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {ij} log \frac {p_ {ij}}{q_ {ij}} }  \\\ 
\frac {d C}{d y_ i} &= 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})} 
\end{align\*}
@

### 문제점, Crowding Problem

> 가우시안 분포가 어느 순간에 경사가 급격하게 감소하기에 생기는 문제점            
> 중심 x로부터 객체들이 멀어질 때, 가까이에 있을 때에는 완만하게 감소를 하는데            
> 적당히 떨어진 부분에 있어서는 급격하게 감소를 해서 결과로 반영을 하기 힘듬              


## t-SNE

> Crowding Problem 문제를 해결하기 위해 개발          
> 정규분포보다 꼬리쪽으로 갈때 경사가 완만한 분포를 원함
> 자유도가 1인 t분포를 채택
> 저차원 공간 상에서만 가우시안 분포대신 t분포를 사용하여 이웃이 뽑힐 확률을 채택

@
p_ {j|i} = \frac {\frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}{\sum \limits_ {k \neq i}^{} \frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2}}
@
@
q_ {i|j} = \frac {(1 + ||y_ i - y_ j||^2)^{-1}}{\sum \limits_ {k \neq l}^{} (1 + ||y_ k - y_ l||^2)^{-1} }
@
@
\frac {d C}{d y_ i} = 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})(1 + ||y_ j - y_ i||^2)^{-1} } 
@[^t-dis]
 
최종적으로 구하고자 하는 미지수는 저차원에 임베딩된 좌표값 $y_ i$           
SNE는 그래디언트 디센트(gradient descent)[^1] 방식으로 $y_ i$ 들을 업데이트한다.           
즉, 처음에 $y_ i$ 를 랜덤으로 초기화해놓고 위에서 구한 그래디언트의 반대방향으로 조금씩 $y_ i$ 들을 갱신해 나가는 것          

![image](https://user-images.githubusercontent.com/32366711/127319224-e222fe7b-686f-4bde-9cf3-503d80d0fda6.png)


# Practice

[IRIS : T-SNE](https://www.machinelearningman.com/post/dimensionality-reduction-using-t-sne)

~~~ python
x = np.array(iris[['sepal_length', 'sepal_width','petal_length','petal_width']])
y = np.array(iris['species'])

x_embedded = TSNE(n_components=2, perplexity=30, n_iter=4000).fit_transform(x)

sns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y)
~~~
![image](https://user-images.githubusercontent.com/32366711/127210356-9787adc6-2ed3-4129-b4a7-b60569408a68.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210363-166f76ad-553c-42ac-ac9b-bc2e389b1194.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210374-a8e9820f-41d7-4aa1-8e52-a7a749511d46.png){: width="250"}



# Gradient of the Cost Function [^gradient]

[youtube link](https://www.youtube.com/watch?v=INHwh8k4XhM)

![image](https://user-images.githubusercontent.com/32366711/128055067-baa290b5-e4e3-4b53-b233-49fbbcc864c4.png)

![image](https://user-images.githubusercontent.com/32366711/128055701-befbc0ec-e92e-4a9f-bfae-bbcd370dc961.png)  

![image](https://user-images.githubusercontent.com/32366711/128056271-f06a0f36-ec0c-466e-a475-04b494ed1850.png)

![image](https://user-images.githubusercontent.com/32366711/128059265-c958ee28-50de-4362-a7e4-51ef658edbca.png)

![image](https://user-images.githubusercontent.com/32366711/128059315-f083ae2d-e5bf-4b9b-8083-2a6e161cef6d.png)

![image](https://user-images.githubusercontent.com/32366711/128059430-aa8a66f7-5fd1-41ab-a588-56e9d68840c0.png)

![image](https://user-images.githubusercontent.com/32366711/128059499-105103ce-abb0-4d77-9b44-5338ed163d6a.png)

![image](https://user-images.githubusercontent.com/32366711/128059574-352b3051-1ca8-49e5-8ea2-a7214e94657c.png)




[^1]: 경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.
[^2]: ![image](https://user-images.githubusercontent.com/32366711/127208561-f9974db7-41ec-4916-b02d-8bb9d5eae6aa.png){:width="300"}
[^denominator]: 전체 확률이 1이 되도록 강제하기 위해 분모를 전체 의 합으로 만듬
[^normal]:  $ \frac {e^{-||x_ i - x_ j||^2}}{2\sigma_ i ^2} $는 정규 분포 식, 여기에 x값을 대입함으로써 확률 밀도 값을 구할 수 있는데, 이 값을 바탕으로 선택될 확률을 계산하는 것
[^y]: 저차원에서의 좌표 시스템
[^gradient]: 위 그림 참조
[^t-dis]: $ f(t) = \frac{\Gamma ( \frac {\nu + 1}{2} )}{\sqrt { \nu \pi } \Gamma(\frac{\nu}{2})}(1 + \frac {t^2}{\nu})^{- \frac {\nu +1}{2}} \\\  \Gamma (n) = (n-1)!$
