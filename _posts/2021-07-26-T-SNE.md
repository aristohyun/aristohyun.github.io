---
layout: post
title: "기계학습, T-SNE"
description: "T-SNE, T-분산 확률근접배치"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, T-SNE, T - Distributed Stochastic Neighbor Embedding]
use_math: true
redirect_from:
  - /2021/07/26/
---

* Kramdown table of contents
{:toc .toc}      


[SNE, T-SNE 1](https://ratsgo.github.io/machine%20learning/2017/04/28/tSNE/){:target = "_ blank"}
[SNE, T-SNE] 2(https://dos-tacos.github.io/paper%20review/TSNE/){:target = "_ blank"}

# 고차원의 시각화

> 많은 기법이 제시되었지만 이는       
> 사람의 해석을 필요로 하며, 2차원 이상의 차원으로 매핑하는 단점이 있었다           
> PCA등의 차원감소 방법은 효과적인 성능을 보이지만 시각화에 한계가 있음              


# SNE (Stochastic Neighbor Embedding)

> 고차원 공간에 존재하는 데이터 $x$의 이웃간의 거리를 최대한 보존하는 저차원의 $y$를 학습하는 방법           
> Stochastic : 거리 정보를 확률적으로 나타내기 때문

@
p_ {j|i} = \frac {exp(-|x_ i - x_ j|^2 / 2\sigma_ i ^2)}{\sum \limits_ {k \neq i}^{} exp(-|x_ i - x_ k|^2 / 2\sigma_ i ^2)}
@

@
q_ {j|i} = \frac {exp(-|y_ i - y_ j|^2)}{\sum \limits_ {k \neq i}^{} exp(-|y_ i - y_ k|^2)}
@

p는 고차원 공간에 존재하는 $i$번째 개체 $x_ i$가 주어졌을 때, $j$번째 이웃인 $x_ j$가 선택될 확률            
q는 저차원에 임베딩 된 $i$번째 개체 $y_ i$가 주어졌을 때, $j$번째 이웃인 $y_ j$가 선택될 확률             

> SNE의 목적은 p와 q의 분포 차이가 최대한 작게끔 하고자 함             
> 차원 축소가 제대로 이루어 졌다면, 고차원에서 이웃으로 뽑힐 확률과, 저차원에서 선택될 확률이 비슷하기 때문


## 비용 함수

> `Kullback-Leibler divergence`            
> 두 확률분포가 얼마나 비슷한지 측정하는 지표          
> 완전히 다르면 1, 동일하면 0의 값을 가짐        

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } 
\end{align\*}
@

SNE 연구진은 계산 속도를 높이기 위해 몇 가지 학습 트릭을 도입.         
$\sigma_ i$는 각 개체마다 데이터 밀도가 달라서 이웃으로 뽑힐 확률이 왜곡되는 현상을 방지하기 위한 값인데,         
반복 실험 결과 p를 계산할 때 쓰는 $\sigma_ i$는 고정된 값을 써도 성능에 큰 차이를 보이지 않았다고 함.     
따라서 $\sigma_ i$ 계산을 생략하게 됨       
또한 i에서 j가 이웃을 뽑힐 확률과 j에서 i가 뽑힐 확률이 동일하다고 생각해도 됨         
따라서          

@
p_ {ij} = \frac {p_ {j|i} + p_ {i|j}}{2} , q_ {ij} = \frac {q_ {j|i} + q_ {i|j}}{2}
@
<br/>
@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } \\\ 
\frac {d C}{d y_ i} &= 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})} 
\end{align\*}
@

 
최종적으로 구하고자 하는 미지수는 저차원에 임베딩된 좌표값 $y_ i$           
SNE는 그래디언트 디센트(gradient descent)[^1] 방식으로 $y_ i$ 들을 업데이트한다.           
즉, 처음에 $y_ i$ 를 랜덤으로 초기화해놓고 위에서 구한 그래디언트의 반대방향으로 조금씩 $y_ i$ 들을 갱신해 나가는 것          
$y_ i$ 의 그래디언트를 보면 우리가 이미 모두 알고 있는 값들이므로 업데이트를 여러번 반복 수행하기만 하면 됨        


# T-SNE (T - Distributed Stochastic Neighbor Embedding)

> SNE가 전제하는 확률분포는 가우시안 분포        
> 그러나 가우시안 분포는 꼬리가 두텁지 않아서 적당히 떨어져있는 데이터와, 아주 멀리 떨어져있는 데이터가 선택될 확률이 크게나지 않음            
> => Crowding Problem             
> 가우시안 분포보다 꼬리가 두터운 t-분포[^2]를 쓴 것이 t-SNE                
> $q_ {ij}$에만 t분포를 적용하고, $p_ {ij}$는 SNE와 같음                 



@
q_ {ij} = \frac {(1 + |y_ i - y_ j|^2)^{-1}}{\sum \limits_ {k \neq l}^{}(1 + |y_ k - y_ l|^2)^{-1}}
@

@
p_ {j|i} = \frac {exp(-|x_ i - x_ j|^2 / 2\sigma_ i ^2)}{\sum \limits_ {k \neq i}^{} exp(-|x_ i - x_ k|^2 / 2\sigma_ i ^2)}
@

@
\begin{align\*}
Cost &= \sum {KL(P_ i || Q_ i)} \\\ 
&= \sum \limits_ {i}^{} \sum \limits_ {j}^{} {p_ {j|i} log \frac {p_ {j|i}}{q_ {j|i}} } \\\ 
\frac {d C}{d y_ i} &= 4 \sum \limits_ {j}^{} {(y_ j - y_ i)(p_ {ij} - q_ {ij})(1 + |y_ j - y_ i|^2)} 
\end{align\*}
@

# Practice

[IRIS : T-SNE](https://www.machinelearningman.com/post/dimensionality-reduction-using-t-sne)

~~~ python
x = np.array(iris[['sepal_length', 'sepal_width','petal_length','petal_width']])
y = np.array(iris['species'])

x_embedded = TSNE(n_components=2, perplexity=30, n_iter=4000).fit_transform(x)

sns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y)
~~~
![image](https://user-images.githubusercontent.com/32366711/127210356-9787adc6-2ed3-4129-b4a7-b60569408a68.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210363-166f76ad-553c-42ac-ac9b-bc2e389b1194.png){: width="250"}
![image](https://user-images.githubusercontent.com/32366711/127210374-a8e9820f-41d7-4aa1-8e52-a7a749511d46.png){: width="250"}


[^1]: 경사 하강법(傾斜下降法, Gradient descent)은 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.
[^2]: ![image](https://user-images.githubusercontent.com/32366711/127208561-f9974db7-41ec-4916-b02d-8bb9d5eae6aa.png){:width="300"}

