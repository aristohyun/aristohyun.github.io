---
layout: post
title: "기계학습, Hierachy Clustering"
description: "Hierachy Clustering, 계층적 군집 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, Hierachy, Clustering]
use_math: true
redirect_from:
  - /2021/07/25/
---

* Kramdown table of contents
{:toc .toc}      


# Hierachy Clustering 
   
> 군집간의 거리(유사도)를 기반으로 계층을 나누어 클러스터링을 하는 알고리즘이며, K Means와는 다르게 군집의 수를 미리 정해주지 않아도 됨            
> 클러스터링 결과 는 일반적으로 덴드로그램 으로 표시한다
ex ) 종 -> 속 -> 강 -> 목 -> 과 -> 문 -> 계 순으로 묶어가며 하나의 그룹으로 만들어가는 과정

## Agglomerative, 병합

> Bottom-Up 방식      
> 비슷한 군집끼리 묶어 가면서 최종 적으로는 하나의 케이스가 될때까지 군집을 묶는 클러스터링 알고리즘           

## Divisive, 분할

> Top-Down 방식      
> 일단 하나의 군집으로 묶은 후, 유사하지 않은 군집을 분할해가는 클러스터링 알고리즘
         

## 거리의 기준

### 비계층적 거리 측정법

> 모든 경우에 사용할 수 있는 거리 측정 방법


군집 - 군집 간에 거리는?

- 요소간 거리중 최소거리를 기준으로 ( Single Linkage )     

@ d(u,v) = min(d(u_ i, v_ j) @

u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산해 가장 짧은 것으로 선택

- 요소간 거리중 최대거리를 기준으로 ( Complete Linkage )       

@ d(u,v) = max(d(u_ i, v_ j) @

u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산해 가장 긴 것으로 선택
  
- 요소간 거리의 평균 거리를 기준으로 ( Average Linkage )      

@ d(u,v) = \frac {\sum \limits_ {i, j}^{} {d(u_ i, v_ j}} {n_ u * n_ v} @

u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산하여 평균을 계산

- 군집의 중심간 거리를 기준으로 ( Centroid )    

@ d(u,v) = || c_ u - c_ v || @
@ c_ u = \frac {1}{n} \sum \limits_ {i=0}^{n} u_ i @


### 계층적 거리 측정법

> 계층적 군집화에서만 사용할 수 있는 방법으로, 이미 전에 계층적으로 합쳐진 적이 있다고 가정하며            
> 이전에 사용했던 정보를 다시 사용하는 방법           
> 따라서 계산량이 앞선 방법들 보다 적어 효율적        

@ u \leftarrow s + t @
군집 u가 군집 s와 t가 합쳐져서 생겼다면 다음의 방식들로 거리를 계산할 수 있다

- 중앙값 거리 (median)

군집 u의 중심 $c_ u$은 이전의 두 군집의 중심이라고 볼 수 있음

@ c_ u  = \frac{1}{2}(c_ s + c_ t)@ 

따라서 다시 처음부터 모든 데이터로부터 중심을 계산하지 않고,       
위 방법으로 중심을 계산해 비계층적 거리 측정법의 Centroid 방식을 사용할 수 있다           

- 가중 거리 (weighted)

@ d(u,v) = \frac {1}{2} (d(s,v) + d(t,v)) @

군집 u와 v간의 거리는 군집 u를 이루는 군집 s, t와 군집 v간의 거리의 평균이라고 볼 수 있다

- 와드 거리 (Ward)

위 가중 거리방법의 변형

@ d(u,v) = \sqrt{ \frac{n_ v + n_ s}{n_ v + n_ s + n_ t}d(v,s)^2 + \frac{n_ v + n_ t}{n_ v + n_ s + n_ t}d(v,t)^2 - \frac{n_ v}{n_ v + n_ s + n_ t}d(s,t)^2 } @


![image](https://user-images.githubusercontent.com/32366711/127048450-77bfa663-2564-4bf4-9a4b-6e20c14286c4.png){: width="300"}

![image](https://user-images.githubusercontent.com/32366711/127049209-fdf8f831-ee43-416f-a379-5fb4293cc7c2.png){: width="300"}

![image](https://user-images.githubusercontent.com/32366711/127049460-bfd48ee9-d2a0-4f81-ac3e-17ac0a199917.png){: width="300"}

![image](https://user-images.githubusercontent.com/32366711/127049676-236578ea-9998-444f-a65e-2bc072a686be.png){: width="300"}

![image](https://user-images.githubusercontent.com/32366711/126897534-0e5a7f15-cbf2-453d-9ed3-97e227bc903d.png)

![image](https://user-images.githubusercontent.com/32366711/126901230-ccf0c013-de20-4e42-a026-02d425e18ac5.png){: width="350"}{: .aligncenter}

## Dendrogram

> 계층으로 나눈 라벨들을 각각의 거리로 어떻게 나누어졌는지 보기 편하게 그린 플롯


# Practice

[Hierarchical Clustering in Python by George Pipis](https://medium.com/swlh/hierarchical-clustering-in-python-9646cfddee35) 

~~~ python

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn import datasets

iris = datasets.load_iris()
df=pd.DataFrame(iris['data'])

np.unique(iris.target,return_counts=True)

# Import the whiten function
from scipy.cluster.vq import whiten
scaled_data = whiten(df.to_numpy())

~~~

~~~ python

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(scaled_data, method = 'ward', metric = 'euclidean')

# Import the dendrogram function
from scipy.cluster.hierarchy import dendrogram

# Create a dendrogram
dn = dendrogram(distance_matrix)

# Display the dendogram
plt.show()

~~~

![image](https://user-images.githubusercontent.com/32366711/126991253-4001ad11-a4a1-4d3b-8890-8eef7e9c5063.png)

![image](https://user-images.githubusercontent.com/32366711/126991677-0998ef5a-5756-46f9-b3c4-c8f4cc279192.png)


~~~ python
# Assign cluster labels
df['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')

df['target'] = iris.target
fig, axes = plt.subplots(1, 2, figsize=(16,8))
axes[0].scatter(df[0], df[1], c=df['target'])
axes[1].scatter(df[0], df[1], c=df['cluster_labels'], cmap=plt.cm.Set1)
axes[0].set_title('Actual', fontsize=18)
axes[1].set_title('Hierarchical', fontsize=18)

axes[0].set_xlabel("sepal length")
axes[0].set_ylabel("sepal width")

axes[1].set_xlabel("sepal length")
axes[1].set_ylabel("sepal width")
~~~

![image](https://user-images.githubusercontent.com/32366711/127045519-b1e93b31-ed48-41bc-bb74-8c7acad26395.png)
