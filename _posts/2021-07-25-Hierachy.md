---
layout: post
title: "기계학습, Hierachy Clustering"
description: "Hierachy Clustering, 계층적 군집 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, Hierachy, Clustering]
use_math: true
redirect_from:
  - /2021/07/25/
---

* Kramdown table of contents
{:toc .toc}      

[1](https://ratsgo.github.io/machine%20learning/2017/04/18/HC/){:target="_ blank"} 
[2](https://towardsdatascience.com/hierarchical-clustering-agglomerative-and-divisive-explained-342e6b20d710){:target="_ blank"} 
[3](https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn){:target="_ blank"} 
[4](https://kr.mathworks.com/help/stats/dendrogram.html) 
[5](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py) 
[6](https://medium.com/swlh/hierarchical-clustering-in-python-9646cfddee35) 


# Hierachy Clustering 
   
> 군집간의 거리(유사도)를 기반으로 계층을 나누어 클러스터링을 하는 알고리즘이며, K Means와는 다르게 군집의 수를 미리 정해주지 않아도 됨            
> 클러스터링 결과 는 일반적으로 덴드로그램 으로 표시한다
ex ) 종 -> 속 -> 강 -> 목 -> 과 -> 문 -> 계 순으로 묶어가며 하나의 그룹으로 만들어가는 과정

## Agglomerative, 병합

> Bottom-Up 방식      
> 비슷한 군집끼리 묶어 가면서 최종 적으로는 하나의 케이스가 될때까지 군집을 묶는 클러스터링 알고리즘           

![image](https://user-images.githubusercontent.com/32366711/126897534-0e5a7f15-cbf2-453d-9ed3-97e227bc903d.png)

![image](https://user-images.githubusercontent.com/32366711/126901230-ccf0c013-de20-4e42-a026-02d425e18ac5.png){: width="350"}{: .aligncenter}


## Divisive, 분할

> Top-Down 방식      
> 일단 하나의 군집으로 묶은 후, 유사하지 않은 군집을 분할해가는 클러스터링 알고리즘


## 거리의 기준

군집 - 군집 간에 거리는?

1. 요소간 거리중 최소거리를 기준으로 ( Single Linkage )         
2. 요소간 거리중 최대거리를 기준으로 ( Complete Linkage )          
3. 요소간 거리의 평균 거리를 기준으로 ( Average Linkage )               
4. 군집의 중심간 거리를 기준으로 ( Centroid )             

## Dendrogram

> 계층으로 나눈 라벨들을 각각의 거리로 어떻게 나누어졌는지 보기 편하게 그린 플롯


# Practice

~~~ python

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn import datasets

iris = datasets.load_iris()
df=pd.DataFrame(iris['data'])

np.unique(iris.target,return_counts=True)

# Import the whiten function
from scipy.cluster.vq import whiten
scaled_data = whiten(df.to_numpy())

~~~

~~~ python

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(scaled_data, method = 'ward', metric = 'euclidean')

# Import the dendrogram function
from scipy.cluster.hierarchy import dendrogram

# Create a dendrogram
dn = dendrogram(distance_matrix)

# Display the dendogram
plt.show()

~~~

![image](https://user-images.githubusercontent.com/32366711/126991253-4001ad11-a4a1-4d3b-8890-8eef7e9c5063.png)

![image](https://user-images.githubusercontent.com/32366711/126991677-0998ef5a-5756-46f9-b3c4-c8f4cc279192.png)


~~~ python
# Assign cluster labels
df['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')

df['target'] = iris.target
fig, axes = plt.subplots(1, 2, figsize=(16,8))
axes[0].scatter(df[0], df[1], c=df['target'])
axes[1].scatter(df[0], df[1], c=df['cluster_labels'], cmap=plt.cm.Set1)
axes[0].set_title('Actual', fontsize=18)
axes[1].set_title('Hierarchical', fontsize=18)

axes[0].set_xlabel("sepal length")
axes[0].set_ylabel("sepal width")

axes[1].set_xlabel("sepal length")
axes[1].set_ylabel("sepal width")
~~~

![image](https://user-images.githubusercontent.com/32366711/126991312-b66faf03-2417-44ee-882f-c5ac9c62c814.png)
