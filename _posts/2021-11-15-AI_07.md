---
layout: post
title: "AI, 7장 계획수립"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/11/15/
---

* Kramdown table of contents
{:toc .toc}  

# 계획수립, Planning

> 주어진 계획수립 무제의 임의의 초기 상태에서 목표 상태 중의 하나로 도달할 수 있게 하는 일련의 행동을 생성하는 것

#### 로봇의 계획수립

- **움직임 계획수립(motion planning)**     
  - 원하는 움직임 작업을 수행하도록 제약조건을 만족시키면서 최소의 비용으로 일련의 움직임을 찾아내는 일
- **경로 계획수립(path planning)**                  
  - 시작 위치에서 목적 위치로 가기 위해 관절이나 바퀴를 이동시킬 순차적인 위치를 결정하는 일             
- **궤적 계획수립(trajectory planning)**              
  - 주어진 경로와 제약조건 및 물리적인 특성을 고려하여 매 시점의 관절등의 위치, 속도, 가속도 등을 결정하는 일

## 계획 수립 문제

> 초기 상태에 대한 명세 
> +원하는 목표 상태에 대한 명세 
> +가능한 행동들에 대한 명세

### 문제 형태에 영향을 주는 요소

- 행동의 결과가 결정적인가 비결정적인가
- 상태 변수는 이산적인가 연속적인가
- 현재 상태를 알 수 있는가. 정확히 알 수 있는가 아니면 간접적인 정보만 알 수 있는가
- 초기 상태의 개수가 얼마나 되는가
- 행동은 지속시간이 있는가
- 여러 개의 행동을 <blue>동시</blue>에 할 수 있는가, 아니면 <blue>한 번에 하나</blue>의 행동만 하는가
- 계획의 목적이 지정된 <blue>목표 상태에 도달</blue>하는 것인가, 아니면 <blue>보상을 최대</blue>로 하는 것인가
- 에이전트가 하나인가 여러 개 있는가
- 에이전트들이 서로 협력하는가 이기적인가
- 에이전트 각자가 자신의 계획을 만드는가, 아니면 전체 에이전트들을 위해 하나의 계획을 만드는가

### 고전적 계획수립 문제, classical planning

> 가장 간단한 계획 수립 문제 부류

- 기본 전제
  - 초기 상태는 하나만 주어진다
  - 행동들에 지속시간이 없고, 
  - 행동의 결과가 결정적이며, 
  - 한번에 하나의 행동만 수행할 수 있다
  - 행동을 하는 <red>에이전트</red>는 <red>하나</red>뿐이다
- 일련의 행동들을 수행한 이후의 세계의 상태는 예측가능하다
- 계획은 일련의 행동들로 정의된다
  - 목표상태에 도달하기 위해 어떤 행동을 해야 하는지 미리 결정할 수 있다


### 마르코프 결정과정 문제, MDP[^MDP]

> Markov Decision Process       
> 행동들의 결과는 비결정적이고,    
> 에이전트가 행동을 통제할 수 있는 문제   

다음 상태는 현재상태와 에이전트의 행동에만 영향을 받으므로
과거 상테에는 영향을 받지 않는다. 각 상태는 확률적으로 독립적이다

`강화학습에서 관심을 갖는 문제`

- 행동들은 지속시간이 없다
- 행동의 결과가 확률에 따라 결정되어 비결정적이다
- 행동의 결과는 관측가능하며, 학인할 수 있다
- 보상함수를 최대화하는 것을 목적으로 한다
- 행동을 하는 에이전트는 하나뿐이다

[^MDP]: 이산시간 마르코프 결정과정 문제, discrete time stochastic control process

#### 예, 빠르게 멀리 가는 방법 찾기

![image](https://user-images.githubusercontent.com/32366711/142654380-e5480dcc-41da-4b7b-af6c-17b68f952dd9.png)


### 부분관측 마르코프 결정과정, POMDP

> partially observable Markov decision process    
> 행동의 결과가 확률에 따라 결정되는 비결정적인 마르코프 결정과정     

`행동의 결과가 부분적(간접적)으로 관측된다`

- 현재 상태를 정확히 알 수 없고 확률적인 분포로만 추정가능하다
- 현재 상태에 대한 확률적 분포를 믿음, belief라고 함
- 행동을 하며 이 belief를 갱신함

#### 예, 호랑이 문제

![image](https://user-images.githubusercontent.com/32366711/142654874-cdef98d3-2dec-4375-b99a-793d13ec9f79.png)


### 다중 에이전트 계획 수립, multi-agent

> 여러 에이전트가 있는 계획수립 문제

- 하나의 <red>공동 목표</red>를 위한 에이전트들이 <red>계획수립</red>을 하는 것
- 작업 및 자원에 대한 협상을 통해 <red>계획을 정제</red>하는 것
- 목표 달성을 위해 에이전트들의 <red>작업을 조정</red>하는 것

## 계획 수립기, planner

> 주어진 문제에 대한 계획을 생성하는 알고리즘 또는 프로그램

### 특정 영역 계획수립기(domain-specific planner)

> 해당 영역에 특화된 계획수립 방법      
> 다른 영역에 적용 불가    

- 로봇의 경로 계획 및 움직임 계획
- 통신망의 통신채널 계획
- 생산현장의 기구 조작 등

### 영역 독립 계획수립기(domain-independent planner) 

> 범용 획수립기           
> 영역에 상관없이 적용 가능

- 특정영역 계획수립기에 비해 처리속도가 느림
- 효과적인 계획수립기 개발이 어려움

상태공간 계획수립과 계획공간 계획수립으로 나누어 개발/사용

### 설정가능 계획수립기(configurable planner)

> 영역 독립 계획수립기를 사용하면서,       
> 해당 영역의 문제를 해결하는 방법에 관한 정보를 입력으로 사용

- 특정영역 계획수립기 보다는 계획수립의 범용성이 높으며 
- 영역독립 계획수립기 보다는 처리 효율성이 좋음

ex. 계층적 태스크 네트워크(HTN) 계획수립

## 계획수립 언어

> 계획수립 문제를 표현하는데 사용하는 언어

#### 고전적 계획수립 문제를 표현하는 언어

- STRIPS, PDDL 등
- 리터럴로 표현되는 상태변수가 중심
- 에이전트를 포함한 세계의 상태는 상태 변수에 값을 지정하여 표현
- 행동에 대한 표현은 행동전후 상태변수 값의 변화내용을 기술함
- 상태변수들이 상태공간을 결정
  - 상태변수들의 개수가 늘어나면, 상태공간의 크기는 기하급수적으로 증가한다

### STRIPS

> Stanford Research Institute Problem Solver[^STRIPS]      
> 자동 계획생성기의 이름        
> 계획수립 문제를 표현하는 언어의 이름            

상태와 행동을 표현하기위해 <red>술어논리</red> 사용

- 상태 : 긍정 리터럴의 논리곱으로 표현
  - $At(Home) \wedge Have(Banana)$

- 목표 : 리터럴들의 논리곱으로 표현
  - $At(Home) \wedge \neg Have(Banana)$
  - $At(x) \wedge Sells(x, Banana)$

- 행동 : 이름, 매개변수 목록, 사전조건, 효과로 구성
  - 이름 : 어떤 일을 하는 것인지 기술
  - 매개변수 목록 : 사전조건과 효과에 값을 전달하는 변수들
  - 사전조건 : 행동을 실행하기 전에 만족되어야 하는 조건기술
  - 효과 : 행동의 실행 후에 생기는 상태변화를 나타내는 것
    - 긍정 리터럴 : 행동 실행으로 새로 생기는 성질 표현 add-list
    - 부정 리터럴 : 행동 실행으로 더 이상 만족되지 않는 성질 표현 delete-list

#### 예, 행동 : 상자 위에 올라간다(Climb Up)

- 사전조건 : 대상의 위치와 상자의 위치가 같고 높이는 아래쪽이다
- 효과 : 높이가 아래쪽에서 위쪽으로 바뀐다

Action : ClimbUp(location)

Precondition : At(location), BoxAt(location), level(Low)

Effect : 

- add-list: Level(High)
- delete-list Level(Low)

### PDDL

> Planning Domain Definition Language   
> 고전적 계획수립 문제의 표현방법을 표준화하기 위해 개발[^PDDL]
> 국제 계획수립 대회의 표준언어(IPC)로 사용, 계속 진화

- 문제 영역 세계에 있는 객체(object)
- 객체의 성질에 대한 술어(predicate)
- 초기 상태
- 목표 상태
- 행동

계획 수립 문제를 두 개의 파일에 나누어 저장
- domain 파일 : 문제 영역 정의에 사용
  - 술어, 행동에 대한 정보 저장
- problem 파일 : 문제 정의에 사용
  - 객체, 초기 상태, 목표 저장

[^STRIPS]: 미국 SRI International의 Richard Fikes와 Nils Nilsson이 개발
[^PDDL]: Drew McDermoot 등 개발

# 계획 수립 방법

## 고전적 계획수립 방법

#### 상태공간 계획수립

> 초기 상태를 목표상태로 변화시키는 일련의 행동(연산자) 찾기

상태공간 : 문제의 세상에서 나타나는 상태들의 집합

- 전향탐색
- 후향탐새
- STRIPS 알고리즘
- GraphPlan 알고리즘

#### 계획공간 계획수립

> 부분 계획(불완전한 계약)작업안의 제약조건을 부여하는 방법으로 계획이 완전한 계획이 되는 방법을 찾음

계획공간 : 모든 가능한 계획들의 집합

#### 계층적 계획수립

> 작업 방법에 대한 지식을 추상화 수준의 계층적인 구조로 기술

추상적 계획에서 구체적인 계획으로 구성

- HTN(Hierarchical tast network) 알고리즘

## 상태공간 계획수립

> 상태공간(state space) 상의 초기 상태에서 목표 상태로의 경로 탐색

- 노드(node) : 세계(world)의 상태
- 에지(edge) : 상태 전이(transition)를 일으키는 행동(action) 
  - 행동 = 바로 적용할 수 있는 기본 행동(primitive action)
  - 연산자(operator) = 기본 행동

![image](https://user-images.githubusercontent.com/32366711/142787391-9d3aa3f9-8e71-4c7e-ba51-d085c2d47a18.png)

### 전향 탐색

- 초기 상태에서 시작
- 적용가능한 연산자를 목표 상태에 도달할 때까지 적용
- 다양한 알고리즘 적용 가능
  - 너비우선 탐색(Breadth-first search)
  - 깊이우선 탐색(Depth-first search)
  - 휴리스틱 탐색 : A* 알고리즘


### 후향 탐색

- 목표 상태에서 시작
- 해당 상태를 만들어내는 행동 선택을 시작 상태에 도달할 때까지 반복

### STRIPS

> 목표상태가 만족되지 않으면, 목표상태를 effect로 만들 수 있는 연산자를 선택하여        
> 
> 연산자의 매개변수를 설정하고 precondition이 만족되는지 확인                

- precondition 중에 만족되지 않은 것이 있다면, 그것을 effect로 하여 위 과정 반복
- precondition들이 만족되면 사용된 매개변수가 설정된 연산자들을 역순으로 나열하여 계획 생성

기본적으로 후향 탐색 방법

목표상태와 초기상태를 비교하고, 일치하지 않는 조건을 만들어줄 수 있는 연산자를 취해주며, 
해당 연산자의 precondition중 일치하지 않는 것 을 계속 반복해서 행동을 

#### 예

<img width="170" alt="image" src="https://user-images.githubusercontent.com/32366711/142787853-26352519-07b1-458a-a30a-b78f1531f7d6.png">

세계
- 2개의 방 : R1, R2
- 방 사이의 출입문 : D
- 로봇 I, 방 R1에 위치
- 블록 B, 방 R2에 위치

목표
- 로봇 I와 블록 B가 방 R2에 함께 있도록 하는 것

술어
- InRoom(x, r) : 물건 x가 방 r에 있다.
- NextTo(x, t) : 물건 x가 방 또는 물건 t 옆에 있다. 
- Status(d, s) : 출입문 d가 상태 s (Open 또는 Closed)이다. 
- Connects(d, rx, ry) : 방 rx와 방 ry가 출입문 d로 연결되어 있다

Operator: GoToDoor(I, dr)            
Precondition: InRoom(I, ra), Connects(dr, ra, rb)                
Effect:               
  add-list: NextTo(I, dr)               
  delete-list:               
  
Operator: GoThruDoor(I, dr)                 
precondition: Connects(dr, ra, rb), NextTo(I, dr), Status(dr, Open)                 
Effect:                
  add-list: InRoom(I, rb)                                   
  delete-list: InRoom(I, ra)                 
  
~~~ python

  # NextTo가 아직 안만들어져있음, 이걸 만드는 연산자(행동)은 GoToDoor
  rb = R2
  Connects(dr, ra, R2), NextTo(I, dr), Status(dr, Open)
  Connects(D, R1, R2)
  dr = D, ra = R1
  NextTo(I, D), Status(D, Open) 
  
  # 이제 초기상태가 모든 프리컨디션을 만족함
  dr = D
  InRoom(I, ra), Connects(D, ra, rb)
  ra = R1, rb = R2
  InRoom(I, R1), Connects(D, R1, R2)
  
~~~
  
### GraphPlan 알고리즘

> 계획수립 그래프(planning graph)를 사용하여 탐색공간 표현            
> 후향 탐색시 초기 상태에 도달할 수 있는 없는 행동의 시도를 축소           

기존 계획공간 계획수립에 비해 매우 빠른 속도

변수가 없는 연산자로 구성된 STRIPS 문제 해결

변수에 가능한 모든 객체를 바인딩함. 가능한 바인딩 조합만큼 변수가 없는 연산자가 생성됨

- 장점: 변수를 사용하지 않아 매칭연산이 용이함
- 단점: 연산자의 개수가 많이 늘어남


#### 알고리즘

`명제 - 행동 반복`

명제 단계 0
- 초기 상태에 주어지는 각 리터럴을 노드로 표현

행동 단계 1
- 명제 단계 0의 명제들에 대해서 적용될 수 있는 각 행동을 노드로 표현
- 명제 단계 0에 나타나는 Precondition의 리터럴 노드와 연결
- 명제 단계 2에 나타나는 Effect의 리터럴 노드와 연

명제 단계 2
- 행동 단계 1의 행동의 Effects 부분에 나타나는 리터럴들과 명제 단계 0의 리터럴들에 대응하는 노드 생성
- 명제 단계 0의 모든 노드를 명제 단계 2에 유지

행동1, 명제2 반복

#### 계획수립 그래프의 확장

- 상호배제 링크
  - 행동 단계와 명제 단계 한 쌍의 추가시,
  - 동시에 실행되거나 만족될 수 없는 동일 단계의 노드들 사이 연결

행동 단계의 상호배제
- 상충되는 결과 도출
  - 한 행동의 effect가 다른 행동의 effect가 만드는 명제를 제거하는 경우
- 간섭
  - 한 행동이 다른 행동의 precondition에서 사용되는 명제를 제거하는 경우
- 경쟁관계 사전조건
  - 두 행동이 직전 단계에서 상호배제 관계에 있는 명제들을 precondition에서 사용하는 경우

명제 단계의 상호배제
- 상충되는 지지
  - 대응되는 두 명제를 만들어내는 이전 단계의 모든 행동이 서로 상호배제 관계에 있는 경우

#### 예 : 생일저녁 준비 문제

생일 날 집에서 자고 있는 누군가를 위해 생일 저녁을 준비하는 일을 계획

목적 : 생일 저녁을 위해 주방의 쓰레기를 치우고, 선물과 저녁 식사 준비

명제 기호
- grab : 쓰레기가 주방에 있다
- dinner : 저녁식사가 준비되어 있다
- present : 선물이 준비되어 있다
- clean : 손이 깨끗하다
- quiet : 조용하다

초기 상태
- 주방에 쓰레기가 있고, 손은 깨끗하며, 주변은 조용하다
- $garb \wedge clean \wedge quiet$

목표 상태
- 주방에 쓰레기가 없고, 저녁 식사와 선물이 준비된 상태
- $ \neg garb \wedge dinner \wedge present $

행동
- cook : 요리하기
- wrap : 선물 포장하기
- carry : 손으로 치우기
- dolly : 손수레로 치우기


Action : cook
  Precondition : clean
  Effect : dinner
  
Action : wrap
  Precondition : quiet
  Effect : present
  
Action : carry
  Precondition : garb
  Effect : $\neg \text{garb} \wedge \neg \text{clean}$
  
Action : dolly
  Precondition : garb
  Eddect : $\neg \text{garb} \wedge \neg \text{quiet}$

<img width="600" alt="image" src="https://user-images.githubusercontent.com/32366711/145713160-b92e3bec-9263-4179-8c96-62257b94b5cc.png">

<img width="600" alt="image" src="https://user-images.githubusercontent.com/32366711/145713171-e7cf98a2-60c7-4cd0-8a91-69bd57b96c50.png">


## 계획공간 계획수립

- 탐색 공간이 부분계획(partial plan)들로 구성
- 부분계획의 구성요소
  - 부분적으로 값이 결정된 행동(partially instantiated actions)의 집합
  - 제약조건(constraints)의 집합
    - 선행 제약 조건
    - 바인딩 제약 조건
    - 인과연결
- 해답 계획이 완성될 때까지 점진적으로 개선

#### 위협

- 삭제 조건 상호작용
- 행동 𝑎가 행동 𝑏의 사전조건 𝑝를 생성하는 인과연결(causal link) 관계
- 행동 𝑐가 𝑝를 삭제하는 Effects 보유
- 𝑐가 인과관계 $ a \overset{p}{\rightarrow} b $ 를 위협하는 상황



## 계층적 계획수립

> 복잡한 태스크를 더 단순한 태스크로 분할

### 계층적 태스크 네트워크, HTN

> 태스크(task)가 목표로 주어질 때,     
> 이 태스크를 추상적 단계에서 분할하여 점차 구체적인 기본 작업들로 구성하여 계획을 수립하는 방법

주어지는 정보
- 기본 태스크(primitive task) 
  - 연산자(행동)에 의해 수행
- 복합 태스크(non-primitive task) 
  - 메소드(method)를 사용하여 더 작은 부분태스크(subtask)들로 분할하여 표현
  – 복합 태스크는 여러가지 방법으로 분할 가능
  – 부분태스크 수행에 제약조건(constraint) 존재 가능

#### HTN 계획 수립

주어진 복합 태스크에 대해 조건에 맞는 메소드를 찾아 적용, 기본 태스크로 표현될 때까지 분할

- 계획구성
  - 기본 태스크를 수행하는 연산자를 기본 태스크에 부여된 순서관계에 따라 순차적으로 나열
-계획실행
  - 초기 상태에서 순차적으로 연산자를 적용하면 최종적으로 주어진 태스크가 완료되는 목표 상태에 도달
