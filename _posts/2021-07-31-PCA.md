---
layout: post
title: "기계학습, PCA"
description: "Principle Component Analysis, 주성분 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, PCA, Principle Component Analysis]
use_math: true
redirect_from:
  - /2021/07/31/
---

* Kramdown table of contents
{:toc .toc}      


[iris 차원 감소](https://makeit.tistory.com/157)

# PCA, PRINCIPLE COMPONENT ANALYSIS, 주성분 분석

> 차원 감소 기법            
> PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법                
> 분산이 가장 큰 축을 첫 번째 주성분, 두 번째로 큰 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환 함

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동시킨다
2. 원점을 지나며, 데이터들에 가장 적합한 직선들을 구한다
3. 각 직선들 중에서 분산이 가장 큰 축 2개에 데이터들을 정사영시켜 새로운 좌표점을 구하여 차원을 축소시킨다


## 가장 적합한 직선

> 가장 적합한 직선은, 데이터를 이 직선에 정사영 시켰을 때 각 데이터부터 직선까지의 거리가 가장 짧거나 원점부터 투영된 점들의 거리(SS, sum of squared ditances)를 최대화 하는 직선               
이렇게 SS가 최대가 되는 직선을 찾고, 이 직선을 PC1, 주성분1이라고 부른다              
![image](https://user-images.githubusercontent.com/32366711/127775240-5d02d51e-757a-4a0f-829c-927adf16bcc3.png)

PC2는 원점을 지나며, PC1과 수직으로 교차하는 직선이다       
마지막으로 PCA 플롯을 그리기 위해 전체적으로 PC1이 수평이 되도록 회전시킨다         
PC1과 PC2가 새로운 X, Y축이 되며, 각각에 투영된 점이 기존 데이터의 새로운 X, Y좌표가 된다             
![image](https://user-images.githubusercontent.com/32366711/127775500-318f9eba-69e5-4161-bd08-a6cf83e49f71.png)

 
## 분산도      

![image](https://user-images.githubusercontent.com/32366711/127775550-8f7acb8c-9ad0-4e09-b44c-9ad7c099af6c.png)

3차원 이상에서

각 차원에 대해 주성분을 계산하여 PC1 PC2 PC3 ... 등을 계산한 후에
분산도를 구해 어떤 선분이 얼마나 차지하는지를 구하여 해당 데이터만 사용하여 해당 차원을 근사하여 보여줄 수 있다

만일 PC3 이하의 분산도와 PC1+PC2의 차이가 크지 않다면, PC1 PC2를 사용하는 것이 데이터를 정확하게 대표하지는 않는다는 뜻


# 특이값 분해, SVD

SVD, 특이값 분해와 같이 할 경우, 단위벡터를 구한다

원점에서부터 직선에 투영시킨 점까지의 거리의 제곱합을 PC1의 고유벡터라고 부르며, 루트를 취한 값을 특이값이라고 한다


# 행렬로써의 PCA

공분산 행렬을 이용해 교유벡터에다가 정사영을 시켜준다는 것

고유벡터를 구해서 정사영 시킬때 분산이 가장 커지며 적합한 직선, PC1이 됨

# 공분산 행렬을 통한 변환(정사영)

첫번째 직선, PC1은



