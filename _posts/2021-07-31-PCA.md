---
layout: post
title: "기계학습, PCA"
description: "Principle Component Analysis, 주성분 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, PCA, Principle Component Analysis]
use_math: true
redirect_from:
  - /2021/07/31/
---

* Kramdown table of contents
{:toc .toc}      


[iris 차원 감소](https://makeit.tistory.com/157)

# 차원 감소 기법

> 차원이 커지면            
> 1. 데이터를 시각적으로 표현하기 어려움,             
> 2. 데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 되므로, 이러한 데이터를 이용해 머신러닝 알고리즘을 학습 하게되면 모델이 복잡해지게 된다. 따라서, 오버피팅 (overfitting) 위험이 커짐


# PCA, PRINCIPLE COMPONENT ANALYSIS, 주성분 분석

> 차원 감소 기법            
> PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법                
> 분산이 가장 큰 축을 첫 번째 주성분, 두 번째로 큰 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환 함


## 기본 개념

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동시킨다
2. 원점을 지나며, 데이터들에 가장 적합한 직선[^1]들을 구한다
3. 각 직선들 중에서 분산이 가장 큰 축 2개에 데이터들을 정사영[^0]시켜 새로운 좌표점을 구하여 차원을 축소시킨다


### 구체적 방법

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동
2. 공변수 행렬 계산
3. 공변수 행렬로부터 고유값과 고유벡터를 계산
4. 고유값을 순서대로 정렬하고 그에 해당되는 고유벡터를 나열
5. 정렬된 고유벡터를 토대로 기존 변수를 변환


## 차원 축소      

1. 고유값 감소율이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 수를 선택
2. 일중 수준 이상의 분산비(보통 70% 이상)를 보존하는 최소의 주성분을 선택

시각화 할떄는 3차원 이하(주성분 3개 이하)로 선택

3차원 이상에서 각 차원에 대해 주성분을 계산하여 PC1 PC2 PC3 ... 등을 계산한 후에
분산도를 구해 어떤 선분이 얼마나 차지하는지를 구하여 해당 데이터만 사용하여 해당 차원을 근사하여 보여줄 수 있다

만일 PC3 이하의 분산도와 PC1, PC2의 차이가 크지 않다면, PC1 PC2를 사용하는 것이 데이터를 정확하게 대표하지는 않는다는 뜻


# 수식

$ x_ 1, x_ 2, x_ 3 , ... , x_ n $ 과 같은 평균이 0으로 맞춰진 데이터들이 있을 때,    
크기가 1인 단위벡터 $v$로 투영 시키면 다음과 같이 쓸 수 있다

$ v^T x_ 1, v^T x_ 2, v^T x_ 3, ... , v^T x_ n $

이 투영된 데이터들의 분산을 구했을때, 분산이 가장 큰 단위 벡터를 찾아야 한다

$
\begin{align\*}
\sigma ^2 &= \frac{1}{N} \sum \limits _ {i=1}^{N} (v^T x_ i - 0)^2  \;\;\; (\because \bar x = 0) \\\ 
 &= \frac{1}{N} \sum \limits _ {i=1}^{N} (v^T x_ i)(v^T x_ i)^ T \\\ 
 &= \frac{1}{N} \sum \limits _ {i=1}^{N} v^T x_ i x_ i ^T v \\\ 
 &= v^T ( \frac{1}{N} \sum \limits _ {i=1}^{N} x_ i x_ i ^ T)v
\end{align\*}
$

이때 다음을 상관계수행렬이라고 부른다

$
\begin{align\*}
C &= \frac{1}{N} \sum \limits _ {i=1}^{N} x_ i x_ i ^ T \\\ 
 &= \frac{1}{N} X^ T X
\end{align\*}
$

이때 첫번째 주성분 벡터 PC1은 위에서 구한 분산이 최대가 될때의 벡터이므로, 아래의 최적화 문제를 푸는 것으로 구할 수 있다. 라그랑주 승수법 이용[^1]

$
\max \limits_ {v} \; v^T C v   \\\ 
v^T v = 1
$
$
L =  v^T C v - \lambda (v^T v - 1) \\\ 
\begin{align\*}
 \frac{\partial L}{\partial \vec{v}} = 2 C \vec{v} - 2\lambda \vec{v} &= 0 \\\ 
C \vec{v} - \lambda \vec{v} &= 0
\end{align\*}
$

## 특징

공분산 행렬의 고유벡터를 사용하므로 단일 가우시안 분포로 추정할 수 있는 데이터에 대해 서로 독립적인 축을 찾는데 사용할 수 있음


### 한계점

데이터의 분포가 가우시안이 아니거나 다중 가우시안 자료들에 대해서는 적용하기가 어려움
- 대안 : 커널 PCA, LLE

분류, 예측 문제에 대해서 데이터의 범주 정보를 고려하지 않기 때문에 범주간 구분이 잘 되도록 변환해 주는 것은 아님
PCA는 단순히 변환돈 축이 최대 분산방향되 정렬되도록 좌표회전을 수행하는 것 뿐


[^0]: 사영이란, $\vec b$를 $\vec a$에 수직인 점까지의 길이를 가지며, $\vec a$와 같은 방향을 갖는 벡터를 찾는다는 것을 의미한다. $(\vec b - p \vec a)^T \vec a = 0 \\\ \vec b ^ T \vec a - p \vec a ^ T \vec a = 0 \\\ p = \frac {\vec b ^ T \vec a}{\vec a ^ T \vec a} = p = \vec b ^ T \vec a \\\ \vec x = (\vec b ^ T \vec a)\vec a$
[^1]: 라그랑주 승수법이란, 제한 조건이 있는 최적화 문제에 사용할 수 있으며, L= objective function + λ(constraint) 의 식을 가진다. 이때 X, Y, λ에 대해 각각 편미분을 하여 X, Y에 대해 풀면 제한조건을 만족하는 최적화된 X, Y를 구할 수 있다
