---
layout: post
title: "기계학습, PCA"
description: "Principle Component Analysis, 주성분 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, PCA, Principle Component Analysis]
use_math: true
redirect_from:
  - /2021/07/31/
---

* Kramdown table of contents
{:toc .toc}      


[iris 차원 감소](https://makeit.tistory.com/157)

# 차원 감소 기법

> 차원이 커지면            
> 1. 데이터를 시각적으로 표현하기 어려움,             
> 2. 데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 되므로, 이러한 데이터를 이용해 머신러닝 알고리즘을 학습 하게되면 모델이 복잡해지게 된다. 따라서, 오버피팅 (overfitting) 위험이 커짐


# PCA, PRINCIPLE COMPONENT ANALYSIS, 주성분 분석

> 차원 감소 기법            
> PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법                
> 분산이 가장 큰 축을 첫 번째 주성분, 두 번째로 큰 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환 함


## 기본 개념

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동시킨다
2. 원점을 지나며, 데이터들에 가장 적합한 직선[^1]들을 구한다
3. 각 직선들 중에서 분산이 가장 큰 축 2개에 데이터들을 정사영[^0]시켜 새로운 좌표점을 구하여 차원을 축소시킨다

### 구체적 방법

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동
2. 공변수 행렬 계산
3. 공변수 행렬로부터 고유값과 고유벡터를 계산
4. 고유값을 순서대로 정렬하고 그에 해당되는 고유벡터를 나열
5. 정렬된 고유벡터를 토대로 기존 변수를 변환

$ \lambda(1) > \lambda(2) > \lambda(3) ... $       
$ e(1) > e(2) > e(3) ... $          

$ Z_ 1 = e(1)\mathbf{X} = e_ {11}X_ 1 + e_ {12}X_ 2 + ...  $        
$ Z_ 2 = e(2)\mathbf{X} = e_ {21}X_ 1 + e_ {22}X_ 2 + ...  $        
$ Z_ 3 = e(3)\mathbf{X} = e_ {31}X_ 1 + e_ {32}X_ 2 + ...  $              
$ ... $


## 행렬

| 관측치 \ 변수 | $ X_ 1 $ | ... | $ X_ p $ |
|:-------------:|:--------:|:-------:|:-------------:|
|   $N_ 1$      |   $ x_ {11} $  | ... |   $ x_ {1p}   | 
| ...           |  ...  | ... |  ...    | 
| $N_ n$        |  $ x_ {n1}  | ... |   $ x_ {np}  | 

$ 
\bar X = 
\begin{bmatrix}
\bar x_ 1\\ 
... \\ 
\bar x_ p
\end{bmatrix}
$[^2]$
C_ n = \begin{bmatrix}
s_ {11} & \cdots & s_ {1p} \\ 
\cdots & \ddots  & \cdots\\ 
s_ {p1} & \cdots & s_ {pp}
\end{bmatrix}
$[^3]$
R = \begin{bmatrix}
1 & r_ {12} & \cdots & r_ {1p} \\ 
r_ {21} & 1 & \cdots & r_ {2p} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
r_ {p1} & r_ {p2} & \cdots & 1
\end{bmatrix}
$[^4]


## 차원 축소      

1. 고유값 감소율이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 수를 선택
2. 일중 수준 이상의 분산비(보통 70% 이상)를 보존하는 최소의 주성분을 선택

시각화 할떄는 3차원 이하(주성분 3개 이하)로 선택

![image](https://user-images.githubusercontent.com/32366711/127775550-8f7acb8c-9ad0-4e09-b44c-9ad7c099af6c.png)

3차원 이상에서 각 차원에 대해 주성분을 계산하여 PC1 PC2 PC3 ... 등을 계산한 후에
분산도를 구해 어떤 선분이 얼마나 차지하는지를 구하여 해당 데이터만 사용하여 해당 차원을 근사하여 보여줄 수 있다

만일 PC3 이하의 분산도와 PC1, PC2의 차이가 크지 않다면, PC1 PC2를 사용하는 것이 데이터를 정확하게 대표하지는 않는다는 뜻


# 수식



### 공분산, Covariance    

> 2개의 특성(또는 변수)간의 상관정도를 나타낸 값

@
\begin{align*}
cov(X, Y) &= E[(X-\mu])(Y-v)]\\\ 
 &= E[XY - v X - \mu Y + \mu v]] \\\ 
 &= E[XY] - vE[X] - \mu E[Y] + \mu v \\\ 
 &= E[XY] - \mu v - \mu v + \mu v \\\  
 &= E[XY] - \mu v \\\ 
\end{align*}

\begin{align*}
E[XY] - \mu v &= \frac {1}{m-1} \sum \limits_ {i=1}^{m}X_ i Y_ i - \mu v  \\\ 
 &= \frac {1}{m-1} X^ T Y - \mu v
\end{align*}

@

벡터에 행렬을 곱하면 선형변환
고유벡터는 이 선형변환을 해도 방향이 변하지 않는 벡터


## 특징

공분산 행렬의 고유벡터를 사용하므로 단일 가우시안 분포로 추정할 수 있는 데이터에 대해 서로 독립적인 축을 찾는데 사용할 수 있음


### 한계점

데이터의 분포가 가우시안이 아니거나 다중 가우시안 자료들에 대해서는 적용하기가 어려움
- 대안 : 커널 PCA, LLE

분류, 예측 문제에 대해서 데이터의 범주 정보를 고려하지 않기 때문에 범주간 구분이 잘 되도록 변환해 주는 것은 아님
PCA는 단순히 변환돈 축이 최대 분산방향되 정렬되도록 좌표회전을 수행하는 것 뿐


[^0]: 사영이란, $\vec b$를 $\vec a$에 수직인 점까지의 길이를 가지며, $\vec a$와 같은 방향을 갖는 벡터를 찾는다는 것을 의미한다. $(\vec b - p \vec a)^T \vec a = 0 \\\ \vec b ^ T \vec a - p \vec a ^ T \vec a = 0 \\\ p = \frac {\vec b ^ T \vec a}{\vec a ^ T \vec a} = p = \vec b ^ T \vec a \\\ \vec x = (\vec b ^ T \vec a)\vec a$
[^1]: 가장 적합한 직선은, 데이터를 이 직선에 정사영 시켰을 때 각 데이터부터 직선까지의 거리가 가장 짧거나 원점부터 투영된 점들의 거리(SS, sum of squared ditances)를 최대화 하는 직선     [^2]: Mean vector, 평균 벡터, 각 변수의 관측치들의 평균
[^3]: Covariance Matrix, 공분산 행렬, 대각 원소에는 원소 p개에 대한 분산, 비대각 원소에는 두 원소간의 공분산
[^4]: Correlation Matrix, 상관계수 행렬, 대각원소는 자기자신에 대한 상관관계이기 떄문에 1

