---
layout: post
title: "기계학습, PCA"
description: "Principal Component Analysis, 주성분 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, PCA, Principal Component Analysis]
use_math: true
redirect_from:
  - /2021/07/31/
---

* Kramdown table of contents
{:toc .toc}      


[iris 차원 감소](https://makeit.tistory.com/157)

# 차원 감소 기법

> 차원이 커지면            
> 1. 데이터를 시각적으로 표현하기 어려움,             
> 2. 데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 되므로, 이러한 데이터를 이용해 머신러닝 알고리즘을 학습 하게되면 모델이 복잡해지게 된다. 따라서, 오버피팅 (overfitting) 위험이 커짐


# PCA, 주성분 분석

> 차원 감소 기법            
> PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법                
> 분산이 가장 큰 축을 첫 번째 주성분, 두 번째로 큰 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환 함


## 기본 개념

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동시킨다
2. 원점을 지나며, 데이터들에 가장 적합한 직선들을 구한다
3. 각 직선들 중에서 분산이 가장 큰 축 2개에 데이터들을 정사영시켜 새로운 좌표점을 구하여 차원을 축소시킨다

![image](https://user-images.githubusercontent.com/32366711/128001766-5884c1c5-dfce-450e-a49d-f623f2ab060e.png)

![image](https://user-images.githubusercontent.com/32366711/128001819-0fc7cbe0-3222-4967-8b53-825d7aee5648.png)

![image](https://user-images.githubusercontent.com/32366711/128001949-6ac15ca5-772b-4a3d-b47e-27e08f936951.png)

![image](https://user-images.githubusercontent.com/32366711/128002663-c53ca1e7-e725-48e8-afb0-f904deb79866.png)

![image](https://user-images.githubusercontent.com/32366711/128003190-b9c27216-f872-4423-bb6b-d1687e9c3a36.png)


### 구체적 방법

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동
2. 데이터의 공변수 행렬 계산
3. 공변수 행렬로부터 고유값(주성분의 분산값)과 고유벡터(새로운 축, 주성분)를 계산
4. 고유값을 순서대로 정렬하고 그에 해당되는 고유벡터를 나열
5. 정렬된 고유벡터를 토대로 기존 변수를 변환


## 선형 결합

$ Z_ p = \alpha_ p ^ T X = \alpha_ {p1} X_ 1 +  \alpha_ {p2} X_ 2 + \cdots + \alpha_ {pp} X_ p $

- $ X_ 1, X_ 2, \cdots, X_ p$ : 원래 변수
- $ \alpha = [\alpha_ {i1}, \alpha_ {i2}, \cdots, \alpha_ {ip}] $ : i 번째 기저(계수)
- $Z_ 1, Z_ 2, \cdots, Z_ p$: 각 기저로 사영 변환 후의 변수(주성분, PC)


### 라그랑주 승수법

> 기본 가정: "제약 조건 g를 만족하는 f의 최솟값 또는 최댓값은 f와 g가 접하는 지점에 존재할 수도 있다          
> 또한 어떠한 함수 f의 최솟값 또는 최댓값은 극점에 존재할 수도 있으며, 다변수 함수의 극점은 전미분 df=0인 지점 중에 존재한다"      

PCA는 주성분 Z의 분산이 최대가 되어야 한다. 이때 Z는 $\alpha ^T X$ 과 같으므로 

$ Var(Z) = Var(\alpha ^T X) = \alpha ^T Var(X) \alpha = \alpha ^T C \alpha $[^3]  (C는 공분산 행렬)

즉 $ \alpha ^T C \alpha $를 최대화 하는 $\alpha$를 찾아야 한다 (이때 $\alpha$의 길이는 1이라는 제한조건이 있음. 단위 벡터이기 때문)

위의 식을 라그랑주 승수법을 도입한 방정식을 세우면

$ 
\begin{align\*} 
 L &= f(x) - \lambda g(x) \\\ 
   &= \alpha ^T C \alpha - \lambda( \alpha^ T \alpha - 1 ) \\\ 
 \frac{\partial L}{\partial \alpha} &= C \alpha - \lambda \alpha = 0 \\\ 
 &= (C - \lambda I) \alpha = 0 
\end{align\*} 
$ 
[^1]

## 고유값, 고유벡터

라그랑주 승수법에 의해           
$ AX = \lambda X \Rightarrow  (A-\lambda I)X = 0 \\\ 
\therefore det|A-\lambda I|=0
$[^2]        
어떤 정방행렬 A(여기서는 공분산 행렬 C)에 대해 다음을 만족하는 $\lambda$를 고유값(eigen value)이라고 하며,     
이때의 X를 고유벡터(eigen vector)라고 한다(여기서는 $\alpha$)

PCA에서 이 공분산 행렬의 고유값은 각 주성분의 분산과 같은 의미를 가지며, 고유벡터는 주성분의 방향(새로운 축의 방향)이 된다


## 차원 감소

여기까지의 방법은 기존의 데이터를 고유벡터에 투영시킨 것이며, 아직 차원의 수는 그대로이다

차원의 수를 줄이기 위해서는 구한 주성분중에서 몇개를 선택할것인지 정해야하는데

정하는 방법은

1. 주성분의 분산(고유값)이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 수를 선택하는 것이며
2. 일정 수준(보통 70% 이상)의 분산비를 보존하는 최소의 주성분을 선택하는 것이다



[^1]: $\frac { \partial (X^T A X) }{\partial X} = X^T(A + A^T)$, $ \frac{\partial L}{\partial \alpha} = \alpha ^T(C + C^T) - \lambda (2 \alpha^T) = 2 \alpha^T C - 2\lambda \alpha^T = 0, (\alpha^T C - \lambda \alpha^T) ^T = C \alpha - \lambda \alpha = 0, (C-\lambda I) \alpha = 0 $
[^2]: 1. Ax = 0을 만족하는 솔루션 x의 집합이 존재하는데, 이것을 nullspace라고 부른다. 2. Ax = 0일때만 위의 식을 만족한다면, A는 역행렬이 존재한다.(invertible). 만약 Ax = 0 을 만족하는 해가 무수히 많다면, A는 비가역 행렬이다. 즉, 역행렬이 존재하지 않는다. 
[^3]: $ Var(Z) = Var(\alpha ^T X) = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)^2 = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)(\alpha ^T x_ i)^T = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)(\alpha ^T x_ i)^T = \frac{1}{n} \sum \limits_ {i=1}^{n} \alpha ^T(x_ i)(x_ i ^T) \alpha  = \alpha ^T Var(X) \alpha$
