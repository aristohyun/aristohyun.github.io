---
layout: post
title: "ML, 7장 앙상블 학습"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/04/07
---

* Kramdown table of contents
{:toc .toc} 

# 앙상블 학습

> 일련의 예측 모델들을 생성하여 모델을 학습

wisdom of crowd, 무작위로 선택된 많은 사람의 답을 모으면 전문가의 답보다 낫다

## 붓스트랩 bootstrap

> 주어진 학습 데이터 집합에서 복원추출하여 다수의 학습 데이터 집합을 만들어내는 기법

- 배깅 : 한번에 여러개 만들고 비교
- 부스팅 : 계속해서 모델을 만들어가며 비교

# 배깅 알고리즘

- 붓스트랩을 통해 여러 개의 학습 데이터 집합 생성       
- 각 학습 데이터 집합별로 분류기, 회귀모델 생성
- 분류기들의 투표나 가중치 투표 및 회귀 모델들의 평균을 내어 최종적으로 값을 예측함
- 회귀도 마찬가지로 가능

## 랜덤 포레스트

> 분류기로 결정트리를 사용하는 배깅 기법

- 무작위로 선택한 속성 중에서 분할 속성을 선택하여
- 여러 결정트리를 만든다

# 부스팅 알고리즘

> K개의 예측 모델을 순차적으로 만들어 가는 앙상블 모델 생성

- 오차에 따라 학습 데이터에 가중치나 값을 변경해가며 예측 모델을 생성한다

## AdaBoost

초기 가중치는 1/N

학습 오류값이 0.5 미만인 분류기가 학습된다면,


- 분류기 신뢰도 $\alpha = 0.5 ln(\frac{1-\epsilon}{\epsilon})$[^epsilon]
- 잘못 판정한 학습 데이터의 가중치는 증대
    - $w_ i \leftarrow w_ i e^a$
- 제대로 판정한 학습 데이터의 가중치는 축소
    - $w_ i \leftarrow w_ i e^{-a}$
- 가중치의 합이 1이 되도록 정규화

과정을 거침

[^epsilon]: 학습 오류값, $\epsilon$

## Gradient Boosting

> 앙상블에 이전까지의 오차를 보정하도록 결정트리 모델을 순차적으로 추가

왜 그레디언트인가

평균 + 학습률 * 모델값 + 학습률 * 모델 ... 100개정도 

트리 앙상블 수랑, 단말 노드의 수는 하이퍼파라미터로 직접 줌

부류에서는 로그오드르 계산한다

잔차가 마이너스값이 나오기 떄문에, 마이너스는 확률이 되지않고 결정트리 단말노드는 0보다 작을수 없기 때문에
회귀에서는 그냥 사용했지만, 분류에서는 로그오드값을 사용한다

## XGBoost

> Gradient Boosting 기법을 개선한 방식

- 분류, 회귀 문제 지원
- 결손값이 있는 데이터 지원
- 대규모 데이터 처리를 위해 분산 환경에서도 실행 가능
- 모델 복잡도 및 과적합 조정을 위한 하이퍼파라미터 포함
- Kaggle의 대부분의 문제 중, 상위 성능의 모델에서 사용되었음
- 다양한 클라우드 서비스 제공

## LightGBM

- 빠른 학습과 예측 시간
- 더 적은 메모리 사용
- 범주형 특징의 자도 변환과 최적 분할