---
layout: post
title: "ML, 3장 비지도 학습, 반지도 학습"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/14
---

* Kramdown table of contents
{:toc .toc} 


# 비지도 학습

> 결과정보가 없는 데이터들에 대해서 특정 패턴을 찾는 것

- 데이터에 잠재한 구조(structure), 계층구조(hierarchy) 를 찾아내는 것
- 숨겨진 사용자 집단(hidden user group)을 찾는 것
- 상대적 거리를 유지하면서 데이터를 낮은 차원으로 표현하는 것
- 문서들을 주제에 따라 구조화하는 것
- 로그(log) 정보를 사용하여 사용패턴(usage pattern)을 찾아내는 것

#### 비지도 학습의 대상

- 군집화(clustering)
- 밀도추정(density estimation)
- 차원축소(dimensionality reduction) 
- 이상치 탐지 (outlier detection) 
- 연관규칙 마이닝 (association rule mining) 
- 토픽 모델링(topic modeling)

# 군집화

> 데이터를 유사한 것들끼리 모우는 것           
> 군집 내의 유사도(similarity)는 크게, 군집 간의 유사도는 작게

- 일반 군집화(hard clustering)
    - 데이터는 하나의 군집에만 소속
    - 예. k-means 알고리즘
- 퍼지 군집화(fuzzy clustering) 
    - 데이터가 여러 군집에 부분적으로 소속
    - 소속정도의 합은 1이 됨
    - 예. 퍼지 k-means 알고리즘

`군집내 분산과 군집간의 거리가 작을수록 좋은 성능`

## 군집간 거리 계산 방법

그룹 r과 s의 거리 $L(r,s)$ 는

- 최단(단순) 연결  
    - $min(D(x_ {ri}, x_ {sj}))$
- 최장(완전) 연결 
    - $max(D(x_ {ri}, x_ {sj}))$
- 평균 연결 
    - $\frac{1}{n_ r n_ s} \sum\limits_ {i=1}^{n_ r} \sum\limits_ {j=1}^{n_ s}  D(x_ {ri}, x_ {sj})$
- 중심 연결 
    - $D(\frac{1}{n_ r} \sum\limits_ {i=1}^{n_ r} x_ {ri},\frac{1}{n_ s} \sum\limits_ {i=1}^{n_ s} x_ {sj})$
- ward 연결
    - 모든 클러스터 내의 분산을 가장 작게 증가시키는 두 클러스터를 합친다. 따라서 비교적 크기가 비슷한 클러스터 끼리 뭉친다
    - 오차 제곱합을 기준으로 수행


## 계층적 군집화 (hierarchical clustering)

> 군집화의 결과가 군집들이 계층적인 구조를 갖도록 하는 것

- 병합형(agglomerative) 계층적 군집화
- 분리형(divisive) 계층적 군집화
    - 모든 데이터를 포함한 군집에서 시작하여 유사성을 바탕으로 군집을 분리하여 점차 계층적인 구조를 갖도록 구성

## 분할 군집화 (partitioning clustering)

> 계층적 구조를 만들지 않고 전체 데이터를 유사한 것들끼리 나누어서
묶는 것

- 예. k-means 알고리즘, DBSCAN 알고리즘

### K-means 알고리즘

1. 군집의 중심 위치 무작위로 선정
2. 군집 중심을 기준으로 군집 재구성
3. 군집별 평균 위치 결정
4. 군집 평균 위치로 군집 중심 조정
5. 수렴할 때까지 2-4 과정 반복

#### 엘보우 방법

> 군집 개수 k에 따른 군집화 결과에 대한 군집 내 SSE(Sum of Squared Errors, 왜곡)를 그래프로 그려서    
> 가장 크게 꺽이는 위치의 k를 선택하는 방법

<img width="400" alt="image" src="https://user-images.githubusercontent.com/32366711/163704909-f3bf0b13-0ce0-415b-bedf-837328497e42.png">


### DBSCAN 알고리즘

> 일정 밀도 이상으로 연결된 영역을 찾아 가면서 군집화

임의의 데이터 p에서 시작하여 

Eps(반경) 내에 MinPts(최소 이웃의 수)를 갖는 데이터는 연결된 것으로 간주하여 군집 구성

# 밀도 추정

> 부류(class)별 데이터를 만들어 냈을 것으로 추정되는 확률분포을 찾는 것

- 각 부류 별로 주어진 데이터를 발생시키는 확률 계산
- 가장 확률이 높은 부류로 분류

## 모수적 밀도 추정

> 분포가 특정 수학적 함수의 형태를 가지고 있다고 가정

- 주어진 데이터를 가장 잘 반영하도록 함수의 파라미터 결정
- 전형적인 형태 : 가우시안(Gaussian) 함수 또는 여러 개의 가우시안 함수의 혼합(Mixture of Gaussian)

## 비모수적 밀도 추정

>분포에 대한 특정 함수를 가정하지 않고, 주어진 데이터를 사용하여 밀도함수의 형태 표현

- 전형적인 형태 : 히스토그램(histogram)


# 차원축소

> 고차원의 데이터를 정보의 손실을 최소화하면서 저차원으로 변환하는 것

- 고차원의 데이터를 2, 3차원으로 변환하여, 직관적으로 분석할 수 있음
- 차원의 저주 문제[^curse_of_dimensionality] 완화 가능


[^curse_of_dimensionality]: 차원이 커질수록 거리 분포가 일정해지는 경향이 있다. 또한 차원이 증가함에 따라 부분공간의 개수가 기하급수적으로 증가한다.

## 주성분 분석, PCA

Principle Component Analysis

> 분산이 큰 몇개의 축들을 기준으로 데이터를 투영(projection)하여
저차원으로 변환
$
\frac{x * r}{|| v ||}
$

데이터를 사영 시킬 때, 분산이 가장 커지는 벡터가 주성분

데이터의 공분산행렬(covariance matrix)에 대한 고유값(eigenvalue)이 큰     
고유벡터(eigenvector)를 사상 축으로 선택



## t-SNE

t-distributed Stochastic Neighbor Embedding

> 유사한 데이터들은 저차원으로 변환되어도 유사할 것이다

A를 중심으로 한 t분포에서 B의 확률 밀도를 계산하여 값이 클수록 유사한 데이터


### 계산 과정

유사도 계산

1. 값 하나와 다른 모든 데이터들의 거리 계산
    - i(현재)를 기준으로 해서 j(다른 값)들간의 확률 밀도함수를 계산 가까울수록 큰 값이 나옴
    - 각각은 1보다 작기 때문에 합은 2보다 작음. 따라서 $p_ {ij}$ 는 1/N보다 작음. 따라서 $p_ {ij}$의 합은 1
2. 저차원에서도 밀도함수를 구해야 함
    - $x_ i$를 $y_ i$로 대응시켜야 함.   
    - y 에 대한 유사성은 다음으로 계산
    - exp가 없어짐. 가우시안 분포가 아니라 t분포라서
3. $x_ i$의 분포값과 $y_ i$의 분포값이 유사해지도록 $y_ i$의 값을 바꿔가며 반복 계산

# 이상치 탐지

> 이상치란, 다른 데이터와 크게 달라서 다른 메커니즘에 의해 생성된 것이 아닌지 의심스러운 데이터

- 잡음, 노이즈와는 다름. 노이즈는 잘못된 값. 오차.
- 이상치는 유의미한 값

이상치 ==> 신규성

- 점 이상치 
    - 다른 데이터와 비교하여 차이가 큰 데이터
- 상황적 이상치
    - 상황에 맞지 않는 데이터
    - ex. 여름에는 25도가 정상이지만, 겨울에 25도는 이상치
- 집단적 이상치
    - 여러 데이터를 모아서 보면 비정상으로 보이는 데이터들의 집단

이를 통해

- 부정사용감지 시스템
- 침입탐지 시스템
- 시스템의 고장 진단
- 임상에서 질환 진단 및 모니터링
- 공공보건에서 유행병 탐지
- 스포츠 통계학에서 특이 사건 감지
- 관측 오류를 감지

할 수 있다

## 이상치 탐지 방법

### Isolation Forest 

> 여러 tree를 만들어서 앙상블을 구성. 

1. 비복원 추출로 데이터 중 일부 샘플링 
2. 데이터 𝑋의 변수 중 하나 𝑞를 무작위로 선택 
3. 변수 𝑞의 범위(min~max)에서 균등하고 분할위치 선택 
4. 1~3과정 일정회수 반복하여 트리 구성 

먼저 리프노드로 떨어져 나간 것은 이상치일 수 있음

이 이상치점수가 크면클수록 이상치일 것이다


### Local Outlier Factor 

> 데이터의 상대적인 밀도까지 고려한 이상치 탐지

𝑟𝑒𝑎𝑐ℎ𝑎𝑏𝑖𝑙𝑖𝑡𝑦−𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑘(𝐴,𝐵) = max {𝑘−𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝐵), 𝑑𝑖𝑠𝑡(𝐴,𝐵)}

𝐴와 𝐵까지의 거리와 𝑘-𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒중 큰 값 사용
𝑘-𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 안에 들어오는 데이터들은 전부 원 밖으로 밀어내고 원 밖은 그대로 거리값 사용

(A에 k-distance 내 B들의 평균 local reachability density) / (A의 local reachability density)  

LOF : 1, 3은 작은 값 2는 큰값

# 반지도 학습

## 반지도 학습의 가정

- 평활성(smoothness, 平滑性) 가정 
    - 가까이 있는 점들은 서로 같은 부류에 속할 가능성이 높음
- 군집(cluster) 가정 
    - 같은 군집에 속하는 데이터는 동일한 부류에 속할 가능성이 높음
- 매니폴드(manifold)[^manifold] 가정  
    - 원래 차원보다 낮은 차원의 매니폴드에 데이터에 분포할 가능성이 높음  
- 대리 라벨 기법
    - 랩퍼 기법
    - 라벨이 있는 데이터로 학습된 모델을 이용해, 라벨이 없는 데이터에게 라벨을 부여
    - 모든 데이터(labeled, unlabeled)를 이용하여 모델 학습
- 그래프 기반 방법
    - 그래프의 노드로 labeled, unlabeled 데이터 표현
    - 노드 사이의 유사도를 기준으로 유사도 가중치가 높은 이웃으로 건너가며 라벨을 추정하는 라벨 전파 방식
- 미세변동 기반 방법
    - 데이터의 증강을 통해 부류가 바뀌지 않을 정도의 변화를 줬을 때도
    - 원 데이터와의 예측 결과가 같아지도록 함

[^manifold]: 국소적으로 유클리드(3차원 좌표계) 공간과 닮은 위상 공간이다. 즉, 국소적으로는 유클리드 공간과 구별할 수 없으나, 대역적으로 독특한 위상수학적 구조를 가질 수 있다.
