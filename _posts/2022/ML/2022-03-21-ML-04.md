---
layout: post
title: "ML, 4장 분류 기법"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/21
---

* Kramdown table of contents
{:toc .toc} 

# 결정트리

> 트리 형태로 의사결정 지식을 표현한 것

- 내부 노드(internal node) : 비교 속성
- 간선(edge) : 속성 값
- 단말 노드(terminal node) : 부류(class), 대표값


# 결정 트리 알고리즘

> 모든 데이터를 포함한 하나의 노드로 구성된 트리에서 시작

그리디 알고리즘 : 지금 좋아보이는걸 선택. 근시안적인 알고리즘

1. 분할 속성 선택
2. 속성값에 따라 서브트리 생성
3. 데이터를 속성값에 따라 분배
4. 1~3 반복


결정트리 과적합 축소 전략
- 가지치기
- 단말 노드가 가져야 하는 최소 데이터 개수 지정
- 결정트리 최대 깊이 제한

## 분할 속성 선택

> 분할한 결과가 가능하면 동질적인 것으로 만드는 속성 선택

- `엔트로피`
  - 동질적인 정도를 측정하는 척도
  - 작을 수록 동질적이며, 클 수록 다른 데이터들이 섞여있음

@
I = - \sum \limits_ {c}^{} p(c) log_ 2 p(c)
@
p(c) : 부류 c에 속하는 것의 비율

그러면 비교를 해서 속성을 찾아야 하는데,

속성으로 분할한 후에 엔트로피가 작은것이 좋은것.

그러면 `현재 엔트로피 값 - 분할후 엔트로피 값`이 큰게 좋음

-> `정보 이득`[^IG] : $ IG = I - I_{res} $[^I_res]

엔트로피는 bits 라는 단위 사용. 정보이론에서 근간

[^IG]: 정보 이득은 클 수록 좋다
[^I_res]: 특정 속성으로 분할한 후의 각 부분집합의 정보량의 가중평균

## 정보 이득

@IG = I - I_ {res}@

`IG, 정보 이득은 커야 좋다`

$
I = - \sum \limits_ {c}^{} p(c) log_ 2 p(c)
$

$
\begin{align\*}
I_ {res} &= \sum \limits_ {v}^{} p(v) I(v) \\\ 
&= - \sum \limits_ {v}^{} p(v) \sum \limits_ {c}^{} p(c|v) log_ 2 p(c|v)
\end{align\*}
$

$
IG = I - I_{res} = -\sum \limits_ {c}^{} p(c) log_ 2 p(c) + \sum \limits_ {v}^{} p(v) \sum \limits_ {c}^{} p(c|v) log_ 2 p(c|v)
$

실제로는 이게 몇퍼센트까지 허용할것인지도 해야함

단점    
- 속성값이 많은 것을 선호, 속성값이 많으면 데이터 집합이 많은 부분집합으로 나뉨

#### 예

![image](https://user-images.githubusercontent.com/32366711/163707012-9b20cb7b-efab-4ee2-804a-ceca4662b208.png)

$
p(\square) = \frac{9}{14} \\\ 
p(\triangle) = \frac{5}{14}
$

전체 데이터에 대한 `엔트로피`

$
\begin{align\*}
I &= - \sum \limits_ {c}^{} p(c) log_ 2 p(c) \\\ 
  &= -\frac{9}{14}log_ 2\frac{9}{14} - \frac{5}{14}log_ 2\frac{5}{14} \\\ 
  &= 0.940 \, bits
\end{align\*}
$

패턴을 기준 분할로 나눈다면,

$
\begin{align\*}
I_ {horiaontal} &= -\frac{2}{5}log_ 2\frac{2}{5} - \frac{3}{5}log_ 2\frac{3}{5} \\\ 
  &= 0.971 \, bits
\end{align\*}
$

$
\begin{align\*}
I_ {diagonal} &= -\frac{0}{4}log_ 2\frac{0}{4} - \frac{4}{4}log_ 2\frac{4}{4} \\\ 
  &= 0.0 \, bits
\end{align\*}
$

$
\begin{align\*}
I_ {vertical} &= -\frac{3}{5}log_ 2\frac{3}{5} - \frac{2}{5}log_ 2\frac{2}{5} \\\ 
  &= 0.971 \, bits
\end{align\*}
$

$
I_ {res} (Pattern) = \sum \limits_ {v}^{} p(v) I(v) = 5/14 * 0.971 + 4/14 * 0.0 + 5/14 * 0.971 = 0.694 \\\ 
\\\ 
\therefore IG(Pattern) = I - I_ {res}(Pattern) = 0.940 - 0.694 = 0.246
$


## 정보이득 비 척도

> 속성값이 많은 속성에 대해 불이익을 주어,          
> 속성값이 적으면서, 정보이득이 큰 값을 찾음

속성값이 많을수록 I(A)가 커짐. 그러면 정보이득비는 작아짐

따라서 정보이득 비는 큰게 좋음

@
GainRatio(A) = \frac{IG(A)}{I(A)} = \frac{I - I_ {res}(A)}{I(A)}
@

$I(A)$

- 속성 A의 속성값을 부류로 간주하여 계산한 엔트로피
- $ I(A) = - \sum\limits_ {v}^{} log(v) log_ 2 p(v)$


$
\begin{align\*}
I(Pattern) &= - \sum\limits_ {v}^{} log(v) log_ 2 p(v) \\\ 
&= -\frac{5}{14} log_ 2 \frac{5}{14} -\frac{4}{14} log_ 2 \frac{4}{14} -\frac{5}{14} log_ 2 \frac{5}{14} \\\ 
&= 1.58
\end{align\*}
$

$
\begin{align\*}
IG(Pattern) &= I - I_ {res} (Pattern) \\\ 
&= 0.940 - 0.694 = 0.246
\end{align\*}
$

$
GainRatio(Pattern) = \frac{IG(Pattern)}{I(Pattern)} = \frac{0.246}{1.58} = 0.156
$

## 지니 지수, 지니 지수 이득

@
Gini = \sum\limits_ {i \neq j}^{} p(i)p(j) \\\ 
Gini(A) = \sum\limits_ {v}^{} p(v) \sum\limits_ {i \neq j}^{} p(i)p(j) \\\ 
GiniGain(A) = Gini - Gini(A)
@

$
Gini = \frac{9}{14} \times \frac{5}{14}
$

$
\begin{align\*}
Gini(Pattern) &= \frac{5}{14} \times \frac{3}{5} \times \frac{2}{5} \\\ 
&+ \frac{5}{14} \times \frac{2}{5} \times \frac{3}{5} \\\ 
&+ \frac{4}{14} \times \frac{4}{4} \times \frac{0}{4} \\\ 
&= 0.171
\end{align\*}
$

$
GiniGain(Pattern) = 0.230 - 0.171 = 0.058
$

## 그외 결정트리 알고리즘

- ID3 알고리즘
  - 범주형 속성값을 갖는 데이터에 대한 결정트리 학습
- C4.5 알고리즘
  - 범주형 속성값과 수치형 속성값을 갖는 데이터로부터 결정트리를 학습
  - ID3를 개선한 알고리즘
- C5.0 알고리즘
  - C4.5를 개선한 알고리즘
- CART 알고리즘
  - 수치형 속성을 갖는 데이터에 대해 적용

# 결정트리를 이용한 회귀

> 단말노드가 부류가 아닌 수치값            
> 해당 조건을 만족하는 것들이 가지는 대표값        

표준편차 축소(reduction of standard deviation), 𝑺𝑫𝑹을 최대로 하는 속성 선택

즉, 나눈 이후의 표준편차의 차이가 최대로 되게 함
@
𝑺𝑫𝑹(𝑨) = 𝑺𝑫 – 𝑺𝑫(𝑨)
@

- 표준편차 $𝑺𝑫 = \sqrt{\frac{1}{N} \sum \limits_ {i=1}^{N}(x_ i - \bar{x})^2 }$
- 𝑺𝑫(𝐴) : 속성 A를 기준으로 분할 후의 부분 집합별 표준표차의 가중평균



ex.

- SD = 9.67
- SD(Pattern) = 5/14 * 12.15 + 5/14*9.36 + 4/14*5.77 = 9.05
- SDR(Pattern) = SD - SD(Pattern) = 9.67 - 9.05 = 0.61



# 로지스틱 회귀

> 로지스틱 함수를 사용한 이진 분류        
> 회귀라고 하지만, 실제 용도는 분류     

@
f(x) = \frac{1}{1 + e^{-ax}}
@

0 ~ 1사이의 값을 갖기에 확률로 해석할수도 있다


## 가능도

> 모델이 학습 데이터를 생성할 가능성

$
P(X) = \prod \limits_ {i=1}^{N} f(x_ i) ^ {y_ i}(1-f(x_ i))^ {1-y_ i}
$

$
\begin{align\*}
Log P &= -\frac{1}{N}log P(X) \\\ 
&= -\frac{1}{N} \sum \limits_ {i=1}^{N}(y_ i log f(x_ i) + (1- f(x_ i)) log(1-y_ i)) \\\ 
\end{align\*}
$

$(1-f(x))log(1-y)$ : 크로스 엔트로피


# K-NN 알고리즘

K-근접 이웃 알고리즘

> 라벨링이 되어있는 데이터들이 주어진 상황에서, 새로운 입력에 대한 결과를 추정할 때      
> 근접한 K개의 데이터에 대한 결과정보를 이용하는 방법

- 학습단계에서 실질적인 학습이 이루어지는게 아닌 데이터만 저장함
  - 학습 데이터가 크면 메모리 문제가 있음
  - 게으른 학습
- 새로운 데이터가 주어지면, 저장된 데이터를 이용하여 학습함
  - 시간이 많이 걸릴 수 있음

## 결과 추정 방식

- 분류
  - 출력이 범주형 값이여야함
  - 따라서 다수결 투표, 개수가 많은 범주 선택, 가중치 반영 가능
- 회귀
  - 출력이 수치형 값이여야함
  - 근접 k개의 평균 값 
  - or 가중합, 거리에 반비례하는 가중치를 사용


- query와 데이터간의 거리 계산
- 효율적으로 근접이웃 탐색
  - 데이터 개수가 많아질수록 계산 시간이 증가하기에 index 자료구조를 사용함
- 근접 이웃 k개로부터 결과 추정

## 거리 계산

- 수치 데이터의 경우 : 유클리드 거리
- 범주형 데이터의 경우 : 응용 분야의 특성에 맞춰 개발

- 1-norm 거리, City block 거리
  - $\sum\limits_ {i=1}^{N}\| x_ i - y_ i \|$
- 2-norm 거리, Euclidean(유클리드)
  - $(\sum\limits_ {i=1}^{N}\| x_ i - y_ i \|^2) ^{1/2}$ 
- P-norm 거리, Minkowski 거리
  - $(\sum\limits_ {i=1}^{N}\| x_ i - y_ i \|^p) ^{1/p}$ 
- Infinity norm 거리
  - $\underset{p \rightarrow \infty}{lim} (\sum\limits_ {i=1}^{N}\| x_ i - y_ i \|^p) ^{1/p} $
  - $= max(\|x_ 1, y_ 1\|, \|x_ 2, y_ 2\| , \cdots, \|x_ n, y_ n\|)$
- Cosine 거리
  - $\frac{\sum\limits_ {i}^{}x_ i y_ i}{\sqrt{\sum\limits_ {i}^{}x_ i ^2} \sqrt{\sum\limits_ {i}^{}y_ i ^2}}$

## 효율적인 근접 이웃 탐색

> 데이터의 개수가 많아지면 계산시간이 증가하기에      
> indexing, 색인 자료구조 사용          

### Ball tree 구조

> 각 노드가 d차원의 초공간인 두 개의 자식 노드를 갖는 이진 트리

각 내부 노드는 데이터를 2개의 부분집합으로 분할하여 가짐

<img width="467" alt="image" src="https://user-images.githubusercontent.com/32366711/163707835-5494913d-8561-43aa-be39-5953408fee3b.png">

### k-d 트리

> k 차원 공간의 데이터를 저장하는 이진 트리 자료 구조

<img width="440" alt="image" src="https://user-images.githubusercontent.com/32366711/163707842-50b156ea-6abf-4894-939c-65f39e20f05d.png">



