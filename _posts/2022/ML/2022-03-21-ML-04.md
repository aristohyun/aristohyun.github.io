---
layout: post
title: "ML, 4장 분류 기법"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/21
---

* Kramdown table of contents
{:toc .toc} 

# 결정트리

> 트리 형태로 의사결정 지식을 표현한 것

- 내부 노드(internal node) : 비교 속성
- 간선(edge) : 속성 값
- 단말 노드(terminal node) : 부류(class), 대표값


# 결정 트리 알고리즘

> 모든 데이터를 포함한 하나의 노드로 구성된 트리에서 시작

그리디 알고리즘 : 지금 좋아보이는걸 선택. 근시안적인 알고리즘

1. 분할 속성 선택
2. 속성값에 따라 서브트리 생성
3. 데이터를 속성값에 따라 분배
4. 1~3 반복

## 분할 속성 선택

> 분할한 결과가 가능하면 동질적인 것으로 만드는 속성 선택

`엔트로피`

- 동질적인 정도를 측정하는 척도

@
I = - \sum \limits_ {c}^{} p(c) log_ 2 p(c)
@
p(c) : 부류 c에 속하는 것의 비율

그러면 비교를 해서 속성을 찾아야 하는데,

속성으로 분할한 후에 엔트로피가 작은것이 좋은것.

그러면 현재 엔트로피값 - 분할후 엔트로피값이 큰게 좋음

-> `정보 이득` : $ IG = I - I_{res} $

엔트로피는 bits 라는 단위 사용. 정보이론에서 근간

## 정보 이득


실제로는 이게 몇퍼센트까지 허용할것인지도 해야함

단점
속성값이 많은 것을 선호, 속성값이 많으면 데이터 집합이 많은 부분집합으로 나뉨

## 정보이득 비 척도

> 속성값이 많은 속성에 대해 불이익을 주어,          
> 속성값이 적으면서, 정보이득이 큰 값을 찾음

속성값이 많을수록 I(A)가 커짐. 그러면 정보이득비는 작아짐

## 지니 지수

> 

뭐가 더 좋은진 상황따라 다름

## 그외 결정트리 알고리즘


# 결정트리를 이용한 회귀

> 단말노드가 부류가 아닌 수치값            
> 해당 조건을 만족하는 것들이 가지는 대표값        

표준편차 축소(reduction of standard deviation) 𝑺𝑫𝑹를 최대로 하는 속성 선택

@
𝑺𝑫𝑹(𝑨) = 𝑺𝑫 – 𝑺𝑫(𝑨)
@

– 표준편차 𝑺𝑫 =
– 𝑺𝑫(𝐴) : 속성 A를 기준으로 분할 후의 부분 집합별 표준표차의 가중평균

나눈 이후의 표준편차의 차이

# 로지스틱 회귀

> 로지스틱 함수를 사용한 이진 분류        
> 회귀라고 하지만, 실제 용도는 분류     

@
f(x) = \frac{1}{1 + e^{-ax}}
@

0 ~ 1사이의 값을 갖기에 확률로서 해석할수도 있다


## 가능도

> 모델이 학습 데이터를 생성할 가능성

$
P(X) = \prod \limits_ {i=1}^{N} f(x_ i) ^ {y_ i}(1-f(x_ i))^ {1-y_ i}
$

@
\begin{align\*}
Log P &= -\frac{1}{N}log P(X) \\\ 
&= -\frac{1}{N} \sum \limits_ {i=1}^{N}() \\\ 
\end{align\*}
@

(1-fx)log(1-y) : 크로스엔트로피


최대 가능도 추정법, MLE

결정트리 과적합 축소 전략
- 가지치기
- 단말 노드가 가져야 하는 최소 데이터 개수 지정
- 결정트리 최대 깊이 제한