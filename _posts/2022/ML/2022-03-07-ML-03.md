---
layout: post
title: "ML, 3장 회귀, 추천"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/10
---

* Kramdown table of contents
{:toc .toc} 

# 회귀

> 학습 데이터에 부합되는 출력값이 실수인 함수를 찾는 문제

오차함수를 찾아, 오차가 작도록 하는 모델을 찾아야함

@
E = \frac{1}{n}\sum \limits_ {i=1}{n} (y_ i - f(x_ i))^2
@

이때 지나치게 단순한 모델(함수)를 사용하면 부적합의 문제가 있으며,

지나치게 복잡한 모델(함수)를 사용하면 과적합의 문제가 있다.

모든 데이터는 오차를 포함하고 있는데, 이 오차까지 학습하게 되면(과적합) 안된다.

### 과적합 대응 방법

모델 복잡도를 성능 평가에 반영

> 목적함수 = 오차의 합(MSE) + 가중치 * 모델 복잡도(패널티 항)

- 어떤 가중치가 너무 큰 역할을 할 때, 이 값에 따라 전체 데이터 값에 큰 영향을 미치게 되면, 다른 가중치들은 학습이 제대로 이루어 지지 않는다
- 이에 복잡도, 패널티항을 주어 수준을 비슷하게 맞춰줌

## 경사 하강법

> 오차 함수 *E*의 그레디언트(gradient)[^gradient] 반대 방향으로 조금씩 움직여 가며 최적의 파라미터를 찾으려는 방법

데이터의 입력과 출력을 이용하여 각 파라미터에 대한 그레디언트를 계산하여 파라미터를 반복적으로 조금씩 조정

learning late, $\eta$.  학습률은 작게, 그래야 최소값을 지나치지 않고 찾을 수 있음

@
E = \frac{1}{n}\sum \limits_ {i=1}{n} (y_ i - f(x_ i))^2 \\\ 
f(x) = ax + b
@

@
\triangledown E = (\frac{\partial E}{\partial a}, \frac{\partial E}{\partial b})
@

@
a \leftarrow a - \eta \frac{\partial E}{\partial a} \\\ 
b \leftarrow b - \eta \frac{\partial E}{\partial b} \\\ 
@


[^gradient]: 각 파라미터에 대해 편미분한 벡터


## 편향-분산 트레이드오프(bias-variance tradeoff)

<img width="527" alt="image" src="https://user-images.githubusercontent.com/32366711/163703730-ca121908-34a4-468e-ab37-4182e1cee6cc.png">

### 편향 (bias)
 
> 학습된 모델들의 평균적인 예측와 실제값의 차이        
> 실제값과 얼마나 차이가 나는가

`큰 편향값은 부적합(underfitting) 문제 초래`

학습 알고리즘에서 모델의 형태에 잘못된 가정을 할 때 발생하는 오차

### 분산 (variance) 

> 학습된 모델들의 예측값들의 평균적인 차이          
> 예측값들끼리 얼마나 차이가 나는가

`큰 분산값은 과적합(overfitting) 문제 초래`

학습 데이터에 내재된 작은 변동(fluctuation) 때문에 발생하는 오차

큰 잡음까지 학습하게 된 것

### 편향-분산 분해

> 학습 알고리즘의 기대 오차를 분석하는 한 가지 방법

실제모델을 모른체 관측값으로만 예측 모델을 만들어야함

이 관측치는 실제모델에서 오차치, 잡음이 추가된 값임

이때 여러 다양한 모델을 예측해 만들어 이들의 평균을 구함

@\bar{F}(x) = E[f_ D (x)]@

오차($y - f(x)$)의 기댓값 $E( y - f(x))$

이때 MSE를 구하면 다음과 같다

@
\begin{align\*}
Err(x_ 0) &= E[y - f(x) | x=x_ 0]^ 2 \\\ 
&= E[f^* (x_ 0) - f(x_ 0) + \epsilon]^2 \\\ 
\end{align\*}
@

$f^*(x_ 0) - f(x_ 0) \equiv  A \quad, \epsilon \equiv  B$ 일때, 

$E[\epsilon] = 0, \quad E[\epsilon^2] = \sigma^2, \quad E[\bar F (X_ 0) - f(x_ 0)] = 0$ 를 이용해 위 식을 정리하면,

@
\begin{align\*}
E\[A^2\] + E\[B^2\] + 2E\[AB\] &= E\[A^2\] + E\[B^2\] + 2E\[A\]\[B\] \\\ 
&= E\[A^2\] + E\[B^2\] \\\ 
&= E\[(f^*(x_ 0) - f(x_ 0))^2\] + \sigma^2 \\\ 
Err(x_ 0) &= (f^* (x_ 0) - \bar F (X_ 0))^2 + E[(f(x_ 0) - \bar F (X_ 0))^2] + \sigma^2 \\\ 
&= bias^2(\bar F) + var(\bar F) + \sigma^2
\end{align\*}
@

즉 오차는 편향, 분산, 그리고 데이터 자체가 내재하는 오류의 합을 의미한다

이는 모든 형태의 지도 학습에 응용되며, 또한 사람의 학습에서 직관적 판단 오류(heuristics)의 효과성을 설명하기 위해 언급되기도 한다.


# 추천

> 개인별로 맞춤형 정보를 제공하려는 기술

사용자에게 맞춤형 정보를 제공하여 정보 검색의 부하를 줄여주는 역할

`희소 행렬(sparse matrix) 형태`

비어있는 부분을 채워, 값이 큰것을 출력하는 것이 추천에 해당


## 내용 기반 추천, content-based filtering

> 고객이 이전에 높게 평가했던 것과 유사한 내용을 갖는 대상을 추천

비교할 대상, 내용이 있어야 비교를 하기에         
항목별 데이터, 항목 프로파일[^item_profile]과 사용자 프로파일[^user_profile]이 있어야 한다

[^item_profile]: 추천 대상 항목에 대한 특징 기술
[^user_profile]: 고객별 선호 대상 정보 기술




### 회귀를 사용한 내용 기반 추천

다만 사용자 프로파일은 알 수 없기에, 이전에 점수를 매긴 영화를 바탕으로 프로파일 값을 계산하여 알아낸 후,

다른 영화를 추천한다

즉, 영화프로파일 * 사용자 프로파일을 계산해서 별점이 나와야 한다면, 

이는 회귀식과 같다. 즉 회귀문제를 풀면 사용자가 좋아할 영화를 계산할 수 있다.

<img width="523" alt="image" src="https://user-images.githubusercontent.com/32366711/163703741-9c1d4a28-7b4f-4271-a61e-01355bb996ff.png">


## 협력 필터링

### 사용자간 협력 필터링

> 추천 대상 사용자와 비슷한 평가를 사용한 집합 이용

유사도 평가에 코사인 거리를 사용한다

@
cos(U_ i, U_ j) = \frac{U_ i \cdot U_ j}{|U_ i||U_ j|}
@

### 항목간 협력 필터링

> 항목간의 유사로를 구하여 유사 항목을 선택

@
\hat r(x, I_ a) = \frac{\sigma_ {I_ b} s(I_ a, I_ b) r(x, I_ b)}{\sigma_ {I_ b} r(x, I_ b)}
@

$r(x, I_ b)$ : u의 항목 $I_ b$에 대한 평가 등급 

$s(I_ a, I_ b)$ : 항목 $I_ a$와 $I_ b$의 유사도



## 은닉 요소 모델

> 행렬 분해에 기반한 방법

분해된 행렬의 곱을 사용하여 추천 행렬의 빈 원소 결정

<img width="518" alt="image" src="https://user-images.githubusercontent.com/32366711/158959973-74e908da-035a-4038-9c50-be160e1717ee.png">


### ALS 알고리즘

<img width="403" alt="image" src="https://user-images.githubusercontent.com/32366711/158960002-cda64b61-3574-4543-b251-abfba1115be7.png">


@
r_ {i,u} = q^T_ i * p_ u
@

목적함수
@
min_ {p,q} \sum \limits_{observed \, r_{i,u}}^{} {(r_ {i,u} - q^T_ i p_ u )^2 + \lambda (|| p_ u ||^2 || q_ i ||^2)}
@

p에 대해서 구하고, q에 대해서 구한 다음 반복

하나만 쓰는게 아니라 여러개 쓰고 성능 좋은거 채택

