---
layout: post
title: "ML, 3장 회귀, 추천, 비지도 학습, 반지도 학습"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/3
  - /blog/ML/3
---

* Kramdown table of contents
{:toc .toc} 

# 회귀

> 학습 데이터에 부합되는 출력값이 실수인 함수를 찾는 문제

오차함수를 찾아, 오차가 작도록 하는 모델을 찾아야함

@
E = \frac{1}{n}\sum \limits_ {i=1}{n} (y_ i - f(x_ i))^2
@

이때 지나치게 단순한 모델(함수)를 사용하면 부적합의 문제가 있으며,

지나치게 복잡한 모델(함수)를 사용하면 과적합의 문제가 있다.

모든 데이터는 오차를 포함하고 있는데, 이 오차까지 학습하게 되면(과적합) 안된다.

### 과적합 대응 방법

모델 복잡도를 성능 평가에 반영

> 목적함수 = 오차의 합(MSE) + 가중치 * 모델 복잡도(패널티 항)

- 어떤 가중치가 너무 큰 역할을 할 때, 이 값에 따라 전체 데이터 값에 큰 영향을 미치게 되면, 다른 가중치들은 학습이 제대로 이루어 지지 않는다
- 이에 복잡도, 패널티항을 주어 수준을 비슷하게 맞춰줌

## 경사 하강법

> 오차 함수 *E*의 그레디언트(gradient)[^gradient] 반대 방향으로 조금씩 움직여 가며 최적의 파라미터를 찾으려는 방법

데이터의 입력과 출력을 이용하여 각 파라미터에 대한 그레디언트를 계산하여 파라미터를 반복적으로 조금씩 조정

learning late, $\eta$.  학습률은 작게, 그래야 최소값을 지나치지 않고 찾을 수 있음

@
E = \frac{1}{n}\sum \limits_ {i=1}{n} (y_ i - f(x_ i))^2 \\\ 
f(x) = ax + b
@

@
\triangledown E = (\frac{\partial E}{\partial a}, \frac{\partial E}{\partial b})
@

@
a \leftarrow a - \eta \frac{\partial E}{\partial a} \\\ 
b \leftarrow b - \eta \frac{\partial E}{\partial b} \\\ 
@


[^gradient]: 각 파라미터에 대해 편미분한 벡터


## 편향-분산 트레이드오프(bias-variance tradeoff)

<img width="527" alt="image" src="https://user-images.githubusercontent.com/32366711/163703730-ca121908-34a4-468e-ab37-4182e1cee6cc.png">

### 편향 (bias)
 
> 학습된 모델들의 평균적인 예측와 실제값의 차이        
> 실제값과 얼마나 차이가 나는가

`큰 편향값은 부적합(underfitting) 문제 초래`

학습 알고리즘에서 모델의 형태에 잘못된 가정을 할 때 발생하는 오차

### 분산 (variance) 

> 학습된 모델들의 예측값들의 평균적인 차이          
> 예측값들끼리 얼마나 차이가 나는가

`큰 분산값은 과적합(overfitting) 문제 초래`

학습 데이터에 내재된 작은 변동(fluctuation) 때문에 발생하는 오차

큰 잡음까지 학습하게 된 것

### 편향-분산 분해

> 학습 알고리즘의 기대 오차를 분석하는 한 가지 방법

실제모델을 모른체 관측값으로만 예측 모델을 만들어야함

이 관측치는 실제모델에서 오차치, 잡음이 추가된 값임

이때 여러 다양한 모델을 예측해 만들어 이들의 평균을 구함

@\bar{F}(x) = E[f_ D (x)]@

오차($y - f(x)$)의 기댓값 $E( y - f(x))$

이때 MSE를 구하면 다음과 같다

@
\begin{align\*}
Err(x_ 0) &= E[y - f(x) | x=x_ 0]^ 2 \\\ 
&= E[f^* (x_ 0) - f(x_ 0) + \epsilon]^2 \\\ 
\end{align\*}
@

$f^*(x_ 0) - f(x_ 0) \equiv  A \quad, \epsilon \equiv  B$ 일때, 

$E[\epsilon] = 0, \quad E[\epsilon^2] = \sigma^2, \quad E[\bar F (X_ 0) - f(x_ 0)] = 0$ 를 이용해 위 식을 정리하면,

@
\begin{align\*}
E\[A^2\] + E\[B^2\] + 2E\[AB\] &= E\[A^2\] + E\[B^2\] + 2E\[A\]\[B\] \\\ 
&= E\[A^2\] + E\[B^2\] \\\ 
&= E\[(f^*(x_ 0) - f(x_ 0))^2\] + \sigma^2 \\\ 
Err(x_ 0) &= (f^* (x_ 0) - \bar F (X_ 0))^2 + E[(f(x_ 0) - \bar F (X_ 0))^2] + \sigma^2 \\\ 
&= bias^2(\bar F) + var(\bar F) + \sigma^2
\end{align\*}
@

즉 오차는 편향, 분산, 그리고 데이터 자체가 내재하는 오류의 합을 의미한다

이는 모든 형태의 지도 학습에 응용되며, 또한 사람의 학습에서 직관적 판단 오류(heuristics)의 효과성을 설명하기 위해 언급되기도 한다.


# 추천

> 개인별로 맞춤형 정보를 제공하려는 기술

사용자에게 맞춤형 정보를 제공하여 정보 검색의 부하를 줄여주는 역할

`희소 행렬(sparse matrix) 형태`

비어있는 부분을 채워, 값이 큰것을 출력하는 것이 추천에 해당


## 내용 기반 추천, content-based filtering

> 고객이 이전에 높게 평가했던 것과 유사한 내용을 갖는 대상을 추천

비교할 대상, 내용이 있어야 비교를 하기에         
항목별 데이터, 항목 프로파일[^item_profile]과 사용자 프로파일[^user_profile]이 있어야 한다

[^item_profile]: 추천 대상 항목에 대한 특징 기술
[^user_profile]: 고객별 선호 대상 정보 기술




### 회귀를 사용한 내용 기반 추천

다만 사용자 프로파일은 알 수 없기에, 이전에 점수를 매긴 영화를 바탕으로 프로파일 값을 계산하여 알아낸 후,

다른 영화를 추천한다

즉, 영화프로파일 * 사용자 프로파일을 계산해서 별점이 나와야 한다면, 

이는 회귀식과 같다. 즉 회귀문제를 풀면 사용자가 좋아할 영화를 계산할 수 있다.

<img width="523" alt="image" src="https://user-images.githubusercontent.com/32366711/163703741-9c1d4a28-7b4f-4271-a61e-01355bb996ff.png">


## 협력 필터링

### 사용자간 협력 필터링

> 추천 대상 사용자와 비슷한 평가를 사용한 집합 이용

유사도 평가에 코사인 거리를 사용한다

@
cos(U_ i, U_ j) = \frac{U_ i \cdot U_ j}{|U_ i||U_ j|}
@

### 항목간 협력 필터링

> 항목간의 유사로를 구하여 유사 항목을 선택

@
\hat r(x, I_ a) = \frac{\sigma_ {I_ b} s(I_ a, I_ b) r(x, I_ b)}{\sigma_ {I_ b} r(x, I_ b)}
@

$r(x, I_ b)$ : u의 항목 $I_ b$에 대한 평가 등급 

$s(I_ a, I_ b)$ : 항목 $I_ a$와 $I_ b$의 유사도



## 은닉 요소 모델

> 행렬 분해에 기반한 방법

분해된 행렬의 곱을 사용하여 추천 행렬의 빈 원소 결정

<img width="518" alt="image" src="https://user-images.githubusercontent.com/32366711/158959973-74e908da-035a-4038-9c50-be160e1717ee.png">


### ALS 알고리즘

<img width="403" alt="image" src="https://user-images.githubusercontent.com/32366711/158960002-cda64b61-3574-4543-b251-abfba1115be7.png">


@
r_ {i,u} = q^T_ i * p_ u
@

목적함수
@
min_ {p,q} \sum \limits_{observed \, r_{i,u}}^{} {(r_ {i,u} - q^T_ i p_ u )^2 + \lambda (|| p_ u ||^2 || q_ i ||^2)}
@

p에 대해서 구하고, q에 대해서 구한 다음 반복

하나만 쓰는게 아니라 여러개 쓰고 성능 좋은거 채택

# 비지도 학습

> 결과정보가 없는 데이터들에 대해서 특정 패턴을 찾는 것

- 데이터에 잠재한 구조(structure), 계층구조(hierarchy) 를 찾아내는 것
- 숨겨진 사용자 집단(hidden user group)을 찾는 것
- 상대적 거리를 유지하면서 데이터를 낮은 차원으로 표현하는 것
- 문서들을 주제에 따라 구조화하는 것
- 로그(log) 정보를 사용하여 사용패턴(usage pattern)을 찾아내는 것

#### 비지도 학습의 대상

- 군집화(clustering)
- 밀도추정(density estimation)
- 차원축소(dimensionality reduction) 
- 이상치 탐지 (outlier detection) 
- 연관규칙 마이닝 (association rule mining) 
- 토픽 모델링(topic modeling)

# 군집화

> 데이터를 유사한 것들끼리 모우는 것           
> 군집 내의 유사도(similarity)는 크게, 군집 간의 유사도는 작게

- 일반 군집화(hard clustering)
    - 데이터는 하나의 군집에만 소속
    - 예. k-means 알고리즘
- 퍼지 군집화(fuzzy clustering) 
    - 데이터가 여러 군집에 부분적으로 소속
    - 소속정도의 합은 1이 됨
    - 예. 퍼지 k-means 알고리즘

`군집내 분산과 군집간의 거리가 작을수록 좋은 성능`

## 군집간 거리 계산 방법

그룹 r과 s의 거리 $L(r,s)$ 는

- 최단(단순) 연결  
    - $min(D(x_ {ri}, x_ {sj}))$
- 최장(완전) 연결 
    - $max(D(x_ {ri}, x_ {sj}))$
- 평균 연결 
    - $\frac{1}{n_ r n_ s} \sum\limits_ {i=1}^{n_ r} \sum\limits_ {j=1}^{n_ s}  D(x_ {ri}, x_ {sj})$
- 중심 연결 
    - $D(\frac{1}{n_ r} \sum\limits_ {i=1}^{n_ r} x_ {ri},\frac{1}{n_ s} \sum\limits_ {i=1}^{n_ s} x_ {sj})$
- ward 연결
    - 모든 클러스터 내의 분산을 가장 작게 증가시키는 두 클러스터를 합친다. 따라서 비교적 크기가 비슷한 클러스터 끼리 뭉친다
    - 오차 제곱합을 기준으로 수행


## 계층적 군집화 (hierarchical clustering)

> 군집화의 결과가 군집들이 계층적인 구조를 갖도록 하는 것

- 병합형(agglomerative) 계층적 군집화
- 분리형(divisive) 계층적 군집화
    - 모든 데이터를 포함한 군집에서 시작하여 유사성을 바탕으로 군집을 분리하여 점차 계층적인 구조를 갖도록 구성

## 분할 군집화 (partitioning clustering)

> 계층적 구조를 만들지 않고 전체 데이터를 유사한 것들끼리 나누어서
묶는 것

- 예. k-means 알고리즘, DBSCAN 알고리즘

### K-means 알고리즘

1. 군집의 중심 위치 무작위로 선정
2. 군집 중심을 기준으로 군집 재구성
3. 군집별 평균 위치 결정
4. 군집 평균 위치로 군집 중심 조정
5. 수렴할 때까지 2-4 과정 반복

#### 엘보우 방법

> 군집 개수 k에 따른 군집화 결과에 대한 군집 내 SSE(Sum of Squared Errors, 왜곡)를 그래프로 그려서    
> 가장 크게 꺽이는 위치의 k를 선택하는 방법

<img width="400" alt="image" src="https://user-images.githubusercontent.com/32366711/163704909-f3bf0b13-0ce0-415b-bedf-837328497e42.png">


### DBSCAN 알고리즘

> 일정 밀도 이상으로 연결된 영역을 찾아 가면서 군집화

임의의 데이터 p에서 시작하여 

Eps(반경) 내에 MinPts(최소 이웃의 수)를 갖는 데이터는 연결된 것으로 간주하여 군집 구성

# 밀도 추정

> 부류(class)별 데이터를 만들어 냈을 것으로 추정되는 확률분포을 찾는 것

- 각 부류 별로 주어진 데이터를 발생시키는 확률 계산
- 가장 확률이 높은 부류로 분류

## 모수적 밀도 추정

> 분포가 특정 수학적 함수의 형태를 가지고 있다고 가정

- 주어진 데이터를 가장 잘 반영하도록 함수의 파라미터 결정
- 전형적인 형태 : 가우시안(Gaussian) 함수 또는 여러 개의 가우시안 함수의 혼합(Mixture of Gaussian)

## 비모수적 밀도 추정

>분포에 대한 특정 함수를 가정하지 않고, 주어진 데이터를 사용하여 밀도함수의 형태 표현

- 전형적인 형태 : 히스토그램(histogram)


# 차원축소

> 고차원의 데이터를 정보의 손실을 최소화하면서 저차원으로 변환하는 것

- 고차원의 데이터를 2, 3차원으로 변환하여, 직관적으로 분석할 수 있음
- 차원의 저주 문제[^curse_of_dimensionality] 완화 가능


[^curse_of_dimensionality]: 차원이 커질수록 거리 분포가 일정해지는 경향이 있다. 또한 차원이 증가함에 따라 부분공간의 개수가 기하급수적으로 증가한다.

## 주성분 분석, PCA

Principle Component Analysis

> 분산이 큰 몇개의 축들을 기준으로 데이터를 투영(projection)하여
저차원으로 변환
$
\frac{x * r}{|| v ||}
$

데이터를 사영 시킬 때, 분산이 가장 커지는 벡터가 주성분

데이터의 공분산행렬(covariance matrix)에 대한 고유값(eigenvalue)이 큰     
고유벡터(eigenvector)를 사상 축으로 선택



## t-SNE

t-distributed Stochastic Neighbor Embedding

> 유사한 데이터들은 저차원으로 변환되어도 유사할 것이다

A를 중심으로 한 t분포에서 B의 확률 밀도를 계산하여 값이 클수록 유사한 데이터


### 계산 과정

유사도 계산

1. 값 하나와 다른 모든 데이터들의 거리 계산
    - i(현재)를 기준으로 해서 j(다른 값)들간의 확률 밀도함수를 계산 가까울수록 큰 값이 나옴
    - 각각은 1보다 작기 때문에 합은 2보다 작음. 따라서 $p_ {ij}$ 는 1/N보다 작음. 따라서 $p_ {ij}$의 합은 1
2. 저차원에서도 밀도함수를 구해야 함
    - $x_ i$를 $y_ i$로 대응시켜야 함.   
    - y 에 대한 유사성은 다음으로 계산
    - exp가 없어짐. 가우시안 분포가 아니라 t분포라서
3. $x_ i$의 분포값과 $y_ i$의 분포값이 유사해지도록 $y_ i$의 값을 바꿔가며 반복 계산

# 이상치 탐지

> 이상치란, 다른 데이터와 크게 달라서 다른 메커니즘에 의해 생성된 것이 아닌지 의심스러운 데이터

- 잡음, 노이즈와는 다름. 노이즈는 잘못된 값. 오차.
- 이상치는 유의미한 값

이상치 ==> 신규성

- 점 이상치 
    - 다른 데이터와 비교하여 차이가 큰 데이터
- 상황적 이상치
    - 상황에 맞지 않는 데이터
    - ex. 여름에는 25도가 정상이지만, 겨울에 25도는 이상치
- 집단적 이상치
    - 여러 데이터를 모아서 보면 비정상으로 보이는 데이터들의 집단

이를 통해

- 부정사용감지 시스템
- 침입탐지 시스템
- 시스템의 고장 진단
- 임상에서 질환 진단 및 모니터링
- 공공보건에서 유행병 탐지
- 스포츠 통계학에서 특이 사건 감지
- 관측 오류를 감지

할 수 있다

## 이상치 탐지 방법

### Isolation Forest 

> 여러 tree를 만들어서 앙상블을 구성. 

1. 비복원 추출로 데이터 중 일부 샘플링 
2. 데이터 𝑋의 변수 중 하나 𝑞를 무작위로 선택 
3. 변수 𝑞의 범위(min~max)에서 균등하고 분할위치 선택 
4. 1~3과정 일정회수 반복하여 트리 구성 

먼저 리프노드로 떨어져 나간 것은 이상치일 수 있음

이 이상치점수가 크면클수록 이상치일 것이다


### Local Outlier Factor 

> 데이터의 상대적인 밀도까지 고려한 이상치 탐지

𝑟𝑒𝑎𝑐ℎ𝑎𝑏𝑖𝑙𝑖𝑡𝑦−𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒𝑘(𝐴,𝐵) = max {𝑘−𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝐵), 𝑑𝑖𝑠𝑡(𝐴,𝐵)}

𝐴와 𝐵까지의 거리와 𝑘-𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒중 큰 값 사용
𝑘-𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 안에 들어오는 데이터들은 전부 원 밖으로 밀어내고 원 밖은 그대로 거리값 사용

(A에 k-distance 내 B들의 평균 local reachability density) / (A의 local reachability density)  

LOF : 1, 3은 작은 값 2는 큰값

# 반지도 학습

## 반지도 학습의 가정

- 평활성(smoothness, 平滑性) 가정 
    - 가까이 있는 점들은 서로 같은 부류에 속할 가능성이 높음
- 군집(cluster) 가정 
    - 같은 군집에 속하는 데이터는 동일한 부류에 속할 가능성이 높음
- 매니폴드(manifold)[^manifold] 가정  
    - 원래 차원보다 낮은 차원의 매니폴드에 데이터에 분포할 가능성이 높음  
- 대리 라벨 기법
    - 랩퍼 기법
    - 라벨이 있는 데이터로 학습된 모델을 이용해, 라벨이 없는 데이터에게 라벨을 부여
    - 모든 데이터(labeled, unlabeled)를 이용하여 모델 학습
- 그래프 기반 방법
    - 그래프의 노드로 labeled, unlabeled 데이터 표현
    - 노드 사이의 유사도를 기준으로 유사도 가중치가 높은 이웃으로 건너가며 라벨을 추정하는 라벨 전파 방식
- 미세변동 기반 방법
    - 데이터의 증강을 통해 부류가 바뀌지 않을 정도의 변화를 줬을 때도
    - 원 데이터와의 예측 결과가 같아지도록 함

[^manifold]: 국소적으로 유클리드(3차원 좌표계) 공간과 닮은 위상 공간이다. 즉, 국소적으로는 유클리드 공간과 구별할 수 없으나, 대역적으로 독특한 위상수학적 구조를 가질 수 있다.
