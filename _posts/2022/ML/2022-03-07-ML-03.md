---
layout: post
title: "ML, 3장 회귀, 추천"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/10
---

* Kramdown table of contents
{:toc .toc} 

# 회귀

> 학습 데이터에 부합되는 출력값이 실수인 함수를 찾는 문제

오차함수를 찾아, 오차가 작도록 하는 모델을 찾아야함

## 경사 하강법

> 오차 함수 $E$의 그레디언트(gradient)[^gradient] 반대 방향으로 조금씩 움직여 가며 최적의 파라미터를 찾으려는 방법

데이터의 입력과 출력을 이용하여 각 파라미터에 대한 그레디언트를 계산하여 파라미터를 반복적으로 조금씩 조정

learning late, 학습률은 작게, 그래야 최소값을 지나치지 않고 찾을 수 있음


[^gradient]: 각 파라미터에 대해 편미분한 벡터


## 지도학습

### 편향-분산 트레이드오프(bias-variance tradeoff)

#### 편향 (bias)

> 학습된 모델들의 평균적인 예측와 실제값의 차이

학습 알고리즘에서 모델의 형태에 잘못된 가정을 할 때 발생하는 오차

큰 편향값은 부적합(underfitting) 문제 초래

#### 분산 (variance) 

> 학습된 모델들의 예측값들의 평균적인 차이

학습 데이터에 내재된 작은 변동(fluctuation) 때문에 발생하는 오차

큰 분산값은 큰 잡음까지 학습하는 과적합(overfitting) 문제 초래

### 편향-분산 분해

실제모델을 모른체 관측값으로만 예측모델을만들어야함

이 관측치는 실제모델에서 오차치, 잡음이 추가된 값임

이때 여러 다양한 모델을 예측해 만들어 이들의 평균을 구함
$\bar{F}(x)$

이때 오차, MSE를 구하면 다음과 같다

$y - f(x)$ 이게 오차
이들의 기댓값 $E( y - f(x))$

@
\begin{align\*}
Err(x_ 0) &= E[y - f(x) | x = x_ 0]^ 2 \\\ 
&= E[f^*(x_ 0) + e - f(x_ 0)]^2 \\\ 
&= 
\end{align\*}
@


기댓값과 실제값의 차이의 평균은 0

16p

첫번째 임포트 다항식


# 추천

> 개인별로 맞춤형 정보를 제공하려는 기술

사용자에게 맞춤형 정보를 제공하여 정보 검색의 부하를 줄여주는 역할

`희소 행렬(sparse matrix) 형태`

비어있는 부분을 채워, 값이 큰것을 출력하는 것이 추천에 해당


## 내용 기반 추천, content-based filtering

> 고객이 이전에 높게 평가했던 것과 유사한 내용을 갖는 대상을 추천

항목별 데이터, 항목 프로파일이 있어야 한다

항목 프로파일 - 명시적 설계, 임베딩

비교할 대상, 내용이 있어야 비교를 함

고객이 뭘 좋아하는지도 알아야 함

cos 이용

두 값간의 코사인 값을 계산. 값이 클수록 유사


### 회귀를 사용한 내용 기반 추천

영화프로파일 * 사용자 프로파일을 계산해서 별점이 나와야 한다면, 이는 회귀식과 같다. 즉 회귀문제를 풀면 사용자가 좋아할 영화를 계산할 수 있다.


## 협력 필터링

유사도평가는 회귀처럼 코사인 거리 사용

@
cos(U_ i, U_ j) = \frac{U_ i \bullet U_ j}{|U_ i||U_ j|}
@



## 은닉 요소 모델

> 행렬 분해에 기반한 방법. 

분해된 행렬의 곱을 사용하여 추천 행렬의 빈 원소 결정

p24 행렬 분해


### ALS 알고리즘

@
r_ {i,u} = q^T_ i * p_ u
@

목적함수
$
min_ {p,q} \sum \limits_{observed r_{i,u}}^{} {(r_ {i,u} - q^T_ i p_ u )^2 + \lambda (|| p_ u ||^2 || q_ i ||^2)}
$

p에 대해서 구하고, q에 대해서 구한 다음 반복

하나만 쓰는게 아니라 여러개 쓰고 성능좋은거 채택

