---
layout: post
title: "ML, 8장 Gradient Boosting"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/8
  - /blog/ML/8
---

* Kramdown table of contents
{:toc .toc} 

# Gradient Boostring 

> 앙상블에 이전까지의 오차를 보정하도록 결정트리 모델을 순차적으로 추가

## 회귀



## 분류



-------------------
0502

# XGBoost

기본은 그레디언트 부스팅. 그레디언트부스팅 발전한게 XGBoost

그다음에 나온게 Lite~~

`Extreme Gradient Boosting`

새로운 트리에 너무 큰값이 들어가면 안되기에 규제화를 함



--------------------------


# Gradient Boosting


왜 그레디언트인가

평균 + 학습률 * 모델값 + 학습률 * 모델 ... 100개정도 

트리 앙상블 수랑, 단말 노드의 수는 하이퍼파라미터로 직접 줌

부류에서는 로그오드르 계산한다

잔차가 마이너스값이 나오기 떄문에, 마이너스는 확률이 되지않고 결정트리 단말노드는 0보다 작을수 없기 때문에
회귀에서는 그냥 사용했지만, 분류에서는 로그오드값을 사용한다

-------

트리를 만드는데 회귀트리. 단말이 숫자값이 나옴. 숫자값을 예측하는 회귀모델

단말노드가 부류가 되었는데, 부류가 아니라 숫자값이 나와야하고, 확률값은 음수가 나올수없기에 로그오드값을 사용함.
그 러면 로그오드값을 사용해 확률을 구할 수 있음

28p 회색부분

이 수식은 아래로 볼록이기에, 미분했을때 0인 부분이 최소값
따라서 미분하면 -2 + 3p가 되고, p가 2/3일때 최소임

따라서 odds 는 p / 1- p = 2/3 / 1/3 = 2 이고, log(odds) = log(2) = 0.69가 된다

# XGBoost

데이터가 너무 많아지면 속도가 느려짐 
-> 근사적인 방법으로 속도를 빠르게 한 방법


44p
refression 은 실제값. 
트리가 얼마나 좋은지 평가할 때, 잔차 제곱을 갯수+람다로 나누고는데
분류에서는 

46p
분류 손실함수는 로그오드값

L 뒷부분만 보면 되는데, Ovalue는 노드 각각의 결과값

트리를 만들긴 만들되, 새로만들 트리값이 너무 커지는건 안좋다
조금씩만 변하게 새로 만들겠다

Ovalue가 규제화, regulization

결국 아웃풋값은 다음페이지 와 같아야 한다

Ls가 최소가 되는 값은 Ovalue가 그 값일때 최소값
그래서 이 값을 원래식에 넣어서 전개를 하면 이렇게되서 파랑값이 나온다

즉 파랑색 값이 Ls가 최소가 되는 값
좋은 값은 클수록 보기 좋으니까 -2를 곱해준다

이렇게 나온게 similarity score


@
L = \sum\limits_ {i=1}^{n} L(y_ i, p_ i ^ 0 + O _ {value}) + \frac{1}{2} \lambda O_ {value} ^ 2
@

@
O _ {value} = -\frac{\sum \limits_ {i=1}^{n}g_ i}{\sum \limits_ {i=1}^{n} h_ i + \lambda}
@

@
S = \frac{(\sum \limits_ {i=1}^{n}g_ i )^2}{\sum \limits_ {i=1}^{n} h_ i + \lambda}
@


1000개 이상의 학습 데이터 100개 미만의 속성
속성 개수 < 학습 데이터 개수

미사용의 경우
이미지 인식 문제
자연어 처리 문제

## XGBoost Optimization

###  Approximate Greedy Algorithm 
Quantile 같은 데이터 개수만큼 범위를 나눔

XGBoost 는 33개로 나눔

###  Parallel learning and Weighted quantile sketch

데이터가 너무 커서 분산을 시킴

히스토그램을 그리는것도 오래걸리기에 Sketch 알고리즘 이용

### Sparsity-Aware Split Finding
### Cache-Aware Access
### Blocks for Out-of-Core Computation
### Random subsampling


# LightGBM

> XGBoost 대비하여 메모리 사용량 및 처리속도를 개선한 방법              
> 히스토그램 사용한 spilt 위치 결정

- Gradient-based One-side sampling (GOSS) 적용
- Exclusive Feature Bundling (EFB) 적용

트리를 만들땐 해당부분만 가져감 Leaf-wise tree growth




# CatBoost