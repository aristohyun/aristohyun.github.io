---
layout: post
title: "ML, 5ì¥ ì œì•½ì¡°ê±´ ìµœì í™”ì™€ ì´ì°¨ê³„íšë²•"
description: "ê¸°ê³„í•™ìŠµ, ì´ê±´ëª…êµìˆ˜ë‹˜"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, ì´ê±´ëª…]
use_math: true
redirect_from:
  - /ML/06
  - /blog/ML/06
---

* Kramdown table of contents
{:toc .toc} 

# í•¨ìˆ˜ ìµœì í™”

> ì–´ë–¤ ëª©ì  í•¨ìˆ˜(objective function)ê°€ ìˆì„ ë•Œ, ì´ í•¨ìˆ˜ë¥¼ ìµœëŒ€ë¡œ
í•˜ê±°ë‚˜ ìµœì†Œë¡œ í•˜ëŠ” ë³€ìˆ˜ ê°’ë¥¼ ì°¾ëŠ” ìµœì í™” ë¬¸ì œ

- ì˜¤ì°¨ë¥¼ ìµœì†Œí™” í•˜ë„ë¡ í•˜ëŠ” í•¨ìˆ˜
- ì†ì‹¤í•¨ìˆ˜, loss function ì„ ìµœì†Œë¡œ í•˜ëŠ” íŒŒë¼ë¯¸í„°, ê°€ì¤‘ì¹˜ë¥¼ ì°¾ëŠ” ë¬¸ì œ

# ê²½ì‚¬í•˜ê°•ë²•

> ë³µì¡í•œ í•¨ìˆ˜ì¸ ê²½ìš°, ì„ì˜ì˜ ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ì—¬         
> í•¨ìˆ˜ ğ‘“(ğ’™)ì˜ ê·¸ë ˆë””ì–¸íŠ¸(gradient) <red>ë°˜ëŒ€ ë°©í–¥</red>ìœ¼ë¡œ ì¡°ê¸ˆì”© ì›€ì§ì—¬ ê°€ë©° ìµœì ì˜ ë³€ìˆ˜ ê°’ë¥¼ ì°¾ìœ¼ë ¤ëŠ” ë°©ë²•

@
f(x) = f(x_ 1, x_ 2) \quad \triangledown f = (\frac{\partial f}{\partial x_ 1},\frac{\partial f}{\partial x_ 2})
@

íƒìƒ‰ ì¤‘ì˜ ë§¤ ìœ„ì¹˜ì—ì„œ ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì—¬, 

ê·¸ë ˆë””ì–¸íŠ¸ ë°˜ëŒ€ë°©í–¥ìœ¼ ë¡œ ì´ë™í•˜ë„ë¡ ë³€ìˆ˜ì˜ ê°’ì„ ë°˜ë³µì ìœ¼ë¡œ ì¡°ê¸ˆì”© ì¡°ì •

@
x_ 1 \leftarrow x_ 1 - \eta\frac{\partial f}{\partial x_ 1} \\\ 
x_ 2 \leftarrow x_ 2 - \eta\frac{\partial f}{\partial x_ 2}
@

$\eta$ : í•™ìŠµë¥ 

> ê·¸ëŸ¬ë‚˜ ë°˜ë“œì‹œ ìµœì í•´ë¥¼ ë³´ì¥í•  ìˆœ ì—†ë‹¤


# ì œì•½ì¡°ê±´ ìµœì í™”

> ì œì•½ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ì„œ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ë³€ìˆ˜ë“¤ì˜ ê°’ì„ ì°¾ëŠ” ë¬¸ì œ

ì œì•½ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°’ë“¤ = ê°€ëŠ¥í•´

## ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜

> ì œì•½ì¡°ê±´ë“¤ê³¼ ëª©ì í•¨ìˆ˜ë¥¼ ê²°í•©í•œ í•¨ìˆ˜

@
L(x_ 1, x_ 2, \lambda, \alpha) = f(x_ 1, x_ 2) + \lambda g(x_ 1, x_ 2) + \alpha h(x_ 1, x_ 2)
@

ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜
- $\lambda$ = ë“±ì‹     
- $\alpha$ = ë¶€ë“±ì‹, $\alpha \geq 0$


### ì œì•½ì¡°ê±´ ìµœì í™”

> $\lambda, \alpha$($\alpha \geq 0$) ë¥¼ ë§ˆìŒëŒ€ë¡œ ë°”ê¾¸ë©°, $L (x_ 1, x_ 2, \lambda, \alpha)$ì˜ ê°’ì„ ì•„ë¬´ë¦¬ í‚¤ì›Œë„      
> $min_ {x_ 1, x_ 2} max_ {\alpha, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)$ì˜ ê°’ì€ $x_ 1, x_ 2$ê°€ ê°€ëŠ¥í•´ ì¼ ë•Œ ë‚˜ì˜¨ë‹¤

@
min_ {x_ 1, x_ 2 \in FS} f(x_ 1, x_ 2) = min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)
@

FS : ê°€ëŠ¥í•´ ì§‘í•©

ì œì•½ì¡°ê±´ì´ ëª‡ê°œê°€ ìˆì„ ì§€ ëª¨ë¥´ëŠ”ë°, ì´ê²ƒë“¤ì˜ ê°€ëŠ¥í•´ë¥¼ ì°¾ì•„ì„œ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒë³´ë‹¤,

ì´ëª¨ë“ ê±¸ í•˜ë‚˜ì˜ ì‹ìœ¼ë¡œ ë§Œë“  ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ë¥¼ í•´ê²°í•˜ëŠ”ê²Œ ë” ì‰¬ìš¸ ìˆ˜ ìˆë‹¤

@
\begin{align\*}
 min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha) &\geq  max_ {\alpha \geq 0, \lambda} min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha) \\\ 
 &\geq max_ {\alpha \geq 0, \lambda} L_ d (\lambda, \alpha) 
\end{align\*}
@

$
L_ d (\lambda, \alpha) = min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha)
$ : ìŒëŒ€í•¨ìˆ˜

1. ë¼ê·¸ë‘ì£¼ í•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤
2. ìŒëŒ€í•¨ìˆ˜ë¥¼ ë§Œë“ ë‹¤
3. ìŒëŒ€í•¨ìˆ˜ë¥¼ ë§Œì¡±í•˜ëŠ” x1 x2ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„ì„ í•œë‹¤
4. ìŒëŒ€í•¨ìˆ˜ì— x1 x2ê°’ì„ ëŒ€ì…í•œë‹¤
5. ìŒëŒ€í•¨ìˆ˜ê°€ ìµœëŒ€ê°€ ë˜ëŠ” ëŒë‹¤ ì•ŒíŒŒë¥¼ êµ¬í•˜ê¸° ìœ„í•´, ê°ê°ì— ëŒ€í•´ í¸ë¯¸ë¶„ í•œë‹¤
6. ì›ë˜ì‹ì—ì„œ ëŒë‹¤ ì•ŒíŒŒë¥¼ ëŒ€ì…í•´ x1 x2ë¥¼ ì°¾ëŠ”ë‹¤


# ì´ì°¨ê³„íšë²•

> ëª©ì í•¨ìˆ˜ê°€ ë³¼ë¡ ì´ì°¨ì‹(convex quadratic)ì´ê³ , ì œì•½ì¡°ê±´ì´ ëª¨ë‘ ì¼ì°¨ì‹ì¸ ìµœì í™” ë¬¸

cvxopt íŒ¨í‚¤ì§€ì˜ quadratic programming solver 

$\alpha$ = cvxopt.solvers.qp(H, f, A, a, B, b) 


$
\underset{x}{min}\frac{1}{2}x^THx + f^Tx \\\ 
Ax \leq a \\\ 
Bx = b
$

@
\begin{align\*}
\text{minimize} \quad & 2x_ {1}^2 + 2x_ {2}^2  + x_ {1}x_ {2} + x_ {1} + x_ {2} \\\ 
\text{subject to} \quad & x_ 1 \geq 0 \\\ 
& x_ 2 \geq 0 \\\ 
& x_ 1 + x_ 2 = 1
\end{align\*}
@

~~~ python
H = 2*matrix([[2, .5], [.5, 1]]) # í¸ë¯¸ë¶„ í›„ x1 x2ê°’, [[x1 ê³„ìˆ˜, x2 ê³„ìˆ˜]]
f = matrix([1.0, 1.0]) # í¸ë¯¸ë¶„ í›„ ìƒìˆ˜ê°’, [x1 ìƒìˆ˜, x2 ìƒìˆ˜]
A = matrix([ [-1, 0], [0 ,-1]]) # Ax <= h
a = matrix([0, 0])
B = matrix([1 ,1], (1,2)) # Bx = b
b = matrix(1.0)
~~~

@
H = \begin{bmatrix}
4 & 1 \\\ 
1 & 4 \\\ 
\end{bmatrix}
\;
f = \begin{bmatrix}
1.0 \\\ 
1.0
\end{bmatrix} 
\
A = \begin{bmatrix}
-1 & 0 \\\ 
0 & -1 \\\ 
\end{bmatrix} 
\;
a = \begin{bmatrix}
0 \\\ 
0
\end{bmatrix} 
\;
B = \begin{bmatrix}
1 & 1 \\\ 
\end{bmatrix} 
\;
b = \begin{bmatrix}
1.0
\end{bmatrix} 
@