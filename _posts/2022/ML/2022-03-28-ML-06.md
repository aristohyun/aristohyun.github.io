---
layout: post
title: "ML, 5장 SVM"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/28
---

* Kramdown table of contents
{:toc .toc} 

# SVM

`Support Vector Machine`

> 분류 오차를 줄이면서 동시에 <red>여백을 최대</red>로 하는 결정 경계(decision boundary)를 찾는 <red>이진 분류기(binary classifier)</red>

- 여백(margin)
    - 결정 경계와 서포트 벡터까지의 거리
- 서포트 벡터(support vector)
    - 결정 경계로부터 가장 가까이에 있는 학습 데이터들


### 초평면[^hyperplane] 기하학

$
w^Tx + b = 0
$

@
\begin{align\*}
x &= x_ p + r\frac{w}{|| w ||} \\\ 
\\\ 
w^Tx &= w^Tx_ p + r\frac{w^T w}{||w||} \\\ 
&= w^Tx_ p + r||w|| \\\ 
\\\ 
w^Tx + b &= w^Tx_ p + b + r||w|| \\\ 
h(x) &= r||w|| \\\ 
r &= \frac{h(x)}{||w||} &&
\end{align\*}
@


[^hyperplane]: 4차원이상의 공간에서 선형 방정식으로 표현되는 결정 경계

## SVM 학습

조건 1.  $t_ i h(x_ i) \geq 1$

- 서포트 벡터 x'에서의 \|h(x')\| = 1    
- h(x) > 0 인 공간에 $t_ i$ = 1    
- h(x) < 0 인 공간에 $t_ i$ = -1    

조건 2. 서포트 벡터와의 거리, 즉 여백을 최대로 한다

- $r = \frac{h(x)}{\|\|w\|\|}$ 서포트 벡터에 대해서는 h(x) = 1     
- $r = \frac{1}{\|\|w\|\|}$    
- $\frac{1}{\|\|w\|\|}$ 을 최대화 해야하니, $\|\|w\|\|$를 최소화 해야한다    


## SVM 최적화 문제

라그랑주 함수를 최소화 하는 w,b를 구해야 한다

> Find w,b which minimizes $J(x) = \frac{1}{2} \|\|w\|\|^2$              
> sunject to $1 - t_ i h (x_ i) \leq 0, i = 1, \cdots, N$

이걸 라그랑주 함수로 표현하면

> Find w, b which minimizes $L(x,b,\alpha) = \frac{1}{2} ||w||^2 + \sum \limits_ {i=1}^{N} \alpha_ i (1-t_ i(w^Tx_ i + b))$       
> subject to $\alpha_ i \geq 0, i = 1, \cdots, N$

이걸 다시 w, b가 없는 쌍대 문제로 바꾸면

> Find $\alpha$ which maximizes  $\widetilde{L}(\alpha) - \frac{1}{2} \sum \limits_ {i=1}^{N} \sum \limits_ {j=1}^{N} \alpha_ i \alpha_ j t_ i t_ j x_ i \cdot x_ j + \sum \limits_ {i=1}^{N} \alpha_ i$    
> subject to $\sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0, i = 1, \cdots, N$
> $\alpha_ i \geq 0,  i = 1, \cdots, N$

#### 유도 식

이건 즉 SVM의 최적화 문제와 같음

$L(x, \alpha, \lambda) = f(x) + \lambda g(x) + \alpha h(x)$

$L(w, b, \alpha) = \frac{1}{2} \|\|w\|\|^2 + \sum \limits_ {i=1}^{N} \alpha_ i (1-t_ i(w^Tx_ i + b))$

이때, w,b에 대해 편미분 했을 때 0이 되어야 한다 (해에 대한KKT 조건)

$ \frac{\partial L}{\partial w} = 0, \frac{\partial L}{\partial b} = 0 $

@
\frac{\partial L}{\partial w} = w - \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i = 0 \\\ 
\frac{\partial L}{\partial b} = - \sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0  \\\ 
\therefore w = \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i , \sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0
@

초평면 함수식    
$h(x) = w^Tx + b = \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i \cdot x + b $ 


~~~ python
sol = cvxopt.solvers.qp(H, f, A, a, B, b) # sol['x'][i]가 0이 아닌 값이 서포트 벡터

# 앞선 식을 정리하다보면 w가 계산되고 (w는 서포트 벡터의 기울기)
# 이렇게 나온 서포트 벡터 하나를 t_i(w^Tx_i + b) = 1 에 넣어 b(절편)를 계산한다
~~~

@
H = \begin{bmatrix}
t_ 1 t_ 1 x_ 1 \cdot x_ 1 & t_ 1 t_ 2 x_ 1 \cdot x_ 1 & \cdots & t_ 1 t_ N x_ 1 \cdot x_ N \\\ 
t_ 2 t_ 1 x_ 2 \cdot x_ 1& t_ 2 t_ 2 x_ 2 \cdot x_ 2 & \cdots & t_ 2 t_ N x_ 2 \cdot x_ N \\\ 
\vdots & \vdots & \ddots  & \vdots \\\ 
t_ N t_ 1 x_ N \cdot x_ 1 & t_ N t_ 2 x_ N \cdot x_ 2  & \cdots & t_ N t_ N x_ N \cdot x_ N \\\ 
\end{bmatrix} = \begin{bmatrix}
t_ 1 \\\ 
t_ 2 \\\ 
\vdots \\\ 
t_ N
\end{bmatrix}
\begin{bmatrix}
t_ 1 \\\ 
t_ 2 \\\ 
\vdots \\\ 
t_ N
\end{bmatrix}^T \cdot^* \begin{bmatrix}
x_ 1 \\\ 
x_ 2 \\\ 
\vdots \\\ 
x_ N
\end{bmatrix}\begin{bmatrix}
x_ 1 \\\ 
x_ 2 \\\ 
\vdots \\\ 
x_ N
\end{bmatrix}^T
@

@
f=\begin{bmatrix}
-1 \\\ 
-1 \\\ 
-1 \\\ 
-1
\end{bmatrix} 
A = \begin{bmatrix}
-1 & 0 & \cdots & 0 \\\ 
0 & -1 & \cdots & 0 \\\ 
\vdots & \vdots & \ddots & \vdots \\\ 
0 & 0 & \cdots & -1 \\\ 
\end{bmatrix} 
a = \begin{bmatrix}
0 \\\ 
0 \\\ 
\vdots \\\ 
0
\end{bmatrix} 
B = \begin{bmatrix}
t_ 1 & t_ 2 & \cdots & t_ N \\\ 
\end{bmatrix} 
b = \begin{bmatrix}
0
\end{bmatrix} 
@


# 선형 분리불가 문제의 SVM

`슬랙변수` $\xi_ i$

- 학습 데이터별로 하나씩 생성
- 분할 초평면에서 서포트 벡터보다 멀리 있는 올바르게 분류 된 데이터는 0
- 그렇지 않으면 $\xi_ i = \| t_ i - h(x_ i) \|$ 

$t_ i h(x_ i) \geq 1 - \xi_ i, i=1, \cdots, N$


~~~ python
svc = LinearSVC(C=0.5) # C는 상한에 대한 제약조건
~~~

# 비선형 SVM

> 데이터를 고차원으로 사상하여 선형 분리를 하자

but 저장 공간, 계산 비용의 증가가 문제임

-> 커널 트릭 사용으로 해결함

## 커널 트릭과 커널 함수

> $\Phi(x_ i) \dot \Phi(x_ j)$를 고차원으로 변환하여 계산하지 않고, 원래 데이터에서 계산              
> 이때, 고차원 변환없이 계산할 수 있는 커널 함수 K 사용

@
K(x_ i, x_ j) = \Phi(x_ i) \dot \Phi(x_ j)
@

- 선형 커널 $x_ i ^ T x_ j$
- 다항식 커널 $(x_ i ^ T x_ j)^d$
- RBF 커널 $e^{\gamma \|\|x_ i - x_ j \|\|^2 / (2\sigma^2)}$
- 시그모이드 , 쌍곡 탄젠트 커널 $\text{tanh}(\gamma(x_ i ^ T x_ j) + \theta)$


# 다중 부류 분류

## 이진 분류기를 이용한 다중 부류 분류 (multiclass classification)

### 일대일 방법 one vs one approach, OVO

- $C_p (q)$ : 부류 p와 q를 비교하여 p면 1 아니면 -1
- N개 부류가 있는 문제에 대해 N(N-1)/2 개의 분류기 학습
- 최종 부류 : 가장 빈번하게 나오는 부류

@
C = arg \, max_ p \sum_ q C_ p (q)
@

### 일대나머지 방법 one vs all approach, OVA

- 부류별 분류기 학습
- 각 학습기별로 데이터 x에 대한 $h(x) = \sum \limits_ {i=1}^{N} a_ i t_ i K(x_ i, x) + b$ 계산
- 최종 부류 : 가장 큰 값을 갖는 부류