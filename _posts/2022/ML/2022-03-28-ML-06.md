---
layout: post
title: "ML, 5장 제약조건 최적화와 이차계획법"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/06
  - /blog/ML/06
---

* Kramdown table of contents
{:toc .toc} 

# 함수 최적화

> 어떤 목적 함수(objective function)가 있을 때, 이 함수를 최대로
하거나 최소로 하는 변수 값를 찾는 최적화 문제

- 오차를 최소화 하도록 하는 함수
- 손실함수, loss function 을 최소로 하는 파라미터, 가중치를 찾는 문제

# 경사하강법

> 복잡한 함수인 경우, 임의의 위치에서 시작하여         
> 함수 𝑓(𝒙)의 그레디언트(gradient) <red>반대 방향</red>으로 조금씩 움직여 가며 최적의 변수 값를 찾으려는 방법

@
f(x) = f(x_ 1, x_ 2) \quad \triangledown f = (\frac{\partial f}{\partial x_ 1},\frac{\partial f}{\partial x_ 2})
@

탐색 중의 매 위치에서 그레디언트를 계산하여, 

그레디언트 반대방향으 로 이동하도록 변수의 값을 반복적으로 조금씩 조정

@
x_ 1 \leftarrow x_ 1 - \eta\frac{\partial f}{\partial x_ 1} \\\ 
x_ 2 \leftarrow x_ 2 - \eta\frac{\partial f}{\partial x_ 2}
@

$\eta$ : 학습률

> 그러나 반드시 최적해를 보장할 순 없다


# 제약조건 최적화

> 제약조건을 만족하면서 목적함수를 최적화하는 변수들의 값을 찾는 문제

제약조건을 만족하는 값들 = 가능해

## 라그랑주 함수

> 제약조건들과 목적함수를 결합한 함수

@
L(x_ 1, x_ 2, \lambda, \alpha) = f(x_ 1, x_ 2) + \lambda g(x_ 1, x_ 2) + \alpha h(x_ 1, x_ 2)
@

라그랑주 승수
- $\lambda$ = 등식     
- $\alpha$ = 부등식, $\alpha \geq 0$


### 제약조건 최적화

> $\lambda, \alpha$($\alpha \geq 0$) 를 마음대로 바꾸며, $L (x_ 1, x_ 2, \lambda, \alpha)$의 값을 아무리 키워도      
> $min_ {x_ 1, x_ 2} max_ {\alpha, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)$의 값은 $x_ 1, x_ 2$가 가능해 일 때 나온다

@
min_ {x_ 1, x_ 2 \in FS} f(x_ 1, x_ 2) = min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)
@

FS : 가능해 집합

제약조건이 몇개가 있을 지 모르는데, 이것들의 가능해를 찾아서 목적함수를 최소화 하는것보다,

이모든걸 하나의 식으로 만든 라그랑주 함수를 해결하는게 더 쉬울 수 있다

@
\begin{align\*}
 min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha) &\geq  max_ {\alpha \geq 0, \lambda} min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha) \\\ 
 &\geq max_ {\alpha \geq 0, \lambda} L_ d (\lambda, \alpha) 
\end{align\*}
@

$
L_ d (\lambda, \alpha) = min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha)
$ : 쌍대함수

1. 라그랑주 함수를 구한다
2. 쌍대함수를 만든다
3. 쌍대함수를 만족하는 x1 x2를 구하기 위해 각각에 대해 편미분을 한다
4. 쌍대함수에 x1 x2값을 대입한다
5. 쌍대함수가 최대가 되는 람다 알파를 구하기 위해, 각각에 대해 편미분 한다
6. 원래식에서 람다 알파를 대입해 x1 x2를 찾는다


# 이차계획법

> 목적함수가 볼록 이차식(convex quadratic)이고, 제약조건이 모두 일차식인 최적화 문

cvxopt 패키지의 quadratic programming solver 

$\alpha$ = cvxopt.solvers.qp(H, f, A, a, B, b) 


$
\underset{x}{min}\frac{1}{2}x^THx + f^Tx \\\ 
Ax \leq a \\\ 
Bx = b
$

@
\begin{align\*}
\text{minimize} \quad & 2x_ {1}^2 + 2x_ {2}^2  + x_ {1}x_ {2} + x_ {1} + x_ {2} \\\ 
\text{subject to} \quad & x_ 1 \geq 0 \\\ 
& x_ 2 \geq 0 \\\ 
& x_ 1 + x_ 2 = 1
\end{align\*}
@

~~~ python
H = 2*matrix([[2, .5], [.5, 1]]) # 편미분 후 x1 x2값, [[x1 계수, x2 계수]]
f = matrix([1.0, 1.0]) # 편미분 후 상수값, [x1 상수, x2 상수]
A = matrix([ [-1, 0], [0 ,-1]]) # Ax <= h
a = matrix([0, 0])
B = matrix([1 ,1], (1,2)) # Bx = b
b = matrix(1.0)
~~~

@
H = \begin{bmatrix}
4 & 1 \\\ 
1 & 4 \\\ 
\end{bmatrix}
\;
f = \begin{bmatrix}
1.0 \\\ 
1.0
\end{bmatrix} 
\
A = \begin{bmatrix}
-1 & 0 \\\ 
0 & -1 \\\ 
\end{bmatrix} 
\;
a = \begin{bmatrix}
0 \\\ 
0
\end{bmatrix} 
\;
B = \begin{bmatrix}
1 & 1 \\\ 
\end{bmatrix} 
\;
b = \begin{bmatrix}
1.0
\end{bmatrix} 
@