---
layout: post
title: "ML, 8장 Gradient Boosting"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/10
  - /blog/ML/10
---

* Kramdown table of contents
{:toc .toc} 

# Gradient Boostring 

> 앙상블에 이전까지의 오차를 보정하도록 결정트리 모델을 순차적으로 추가

## 회귀



## 분류

--------------------------


# Gradient Boosting


왜 그레디언트인가

평균 + 학습률 * 모델값 + 학습률 * 모델 ... 100개정도 

트리 앙상블 수랑, 단말 노드의 수는 하이퍼파라미터로 직접 줌

부류에서는 로그오드르 계산한다

잔차가 마이너스값이 나오기 떄문에, 마이너스는 확률이 되지않고 결정트리 단말노드는 0보다 작을수 없기 때문에
회귀에서는 그냥 사용했지만, 분류에서는 로그오드값을 사용한다

-------

트리를 만드는데 회귀트리. 단말이 숫자값이 나옴. 숫자값을 예측하는 회귀모델

단말노드가 부류가 되었는데, 부류가 아니라 숫자값이 나와야하고, 확률값은 음수가 나올수없기에 로그오드값을 사용함.
그 러면 로그오드값을 사용해 확률을 구할 수 있음

28p 회색부분

이 수식은 아래로 볼록이기에, 미분했을때 0인 부분이 최소값
따라서 미분하면 -2 + 3p가 되고, p가 2/3일때 최소임

따라서 odds 는 p / 1- p = 2/3 / 1/3 = 2 이고, log(odds) = log(2) = 0.69가 된다

# XGBoost

데이터가 너무 많아지면 속도가 느려짐 
-> 근사적인 방법으로 속도를 빠르게 한 방법


44p
refression 은 실제값. 
트리가 얼마나 좋은지 평가할 때, 잔차 제곱을 갯수+람다로 나누고는데
분류에서는 

46p
분류 손실함수는 로그오드값

L 뒷부분만 보면 되는데, Ovalue는 노드 각각의 결과값

트리를 만들긴 만들되, 새로만들 트리값이 너무 커지는건 안좋다
조금씩만 변하게 새로 만들겠다

Ovalue가 규제화, regulization

결국 아웃풋값은 다음페이지 와 같아야 한다

Ls가 최소가 되는 값은 Ovalue가 그 값일때 최소값
그래서 이 값을 원래식에 넣어서 전개를 하면 이렇게되서 파랑값이 나온다

즉 파랑색 값이 Ls가 최소가 되는 값
좋은 값은 클수록 보기 좋으니까 -2를 곱해준다

이렇게 나온게 similarity score




