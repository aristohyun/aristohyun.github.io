---
layout: post
title: "ML, 5장 SVM"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /2022/03/28
---

* Kramdown table of contents
{:toc .toc} 

# SVM

`Support Vector Machine`

> 분류 오차를 줄이면서 동시에 <red>여백을 최대</red>로 하는 결정 경계(decision boundary)를 찾는 <red>이진 분류기(binary classifier)</red>

- 여백(margin)
    - 결정 경계와 서포트 벡터까지의 거리
- 서포트 벡터(support vector)
    - 결정 경계로부터 가장 가까이에 있는 학습 데이터들


### 초평면[^hyperplane] 기하학

$
w^Tx + b = 0
$

@
\begin{flalign\*}

x &= x_ p + r\frac{w}{|| w ||} \\\ 
w^Tx &= w^Tx_ p + r\frac{w^T w}{||w||} \\\ 
&= w^Tx_ p + r||w|| \\\ 
w^Tx + b &= w^Tx_ p + b + r||w|| \\\ 
h(x) &= r||w|| \\\ 
r &= \frac{h(x)}{||w||} &&

\end{flalign\*}
@


[^hyperplane]: 4차원이상의 공간에서 선형 방정식으로 표현되는 결정 경계

## SVM 학습

조건 1.  $t_ i h(x_ i) \geq 1$

- 서포트 벡터 x'에서의 |h(x')| = 1
- h(x) > 0 인 공간에 $t_ i$ = 1
- h(x) < 0 인 공간에 $t_ i$ = -1

조건 2. 서포트 벡터와의 거리, 즉 여백을 최대로 한다

- $r = \frac{h(x)}{||w||}$ 서포트 벡터에 대해서는 h(x) = 1
- $r = \frac{1}{||w||}$
- $\frac{1}{||w||}$ 을 최대화 해야하니, ||w||를 최소화 해야한다


## SVM 최적화 문제

# 선형 분리불가 문제의 SVM

# 비선형 SVM

## 커널 트릭과 커널 함수

# 다중 부류 분류