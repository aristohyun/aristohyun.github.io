---
layout: post
title: "ML, 9장 신경망"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/9
  - /blog/ML/9
---

* Kramdown table of contents
{:toc .toc} 


# 신경망
 
> 신경망(neural network, artificial neural network)             
> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야


신경망 안에 딥뉴럴네트워크가 있음

한 뉴런의 축색돌기 끝과 다른 뉴런의 수상돌기가 맞닿아 있는것을 신경연접이라고 함

![image](https://user-images.githubusercontent.com/32366711/173879932-891d44b2-15d7-44e3-a2f7-a754e3a1a336.png)



# 퍼셉트론

> 로젠블랏이 제안한 학습가능한 신경망 모델

입력신호에 가중치를 주어 출력값을 조절하는 방식

@
s = \sum \limits_ {i=1}^{d} w_ i x_ i + b = \sum \limits_ {i=0}^{d} w_ i x_ i
@

![image](https://user-images.githubusercontent.com/32366711/173879952-5c19c153-6462-4923-8a71-20c215b7360a.png)

~~~ python
# 간단한 뉴런 하나의 계산
def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0
    return activation
~~~

#### OR 연산

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b ) = x_ 1 + x_ 2 - 0.5
@

#### AND 연산

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b ) = x_ 1 + x_ 2 - 1.5
@

## 문제점

선형 분리가능문제만 해결할 수 있음 (3차원일땐 면)           
-> 선형 분리불가 문제, 간단한 XOR문제도 해결 못하더라

# 다층 퍼셉트론

> 여러 개의 퍼셉트론을 층 구조로 구성한 신경망 모델
![image](https://user-images.githubusercontent.com/32366711/173880022-fd21bd18-f498-4243-a034-96c6fae41fe5.png)

OR 뉴런과 NAND 뉴런을 AND 뉴런으로 합치면 XOR 완성

-> 파라미터를 자동으로 결정할 방법이 없다       
-> 학습을 시킬 수 없다          

학습?

> 입력-출력 ($x_ i$, $y_ i$ )의 학습 데이터에 대해서,       
> 기대출력 $y_ i$와 다층 퍼셉트론의 출력 $f(x_ 𝑖)$의 차이, 즉 오차(error)가 최소가 되도록 가중치 $w$를 결정하는 것 

학습 가능한 다층 퍼셉트론 

그러면 오차 역전파(error backpropagation) 알고리즘을 써보자

활성화 함수를 계단 함수에서 미분가능한 시그모이드 함수로 대체 

경사 하강법 적용 

## 활성화 함수

#### 계단
~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0
~~~

#### 시그모이드

$
\begin{align\*}
\sigma(x,a) &= \frac{1}{1 + e^{-ax}} \\\ 
\sigma'(x,a) &= a\sigma(x,a)(1-\sigma(x,a))
\end{align\*}
$

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x, a=1):  # 미분값을 따로 계산할 필요가 없다
    return a*sigmoid(x,a)*(1 - sigmoid(x,a))
~~~

#### 쌍곡 탄젠트함수

$
\begin{align\*}
\tanh(x,a) &= \frac{e^{2x} - 1}{e^{2x} + 1} \\\ 
\tanh'(x,a) &= 1 - tanh^2(x)
\end{align\*}
$

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

## 동작

![image](https://user-images.githubusercontent.com/32366711/173880082-070277bd-cafb-4336-ac68-b578a10a2eb7.png)

# 미분

> 함수 𝑓(𝑥)의 변수 𝑥에 대한 순간변화율     
> 𝑥의 아주 미세한 변화에 대한 𝑓(𝑥)의 변화량     

@
f'(x) = \frac{d f(x)}{dx} = \underset{\triangle x \rightarrow 0}{lim} \frac{f(x + \triangle x) - f(x)}{\triangle x}
@

## 연쇄 법칙 Chain Rule

## 편미분

> 다변수 함수에 대하여, 그 중 `하나의 변수`에 주목하고 나머지 변수의 값을 고정시켜 놓고 그 변수에 대해 하는 미분 

## 다변수 함수의 연쇄 법칙

## 그레디언트

어느방향으로 얼마나 커지는가

오차가 최소가되는 값을 찾아야하니까 그레디언트 반대방향으로

# 다층 퍼셉트론의 학습

-> `오차 역전파 알고리즘`

![image](https://user-images.githubusercontent.com/32366711/173880205-145ecdc8-80b6-4541-8cdf-b6cb09396490.png)

입력 노드 d개를 통해 히든 노드 1개(j)의 값을 구할 수 있음. 이렇게 히든 노드 총 p개를 구함
$
zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji} x_ i + u_ {j0} \\\ 
z_ j = f(zsum_ j)
$

입력값 x와 가중치 u의 곱의 총합 (zsum)을 구한 후 활성화 함수를 거쳐(f(zsum)) 노드 z의 값을 얻을 수 있음

히든 노드 p개를 통해 출력 노드 1개(j)의 값을 구할 수 있음. 이렇게 총 출력 노드는 m개를 구함

$
osum_ k = \sum \limits_ {j=1}^{p} v_ {kj} z_ j + v_ {k0} \\\ 
$

마찬가지로 노드 z를 x처럼, 가중치 v를 가중치 u 처럼 생각해        
osum과 출력값 o를 구할 수 있음

이렇게 임의의 가중치를 통해 예측값 o를 구했으면, 원래 출력값인 y와 비교해 오차(오차 제곱합)를 구할 수 있음

$
E = \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2
$

다음의 오차함수를 통해 그레디언트 디센트 방식을 통해 가중치를 조정하려함.

$
v^{t+1} = v^{t} - \eta \frac{\partial E}{\partial v} \\\ 
u^{t+1} = u^{t} - \eta \frac{\partial E}{\partial u}
$

다음의 오차함수의 v의 미분값과 u의 미분값만 구하면 됨

다음을 구하기 위해 체인룰을 적용

먼저 오차부터 역으로 오는 것이기에 v에 대해 먼저 계산해야 하는데          
p개의 z마다 m개의 v가 있음. j번째 z의 ($z_ j$) k번째 가중치($v_ kj$)에 대해서 오차역전파를 계산하려고 한다면, (k번째 가중치로 생성되는 결과는 $o_ k$, 모든 z의 k번째 가중치와의 곱의 합)

$
\frac{\partial E}{\partial v_ {kj}} = \frac{\partial E}{\partial o_ k} \frac{\partial o_ k}{\partial v_ {kj}} \\\ 
\frac{\partial E}{\partial o_ k} = \frac{\partial}{\partial o_ k} \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2 = o_ k - t_ k \\\ 
\frac{\partial o_ k}{\partial v_ {kj}} = \frac{\partial}{\partial v_ {kj}} \sum \limits_ {j=1}^{p} v_ {kj} z_ j + v_ {k0}  = z_ j * f'(osum_ k) \\\ 
\therefore \frac{\partial E}{\partial v_ {kj}} = (o_ k - t_ k) * z_ j * f'(osum_ k)
$

마찬가지로 d개의 x마다 p개의 u가 있음. i번째 x의 ($x_ i$) j번째 가중치($u_ ji$)에 대해서 오차역전파를 계산하려고 한다면, (j번째 가중치로 생성되는 결과는 $z_ j$, 모든 x의 j번째 가중치와의 곱의 합)

$
\frac{\partial E}{\partial u_ {ji}} = \frac{\partial E}{\partial z_ j} \frac{\partial z_ j}{\partial u_ {ji}} \\\ 
\frac{\partial E}{\partial z_ j} = \sum \limits_ {k=1}^{m} \frac{\partial E}{\partial o_ k}\frac{\partial o_ k}{\partial z_ j}  \\\ 
\sum \limits_ {k=1}^{m} (o_ k - t_ k) f'(osum_ k) v_ {kj} \\\ 
\frac{\partial z_ j}{\partial u_ {ji}} = \frac{\partial}{\partial u_ {ji}} zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji} x_ i + u_ {j0} = x_ i * f'(zsum_ j) \\\ 
\therefore \frac{\partial E}{\partial v_ {kj}} = \sum \limits_ {i=1}^{d} (o_ k - t_ k) * v_ {kj} * f'(osum_ k) * x_ i * f'(zsum_ j)
$


# 분류 문제

## 종류

## 출력값 표현

분류 수 만큼 자리수가 있고, 해당하는 자리의 값이 1이면 원핫인코딩, 이게 좀더 자주쓰인다


~~~ python
data = ["cold", "cold", "warm", "hot", "hot", "cold"] # 문자열, 기호
label_encoder = LabelEncoder() # 정수 인코딩
onethot_encoder = OneHotEncoder(sparse=False) # 원핫 인코딩
~~~

# 오차 함수, 손실 함수

> 기대하는 출력과 모델의 출력 차이를 축적하여 표현하는 함수

## 회귀 문제의 오차 함수

$
E = \frac{1}{2} \sum \limits_ {i=1}^{N} (y(x_ i, w) - t_ i)^2
$

## 이진 분류 문제의 오차 함수

활성화 함수로 시그모이드 함수를 사용

-> 구간 (0,1) 사이의 값을 출력함으로써 확률로 해석 가능하다

y(x,w)는 조건부 확률 p($C_ 1$\|x), 1- y(x,w)는 조건부 확률 p($C_ 2$\|x), 

따라서 입력 x와 가중치 w에 대한 목표값 t에 대한 조건부 확률은

$
p(t \| x,w) = y(x,w)^t \{1-y(x,w)\}^{1-t}
$

#### 가능도, likelihood

$
p(D;w) = \prod \limits_ {i=1}^{N} y(x,w)^t \{1-y(x,w)\}^{1-t}
$

#### 오차 함수

= 음의 로그 가능도, 크로스 엔트로피

$
\begin{align\*}
E(w) &= -log \prod \limits_ {i=1}^{N} y(x,w)^t \{1-y(x,w)\}^{1-t} \\\ 
 &= - \sum \limits_ {i=1}^{N} (t_ i log y(w_ i, w) + (1-t_ i)log (1-y(x_ i, w)))
\end{align\*}
$

## 다부류 분류 문제의 오차함수

출력의 합은 1

#### 소프트맥스 층

최종 출력을 분류 함수로 변환하는 층

$
y_ k = \frac{e^{z_ k}}{\sum \limits_ {i=1}^{K} e^{z_ i}
$

~~~ python
def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

학습 데이터 $(x_ i, t_ i)$의 조건부 확률
$
p(t_ i \| x_ i, w) = \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik}
$

전체 데이터 D에 대한 가능도

$
p(D;w) = \prod \limits_ {i=1}^{N} \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik}
$

> 데이터의 가능도를 최대로 하는 파라미터 w를 추정하는것이 목표

$
\underset{w}{argmax} p(D;w)
$

#### 오차함수 E(w)

 = 가능도의 음의 로그 가능도

$
E(w) = - log \prod \limits_ {i=1}^{N} \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik} = -\sum \limits_ {i=1}^{N}\sum \limits_ {k=1}^{K} t_ {ik} log y_ k (x_ i, w)
$

#### 오차함수에 대한 교차 엔트로피
$
E(w) = - \sum \limits_ {K}{k=1} t_ {ik} log y_ k (x_ i, w)
$

#### MSE, mean squared error

$
E = \frac{1}{2} \sum \limits_ {n}^{k=1} (o_ k - y_ k)^2
$


# RBF 망

## RBF 함수

> 기존 벡터 $\mu$와 입력 벡터 x의 유사도를 측정하는 함수

$
\phi (x, \mu) = exp(-\beta \|\| x - \mu \|\|^2)
$


가우시안이랑 비슷하게생겼는데, 가우시안은 최대가 1이아님

평균과 표준편차에 대한 정보를 조절해서

함수 공간에서 여러 그래프를 결합해서 사용

## RBF 망

> 어떤 함수 $f_ k (x)$를 다음과 같이 RBF 함수들의 선형 결합 형태로 근사시키는 모델

$
f_ k (x) \approx \sum \limits_ {i=1}^{N}w_ {kj}\phi_ i (x, \mu_ i) + b_ k
$

$
o_ k = \sum \limits_ {j=1}^{N} w_ {kj} \phi_ j + b_ k
$

### RBF 망의 학습

#### 오차 함수 E
$
E = \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2
$

#### 경사 하강법 사용

> 기준 벡터 $\mu_ j$와 파라미터 $\beta_ j$, 가중치 $w_ {kj}$ 결정

부류별 군집화 결과를 사용한 기준 벡터 $\mu_ j$와 파라미터 $\beta_ j$ 초기화

군집의 중심은 기준 벡터 $\mu_ j$

분산의 역수 $\beta_ j$

$
\sigma = \frac{1}{m} \sum \limits_ {i=1}^{m} \|\| x_ i - \mu \|\|  \;\;\;\; \;\;\;\; \beta = \frac{1}{2\sigma ^2}
$

![image](https://user-images.githubusercontent.com/32366711/173880379-5e610745-6cc7-468b-8013-8defab04e3c9.png)

~~~ python
def basisFunc(self, c, d):
    assert len(d) == self.indim
    return np.exp(-self.beta * norm(c-d)**2)

def activationFunc(self, X):
    G = np.zeros((X.shape[0], self.numCenters), float)
    for ci, c in enumerate(self.centers):
        for xi, x in enumerate(X):
            G[x1, c1] = self.basisFunc(c, x)

def train(self, X, Y):
    rnd_idx = random.permutation(X.shape[0])[:self.numCenters]
    self.centers = [X[i,:] for i in rnd_idx]
    G = self.activationFunc(X)
    self.W = np.dot(pniv(G), Y)

def predict(self, X):
    G = self.activationFunc(X)
    Y = np.dot(G, self.W)
    return Y
~~~


## 학습

최소가되는 오차함수를 정의해서 경사하강법 적용


기준 뮤와 베타값을 초기화해야함

군집의 중심 

norm 벡터의 정규화는 크기, 차이의 크기의 평균

