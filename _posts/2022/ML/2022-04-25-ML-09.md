---
layout: post
title: "ML, 9ì¥ ì‹ ê²½ë§"
description: "ê¸°ê³„í•™ìŠµ, ì´ê±´ëª…êµìˆ˜ë‹˜"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, ì´ê±´ëª…]
use_math: true
redirect_from:
  - /ML/9
  - /blog/ML/9
---

* Kramdown table of contents
{:toc .toc} 


# ì‹ ê²½ë§
 
> ì‹ ê²½ë§(neural network, artificial neural network)             
> ì¸ê°„ ë‘ë‡Œì— ëŒ€í•œ ê³„ì‚°ì  ëª¨ë¸ì„ í†µí•´ ì¸ê³µì§€ëŠ¥ì„ êµ¬í˜„í•˜ë ¤ëŠ” ë¶„ì•¼


ì‹ ê²½ë§ ì•ˆì— ë”¥ë‰´ëŸ´ë„¤íŠ¸ì›Œí¬ê°€ ìˆìŒ

í•œ ë‰´ëŸ°ì˜ ì¶•ìƒ‰ëŒê¸° ëê³¼ ë‹¤ë¥¸ ë‰´ëŸ°ì˜ ìˆ˜ìƒëŒê¸°ê°€ ë§ë‹¿ì•„ ìˆëŠ”ê²ƒì„ ì‹ ê²½ì—°ì ‘ì´ë¼ê³  í•¨

![image](https://user-images.githubusercontent.com/32366711/173879932-891d44b2-15d7-44e3-a2f7-a754e3a1a336.png)



# í¼ì…‰íŠ¸ë¡ 

> ë¡œì  ë¸”ëì´ ì œì•ˆí•œ í•™ìŠµê°€ëŠ¥í•œ ì‹ ê²½ë§ ëª¨ë¸

ì…ë ¥ì‹ í˜¸ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ì¶œë ¥ê°’ì„ ì¡°ì ˆí•˜ëŠ” ë°©ì‹

@
s = \sum \limits_ {i=1}^{d} w_ i x_ i + b = \sum \limits_ {i=0}^{d} w_ i x_ i
@

![image](https://user-images.githubusercontent.com/32366711/173879952-5c19c153-6462-4923-8a71-20c215b7360a.png)

~~~ python
# ê°„ë‹¨í•œ ë‰´ëŸ° í•˜ë‚˜ì˜ ê³„ì‚°
def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0
    return activation
~~~

#### OR ì—°ì‚°

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b ) = x_ 1 + x_ 2 - 0.5
@

#### AND ì—°ì‚°

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b ) = x_ 1 + x_ 2 - 1.5
@

## ë¬¸ì œì 

ì„ í˜• ë¶„ë¦¬ê°€ëŠ¥ë¬¸ì œë§Œ í•´ê²°í•  ìˆ˜ ìˆìŒ (3ì°¨ì›ì¼ë• ë©´)           
-> ì„ í˜• ë¶„ë¦¬ë¶ˆê°€ ë¬¸ì œ, ê°„ë‹¨í•œ XORë¬¸ì œë„ í•´ê²° ëª»í•˜ë”ë¼

# ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ 

> ì—¬ëŸ¬ ê°œì˜ í¼ì…‰íŠ¸ë¡ ì„ ì¸µ êµ¬ì¡°ë¡œ êµ¬ì„±í•œ ì‹ ê²½ë§ ëª¨ë¸
![image](https://user-images.githubusercontent.com/32366711/173880022-fd21bd18-f498-4243-a034-96c6fae41fe5.png)

OR ë‰´ëŸ°ê³¼ NAND ë‰´ëŸ°ì„ AND ë‰´ëŸ°ìœ¼ë¡œ í•©ì¹˜ë©´ XOR ì™„ì„±

-> íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ê²°ì •í•  ë°©ë²•ì´ ì—†ë‹¤       
-> í•™ìŠµì„ ì‹œí‚¬ ìˆ˜ ì—†ë‹¤          

í•™ìŠµ?

> ì…ë ¥-ì¶œë ¥ ($x_ i$, $y_ i$ )ì˜ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œ,       
> ê¸°ëŒ€ì¶œë ¥ $y_ i$ì™€ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì˜ ì¶œë ¥ $f(x_ ğ‘–)$ì˜ ì°¨ì´, ì¦‰ ì˜¤ì°¨(error)ê°€ ìµœì†Œê°€ ë˜ë„ë¡ ê°€ì¤‘ì¹˜ $w$ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒ 

í•™ìŠµ ê°€ëŠ¥í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  

ê·¸ëŸ¬ë©´ ì˜¤ì°¨ ì—­ì „íŒŒ(error backpropagation) ì•Œê³ ë¦¬ì¦˜ì„ ì¨ë³´ì

í™œì„±í™” í•¨ìˆ˜ë¥¼ ê³„ë‹¨ í•¨ìˆ˜ì—ì„œ ë¯¸ë¶„ê°€ëŠ¥í•œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ëŒ€ì²´ 

ê²½ì‚¬ í•˜ê°•ë²• ì ìš© 

## í™œì„±í™” í•¨ìˆ˜

#### ê³„ë‹¨
~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0
~~~

#### ì‹œê·¸ëª¨ì´ë“œ

$
\begin{align\*}
\sigma(x,a) &= \frac{1}{1 + e^{-ax}} \\\ 
\sigma'(x,a) &= a\sigma(x,a)(1-\sigma(x,a))
\end{align\*}
$

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x, a=1):  # ë¯¸ë¶„ê°’ì„ ë”°ë¡œ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ë‹¤
    return a*sigmoid(x,a)*(1 - sigmoid(x,a))
~~~

#### ìŒê³¡ íƒ„ì  íŠ¸í•¨ìˆ˜

$
\begin{align\*}
\tanh(x,a) &= \frac{e^{2x} - 1}{e^{2x} + 1} \\\ 
\tanh'(x,a) &= 1 - tanh^2(x)
\end{align\*}
$

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

## ë™ì‘

![image](https://user-images.githubusercontent.com/32366711/173880082-070277bd-cafb-4336-ac68-b578a10a2eb7.png)

# ë¯¸ë¶„

> í•¨ìˆ˜ ğ‘“(ğ‘¥)ì˜ ë³€ìˆ˜ ğ‘¥ì— ëŒ€í•œ ìˆœê°„ë³€í™”ìœ¨     
> ğ‘¥ì˜ ì•„ì£¼ ë¯¸ì„¸í•œ ë³€í™”ì— ëŒ€í•œ ğ‘“(ğ‘¥)ì˜ ë³€í™”ëŸ‰     

@
f'(x) = \frac{d f(x)}{dx} = \underset{\triangle x \rightarrow 0}{lim} \frac{f(x + \triangle x) - f(x)}{\triangle x}
@

## ì—°ì‡„ ë²•ì¹™ Chain Rule

## í¸ë¯¸ë¶„

> ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì— ëŒ€í•˜ì—¬, ê·¸ ì¤‘ `í•˜ë‚˜ì˜ ë³€ìˆ˜`ì— ì£¼ëª©í•˜ê³  ë‚˜ë¨¸ì§€ ë³€ìˆ˜ì˜ ê°’ì„ ê³ ì •ì‹œì¼œ ë†“ê³  ê·¸ ë³€ìˆ˜ì— ëŒ€í•´ í•˜ëŠ” ë¯¸ë¶„ 

## ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì—°ì‡„ ë²•ì¹™

## ê·¸ë ˆë””ì–¸íŠ¸

ì–´ëŠë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì»¤ì§€ëŠ”ê°€

ì˜¤ì°¨ê°€ ìµœì†Œê°€ë˜ëŠ” ê°’ì„ ì°¾ì•„ì•¼í•˜ë‹ˆê¹Œ ê·¸ë ˆë””ì–¸íŠ¸ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ

# ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì˜ í•™ìŠµ

-> `ì˜¤ì°¨ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜`

![image](https://user-images.githubusercontent.com/32366711/173880205-145ecdc8-80b6-4541-8cdf-b6cb09396490.png)

ì…ë ¥ ë…¸ë“œ dê°œë¥¼ í†µí•´ íˆë“  ë…¸ë“œ 1ê°œ(j)ì˜ ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŒ. ì´ë ‡ê²Œ íˆë“  ë…¸ë“œ ì´ pê°œë¥¼ êµ¬í•¨
$
zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji} x_ i + u_ {j0} \\\ 
z_ j = f(zsum_ j)
$

ì…ë ¥ê°’ xì™€ ê°€ì¤‘ì¹˜ uì˜ ê³±ì˜ ì´í•© (zsum)ì„ êµ¬í•œ í›„ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì³(f(zsum)) ë…¸ë“œ zì˜ ê°’ì„ ì–»ì„ ìˆ˜ ìˆìŒ

íˆë“  ë…¸ë“œ pê°œë¥¼ í†µí•´ ì¶œë ¥ ë…¸ë“œ 1ê°œ(j)ì˜ ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŒ. ì´ë ‡ê²Œ ì´ ì¶œë ¥ ë…¸ë“œëŠ” mê°œë¥¼ êµ¬í•¨

$
osum_ k = \sum \limits_ {j=1}^{p} v_ {kj} z_ j + v_ {k0} \\\ 
$

ë§ˆì°¬ê°€ì§€ë¡œ ë…¸ë“œ zë¥¼ xì²˜ëŸ¼, ê°€ì¤‘ì¹˜ vë¥¼ ê°€ì¤‘ì¹˜ u ì²˜ëŸ¼ ìƒê°í•´        
osumê³¼ ì¶œë ¥ê°’ oë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ

ì´ë ‡ê²Œ ì„ì˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ í†µí•´ ì˜ˆì¸¡ê°’ oë¥¼ êµ¬í–ˆìœ¼ë©´, ì›ë˜ ì¶œë ¥ê°’ì¸ yì™€ ë¹„êµí•´ ì˜¤ì°¨(ì˜¤ì°¨ ì œê³±í•©)ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ

$
E = \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2
$

ë‹¤ìŒì˜ ì˜¤ì°¨í•¨ìˆ˜ë¥¼ í†µí•´ ê·¸ë ˆë””ì–¸íŠ¸ ë””ì„¼íŠ¸ ë°©ì‹ì„ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ë ¤í•¨.

$
v^{t+1} = v^{t} - \eta \frac{\partial E}{\partial v} \\\ 
u^{t+1} = u^{t} - \eta \frac{\partial E}{\partial u}
$

ë‹¤ìŒì˜ ì˜¤ì°¨í•¨ìˆ˜ì˜ vì˜ ë¯¸ë¶„ê°’ê³¼ uì˜ ë¯¸ë¶„ê°’ë§Œ êµ¬í•˜ë©´ ë¨

ë‹¤ìŒì„ êµ¬í•˜ê¸° ìœ„í•´ ì²´ì¸ë£°ì„ ì ìš©

ë¨¼ì € ì˜¤ì°¨ë¶€í„° ì—­ìœ¼ë¡œ ì˜¤ëŠ” ê²ƒì´ê¸°ì— vì— ëŒ€í•´ ë¨¼ì € ê³„ì‚°í•´ì•¼ í•˜ëŠ”ë°          
pê°œì˜ zë§ˆë‹¤ mê°œì˜ vê°€ ìˆìŒ. jë²ˆì§¸ zì˜ ($z_ j$) kë²ˆì§¸ ê°€ì¤‘ì¹˜($v_ kj$)ì— ëŒ€í•´ì„œ ì˜¤ì°¨ì—­ì „íŒŒë¥¼ ê³„ì‚°í•˜ë ¤ê³  í•œë‹¤ë©´, (kë²ˆì§¸ ê°€ì¤‘ì¹˜ë¡œ ìƒì„±ë˜ëŠ” ê²°ê³¼ëŠ” $o_ k$, ëª¨ë“  zì˜ kë²ˆì§¸ ê°€ì¤‘ì¹˜ì™€ì˜ ê³±ì˜ í•©)

$
\frac{\partial E}{\partial v_ {kj}} = \frac{\partial E}{\partial o_ k} \frac{\partial o_ k}{\partial v_ {kj}} \\\ 
\frac{\partial E}{\partial o_ k} = \frac{\partial}{\partial o_ k} \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2 = o_ k - t_ k \\\ 
\frac{\partial o_ k}{\partial v_ {kj}} = \frac{\partial}{\partial v_ {kj}} \sum \limits_ {j=1}^{p} v_ {kj} z_ j + v_ {k0}  = z_ j * f'(osum_ k) \\\ 
\therefore \frac{\partial E}{\partial v_ {kj}} = (o_ k - t_ k) * z_ j * f'(osum_ k)
$

ë§ˆì°¬ê°€ì§€ë¡œ dê°œì˜ xë§ˆë‹¤ pê°œì˜ uê°€ ìˆìŒ. ië²ˆì§¸ xì˜ ($x_ i$) jë²ˆì§¸ ê°€ì¤‘ì¹˜($u_ ji$)ì— ëŒ€í•´ì„œ ì˜¤ì°¨ì—­ì „íŒŒë¥¼ ê³„ì‚°í•˜ë ¤ê³  í•œë‹¤ë©´, (jë²ˆì§¸ ê°€ì¤‘ì¹˜ë¡œ ìƒì„±ë˜ëŠ” ê²°ê³¼ëŠ” $z_ j$, ëª¨ë“  xì˜ jë²ˆì§¸ ê°€ì¤‘ì¹˜ì™€ì˜ ê³±ì˜ í•©)

$
\frac{\partial E}{\partial u_ {ji}} = \frac{\partial E}{\partial z_ j} \frac{\partial z_ j}{\partial u_ {ji}} \\\ 
\frac{\partial E}{\partial z_ j} = \sum \limits_ {k=1}^{m} \frac{\partial E}{\partial o_ k}\frac{\partial o_ k}{\partial z_ j}  \\\ 
\sum \limits_ {k=1}^{m} (o_ k - t_ k) f'(osum_ k) v_ {kj} \\\ 
\frac{\partial z_ j}{\partial u_ {ji}} = \frac{\partial}{\partial u_ {ji}} zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji} x_ i + u_ {j0} = x_ i * f'(zsum_ j) \\\ 
\therefore \frac{\partial E}{\partial v_ {kj}} = \sum \limits_ {i=1}^{d} (o_ k - t_ k) * v_ {kj} * f'(osum_ k) * x_ i * f'(zsum_ j)
$


# ë¶„ë¥˜ ë¬¸ì œ

## ì¢…ë¥˜

## ì¶œë ¥ê°’ í‘œí˜„

ë¶„ë¥˜ ìˆ˜ ë§Œí¼ ìë¦¬ìˆ˜ê°€ ìˆê³ , í•´ë‹¹í•˜ëŠ” ìë¦¬ì˜ ê°’ì´ 1ì´ë©´ ì›í•«ì¸ì½”ë”©, ì´ê²Œ ì¢€ë” ìì£¼ì“°ì¸ë‹¤


~~~ python
data = ["cold", "cold", "warm", "hot", "hot", "cold"] # ë¬¸ìì—´, ê¸°í˜¸
label_encoder = LabelEncoder() # ì •ìˆ˜ ì¸ì½”ë”©
onethot_encoder = OneHotEncoder(sparse=False) # ì›í•« ì¸ì½”ë”©
~~~

# ì˜¤ì°¨ í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜

> ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥ê³¼ ëª¨ë¸ì˜ ì¶œë ¥ ì°¨ì´ë¥¼ ì¶•ì í•˜ì—¬ í‘œí˜„í•˜ëŠ” í•¨ìˆ˜

## íšŒê·€ ë¬¸ì œì˜ ì˜¤ì°¨ í•¨ìˆ˜

$
E = \frac{1}{2} \sum \limits_ {i=1}^{N} (y(x_ i, w) - t_ i)^2
$

## ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ ì˜¤ì°¨ í•¨ìˆ˜

í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©

-> êµ¬ê°„ (0,1) ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•¨ìœ¼ë¡œì¨ í™•ë¥ ë¡œ í•´ì„ ê°€ëŠ¥í•˜ë‹¤

y(x,w)ëŠ” ì¡°ê±´ë¶€ í™•ë¥  p($C_ 1$\|x), 1- y(x,w)ëŠ” ì¡°ê±´ë¶€ í™•ë¥  p($C_ 2$\|x), 

ë”°ë¼ì„œ ì…ë ¥ xì™€ ê°€ì¤‘ì¹˜ wì— ëŒ€í•œ ëª©í‘œê°’ tì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì€

$
p(t \| x,w) = y(x,w)^t \{1-y(x,w)\}^{1-t}
$

#### ê°€ëŠ¥ë„, likelihood

$
p(D;w) = \prod \limits_ {i=1}^{N} y(x,w)^t \{1-y(x,w)\}^{1-t}
$

#### ì˜¤ì°¨ í•¨ìˆ˜

= ìŒì˜ ë¡œê·¸ ê°€ëŠ¥ë„, í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼

$
\begin{align\*}
E(w) &= -log \prod \limits_ {i=1}^{N} y(x,w)^t \{1-y(x,w)\}^{1-t} \\\ 
 &= - \sum \limits_ {i=1}^{N} (t_ i log y(w_ i, w) + (1-t_ i)log (1-y(x_ i, w)))
\end{align\*}
$

## ë‹¤ë¶€ë¥˜ ë¶„ë¥˜ ë¬¸ì œì˜ ì˜¤ì°¨í•¨ìˆ˜

ì¶œë ¥ì˜ í•©ì€ 1

#### ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¸µ

ìµœì¢… ì¶œë ¥ì„ ë¶„ë¥˜ í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ì¸µ

$
y_ k = \frac{e^{z_ k}}{\sum \limits_ {i=1}^{K} e^{z_ i}
$

~~~ python
def softmax(x):
    return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

í•™ìŠµ ë°ì´í„° $(x_ i, t_ i)$ì˜ ì¡°ê±´ë¶€ í™•ë¥ 
$
p(t_ i \| x_ i, w) = \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik}
$

ì „ì²´ ë°ì´í„° Dì— ëŒ€í•œ ê°€ëŠ¥ë„

$
p(D;w) = \prod \limits_ {i=1}^{N} \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik}
$

> ë°ì´í„°ì˜ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” íŒŒë¼ë¯¸í„° wë¥¼ ì¶”ì •í•˜ëŠ”ê²ƒì´ ëª©í‘œ

$
\underset{w}{argmax} p(D;w)
$

#### ì˜¤ì°¨í•¨ìˆ˜ E(w)

 = ê°€ëŠ¥ë„ì˜ ìŒì˜ ë¡œê·¸ ê°€ëŠ¥ë„

$
E(w) = - log \prod \limits_ {i=1}^{N} \prod \limits_ {k=1}^{K} y_ k (X_ i, w)^t_ {ik} = -\sum \limits_ {i=1}^{N}\sum \limits_ {k=1}^{K} t_ {ik} log y_ k (x_ i, w)
$

#### ì˜¤ì°¨í•¨ìˆ˜ì— ëŒ€í•œ êµì°¨ ì—”íŠ¸ë¡œí”¼
$
E(w) = - \sum \limits_ {K}{k=1} t_ {ik} log y_ k (x_ i, w)
$

#### MSE, mean squared error

$
E = \frac{1}{2} \sum \limits_ {n}^{k=1} (o_ k - y_ k)^2
$


# RBF ë§

## RBF í•¨ìˆ˜

> ê¸°ì¡´ ë²¡í„° $\mu$ì™€ ì…ë ¥ ë²¡í„° xì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜

$
\phi (x, \mu) = exp(-\beta \|\| x - \mu \|\|^2)
$


ê°€ìš°ì‹œì•ˆì´ë‘ ë¹„ìŠ·í•˜ê²Œìƒê²¼ëŠ”ë°, ê°€ìš°ì‹œì•ˆì€ ìµœëŒ€ê°€ 1ì´ì•„ë‹˜

í‰ê· ê³¼ í‘œì¤€í¸ì°¨ì— ëŒ€í•œ ì •ë³´ë¥¼ ì¡°ì ˆí•´ì„œ

í•¨ìˆ˜ ê³µê°„ì—ì„œ ì—¬ëŸ¬ ê·¸ë˜í”„ë¥¼ ê²°í•©í•´ì„œ ì‚¬ìš©

## RBF ë§

> ì–´ë–¤ í•¨ìˆ˜ $f_ k (x)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ RBF í•¨ìˆ˜ë“¤ì˜ ì„ í˜• ê²°í•© í˜•íƒœë¡œ ê·¼ì‚¬ì‹œí‚¤ëŠ” ëª¨ë¸

$
f_ k (x) \approx \sum \limits_ {i=1}^{N}w_ {kj}\phi_ i (x, \mu_ i) + b_ k
$

$
o_ k = \sum \limits_ {j=1}^{N} w_ {kj} \phi_ j + b_ k
$

### RBF ë§ì˜ í•™ìŠµ

#### ì˜¤ì°¨ í•¨ìˆ˜ E
$
E = \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^2
$

#### ê²½ì‚¬ í•˜ê°•ë²• ì‚¬ìš©

> ê¸°ì¤€ ë²¡í„° $\mu_ j$ì™€ íŒŒë¼ë¯¸í„° $\beta_ j$, ê°€ì¤‘ì¹˜ $w_ {kj}$ ê²°ì •

ë¶€ë¥˜ë³„ êµ°ì§‘í™” ê²°ê³¼ë¥¼ ì‚¬ìš©í•œ ê¸°ì¤€ ë²¡í„° $\mu_ j$ì™€ íŒŒë¼ë¯¸í„° $\beta_ j$ ì´ˆê¸°í™”

êµ°ì§‘ì˜ ì¤‘ì‹¬ì€ ê¸°ì¤€ ë²¡í„° $\mu_ j$

ë¶„ì‚°ì˜ ì—­ìˆ˜ $\beta_ j$

$
\sigma = \frac{1}{m} \sum \limits_ {i=1}^{m} \|\| x_ i - \mu \|\|  \;\;\;\; \;\;\;\; \beta = \frac{1}{2\sigma ^2}
$

![image](https://user-images.githubusercontent.com/32366711/173880379-5e610745-6cc7-468b-8013-8defab04e3c9.png)

~~~ python
def basisFunc(self, c, d):
    assert len(d) == self.indim
    return np.exp(-self.beta * norm(c-d)**2)

def activationFunc(self, X):
    G = np.zeros((X.shape[0], self.numCenters), float)
    for ci, c in enumerate(self.centers):
        for xi, x in enumerate(X):
            G[x1, c1] = self.basisFunc(c, x)

def train(self, X, Y):
    rnd_idx = random.permutation(X.shape[0])[:self.numCenters]
    self.centers = [X[i,:] for i in rnd_idx]
    G = self.activationFunc(X)
    self.W = np.dot(pniv(G), Y)

def predict(self, X):
    G = self.activationFunc(X)
    Y = np.dot(G, self.W)
    return Y
~~~


## í•™ìŠµ

ìµœì†Œê°€ë˜ëŠ” ì˜¤ì°¨í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì„œ ê²½ì‚¬í•˜ê°•ë²• ì ìš©


ê¸°ì¤€ ë®¤ì™€ ë² íƒ€ê°’ì„ ì´ˆê¸°í™”í•´ì•¼í•¨

êµ°ì§‘ì˜ ì¤‘ì‹¬ 

norm ë²¡í„°ì˜ ì •ê·œí™”ëŠ” í¬ê¸°, ì°¨ì´ì˜ í¬ê¸°ì˜ í‰ê· 

