---
layout: post
title: "ML, 5장 제약조건 최적화와 이차계획법, SVM"
description: "기계학습, 이건명교수님"
categories: [MachineLearning]
tags: [2022-1, Machine Learning, ML, 이건명]
use_math: true
redirect_from:
  - /ML/5
  - /blog/ML/5
---

* Kramdown table of contents
{:toc .toc} 

# 함수 최적화

> 어떤 목적 함수(objective function)가 있을 때, 이 함수를 최대로
하거나 최소로 하는 변수 값를 찾는 최적화 문제

- 오차를 최소화 하도록 하는 함수
- 손실함수, loss function 을 최소로 하는 파라미터, 가중치를 찾는 문제

# 경사하강법

> 복잡한 함수인 경우, 임의의 위치에서 시작하여         
> 함수 𝑓(𝒙)의 그레디언트(gradient) <red>반대 방향</red>으로 조금씩 움직여 가며 최적의 변수 값를 찾으려는 방법

@
f(x) = f(x_ 1, x_ 2) \quad \triangledown f = (\frac{\partial f}{\partial x_ 1},\frac{\partial f}{\partial x_ 2})
@

탐색 중의 매 위치에서 그레디언트를 계산하여, 

그레디언트 반대방향으 로 이동하도록 변수의 값을 반복적으로 조금씩 조정

@
x_ 1 \leftarrow x_ 1 - \eta\frac{\partial f}{\partial x_ 1} \\\ 
x_ 2 \leftarrow x_ 2 - \eta\frac{\partial f}{\partial x_ 2}
@

$\eta$ : 학습률

> 그러나 반드시 최적해를 보장할 순 없다


# 제약조건 최적화

> 제약조건을 만족하면서 목적함수를 최적화하는 변수들의 값을 찾는 문제

제약조건을 만족하는 값들 = 가능해

## 라그랑주 함수

> 제약조건들과 목적함수를 결합한 함수

@
L(x_ 1, x_ 2, \lambda, \alpha) = f(x_ 1, x_ 2) + \lambda g(x_ 1, x_ 2) + \alpha h(x_ 1, x_ 2)
@

라그랑주 승수
- $\lambda$ = 등식     
- $\alpha$ = 부등식, $\alpha \geq 0$


### 제약조건 최적화

> $\lambda, \alpha$($\alpha \geq 0$) 를 마음대로 바꾸며, $L (x_ 1, x_ 2, \lambda, \alpha)$의 값을 아무리 키워도      
> $min_ {x_ 1, x_ 2} max_ {\alpha, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)$의 값은 $x_ 1, x_ 2$가 가능해 일 때 나온다

@
min_ {x_ 1, x_ 2 \in FS} f(x_ 1, x_ 2) = min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha)
@

FS : 가능해 집합

제약조건이 몇개가 있을 지 모르는데, 이것들의 가능해를 찾아서 목적함수를 최소화 하는것보다,

이모든걸 하나의 식으로 만든 라그랑주 함수를 해결하는게 더 쉬울 수 있다

@
\begin{align\*}
 min_ {x_ 1, x_ 2} max_ {\alpha \geq 0, \lambda} L (x_ 1, x_ 2, \lambda, \alpha) &\geq  max_ {\alpha \geq 0, \lambda} min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha) \\\ 
 &\geq max_ {\alpha \geq 0, \lambda} L_ d (\lambda, \alpha) 
\end{align\*}
@

$
L_ d (\lambda, \alpha) = min_ {x_ 1, x_ 2} L (x_ 1, x_ 2, \lambda, \alpha)
$ : 쌍대함수

1. 라그랑주 함수를 구한다
2. 쌍대함수를 만든다
3. 쌍대함수를 만족하는 x1 x2를 구하기 위해 각각에 대해 편미분을 한다
4. 쌍대함수에 x1 x2값을 대입한다
5. 쌍대함수가 최대가 되는 람다 알파를 구하기 위해, 각각에 대해 편미분 한다
6. 원래식에서 람다 알파를 대입해 x1 x2를 찾는다


# 이차계획법

> 목적함수가 볼록 이차식(convex quadratic)이고, 제약조건이 모두 일차식인 최적화 문

cvxopt 패키지의 quadratic programming solver 

$\alpha$ = cvxopt.solvers.qp(H, f, A, a, B, b) 


$
\underset{x}{min}\frac{1}{2}x^THx + f^Tx \\\ 
Ax \leq a \\\ 
Bx = b
$

@
\begin{align\*}
\text{minimize} \quad & 2x_ {1}^2 + 2x_ {2}^2  + x_ {1}x_ {2} + x_ {1} + x_ {2} \\\ 
\text{subject to} \quad & x_ 1 \geq 0 \\\ 
& x_ 2 \geq 0 \\\ 
& x_ 1 + x_ 2 = 1
\end{align\*}
@

~~~ python
H = 2*matrix([[2, .5], [.5, 1]]) # 편미분 후 x1 x2값, [[x1 계수, x2 계수]]
f = matrix([1.0, 1.0]) # 편미분 후 상수값, [x1 상수, x2 상수]
A = matrix([ [-1, 0], [0 ,-1]]) # Ax <= h
a = matrix([0, 0])
B = matrix([1 ,1], (1,2)) # Bx = b
b = matrix(1.0)
~~~

@
H = \begin{bmatrix}
4 & 1 \\\ 
1 & 4 \\\ 
\end{bmatrix}
\;
f = \begin{bmatrix}
1.0 \\\ 
1.0
\end{bmatrix} 
\
A = \begin{bmatrix}
-1 & 0 \\\ 
0 & -1 \\\ 
\end{bmatrix} 
\;
a = \begin{bmatrix}
0 \\\ 
0
\end{bmatrix} 
\;
B = \begin{bmatrix}
1 & 1 \\\ 
\end{bmatrix} 
\;
b = \begin{bmatrix}
1.0
\end{bmatrix} 
@


# SVM

`Support Vector Machine`

> 분류 오차를 줄이면서 동시에 <red>여백을 최대</red>로 하는 결정 경계(decision boundary)를 찾는 <red>이진 분류기(binary classifier)</red>

- 여백(margin)
    - 결정 경계와 서포트 벡터까지의 거리
- 서포트 벡터(support vector)
    - 결정 경계로부터 가장 가까이에 있는 학습 데이터들


### 초평면[^hyperplane] 기하학

$
w^Tx + b = 0
$

@
\begin{align\*}
x &= x_ p + r\frac{w}{|| w ||} \\\ 
\\\ 
w^Tx &= w^Tx_ p + r\frac{w^T w}{||w||} \\\ 
&= w^Tx_ p + r||w|| \\\ 
\\\ 
w^Tx + b &= w^Tx_ p + b + r||w|| \\\ 
h(x) &= r||w|| \\\ 
r &= \frac{h(x)}{||w||} &&
\end{align\*}
@


[^hyperplane]: 4차원이상의 공간에서 선형 방정식으로 표현되는 결정 경계

## SVM 학습

조건 1.  $t_ i h(x_ i) \geq 1$

- 서포트 벡터 x'에서의 \|h(x')\| = 1    
- h(x) > 0 인 공간에 $t_ i$ = 1    
- h(x) < 0 인 공간에 $t_ i$ = -1    

조건 2. 서포트 벡터와의 거리, 즉 여백을 최대로 한다

- $r = \frac{h(x)}{\|\|w\|\|}$ 서포트 벡터에 대해서는 h(x) = 1     
- $r = \frac{1}{\|\|w\|\|}$    
- $\frac{1}{\|\|w\|\|}$ 을 최대화 해야하니, $\|\|w\|\|$를 최소화 해야한다    


## SVM 최적화 문제

라그랑주 함수를 최소화 하는 w,b를 구해야 한다

> Find w,b which minimizes $J(x) = \frac{1}{2} \|\|w\|\|^2$              
> sunject to $1 - t_ i h (x_ i) \leq 0, i = 1, \cdots, N$

이걸 라그랑주 함수로 표현하면

> Find w, b which minimizes $L(x,b,\alpha) = \frac{1}{2} ||w||^2 + \sum \limits_ {i=1}^{N} \alpha_ i (1-t_ i(w^Tx_ i + b))$       
> subject to $\alpha_ i \geq 0, i = 1, \cdots, N$

이걸 다시 w, b가 없는 쌍대 문제로 바꾸면

> Find $\alpha$ which maximizes  $\widetilde{L}(\alpha) - \frac{1}{2} \sum \limits_ {i=1}^{N} \sum \limits_ {j=1}^{N} \alpha_ i \alpha_ j t_ i t_ j x_ i \cdot x_ j + \sum \limits_ {i=1}^{N} \alpha_ i$    
> subject to $\sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0, i = 1, \cdots, N$
> $\alpha_ i \geq 0,  i = 1, \cdots, N$

#### 유도 식

이건 즉 SVM의 최적화 문제와 같음

$L(x, \alpha, \lambda) = f(x) + \lambda g(x) + \alpha h(x)$

$L(w, b, \alpha) = \frac{1}{2} \|\|w\|\|^2 + \sum \limits_ {i=1}^{N} \alpha_ i (1-t_ i(w^Tx_ i + b))$

이때, w,b에 대해 편미분 했을 때 0이 되어야 한다 (해에 대한KKT 조건)

$ \frac{\partial L}{\partial w} = 0, \frac{\partial L}{\partial b} = 0 $

@
\frac{\partial L}{\partial w} = w - \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i = 0 \\\ 
\frac{\partial L}{\partial b} = - \sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0  \\\ 
\therefore w = \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i , \sum \limits_ {i=1}^{N} \alpha_ i t_ i = 0
@

초평면 함수식    
$h(x) = w^Tx + b = \sum \limits_ {i=1}^{N} \alpha_ i t_ i x_ i \cdot x + b $ 


~~~ python
sol = cvxopt.solvers.qp(H, f, A, a, B, b) # sol['x'][i]가 0이 아닌 값이 서포트 벡터

# 앞선 식을 정리하다보면 w가 계산되고 (w는 서포트 벡터의 기울기)
# 이렇게 나온 서포트 벡터 하나를 t_i(w^Tx_i + b) = 1 에 넣어 b(절편)를 계산한다
~~~

@
H = \begin{bmatrix}
t_ 1 t_ 1 x_ 1 \cdot x_ 1 & t_ 1 t_ 2 x_ 1 \cdot x_ 1 & \cdots & t_ 1 t_ N x_ 1 \cdot x_ N \\\ 
t_ 2 t_ 1 x_ 2 \cdot x_ 1& t_ 2 t_ 2 x_ 2 \cdot x_ 2 & \cdots & t_ 2 t_ N x_ 2 \cdot x_ N \\\ 
\vdots & \vdots & \ddots  & \vdots \\\ 
t_ N t_ 1 x_ N \cdot x_ 1 & t_ N t_ 2 x_ N \cdot x_ 2  & \cdots & t_ N t_ N x_ N \cdot x_ N \\\ 
\end{bmatrix} = \begin{bmatrix}
t_ 1 \\\ 
t_ 2 \\\ 
\vdots \\\ 
t_ N
\end{bmatrix}
\begin{bmatrix}
t_ 1 \\\ 
t_ 2 \\\ 
\vdots \\\ 
t_ N
\end{bmatrix}^T \cdot^* \begin{bmatrix}
x_ 1 \\\ 
x_ 2 \\\ 
\vdots \\\ 
x_ N
\end{bmatrix}\begin{bmatrix}
x_ 1 \\\ 
x_ 2 \\\ 
\vdots \\\ 
x_ N
\end{bmatrix}^T
@

@
f=\begin{bmatrix}
-1 \\\ 
-1 \\\ 
-1 \\\ 
-1
\end{bmatrix} 
A = \begin{bmatrix}
-1 & 0 & \cdots & 0 \\\ 
0 & -1 & \cdots & 0 \\\ 
\vdots & \vdots & \ddots & \vdots \\\ 
0 & 0 & \cdots & -1 \\\ 
\end{bmatrix} 
a = \begin{bmatrix}
0 \\\ 
0 \\\ 
\vdots \\\ 
0
\end{bmatrix} 
B = \begin{bmatrix}
t_ 1 & t_ 2 & \cdots & t_ N \\\ 
\end{bmatrix} 
b = \begin{bmatrix}
0
\end{bmatrix} 
@


# 선형 분리불가 문제의 SVM

`슬랙변수` $\xi_ i$

- 학습 데이터별로 하나씩 생성
- 분할 초평면에서 서포트 벡터보다 멀리 있는 올바르게 분류 된 데이터는 0
- 그렇지 않으면 $\xi_ i = \| t_ i - h(x_ i) \|$ 

$t_ i h(x_ i) \geq 1 - \xi_ i, i=1, \cdots, N$


~~~ python
svc = LinearSVC(C=0.5) # C는 상한에 대한 제약조건
~~~

# 비선형 SVM

> 데이터를 고차원으로 사상하여 선형 분리를 하자

but 저장 공간, 계산 비용의 증가가 문제임

-> 커널 트릭 사용으로 해결함

## 커널 트릭과 커널 함수

> $\Phi(x_ i) \dot \Phi(x_ j)$를 고차원으로 변환하여 계산하지 않고, 원래 데이터에서 계산              
> 이때, 고차원 변환없이 계산할 수 있는 커널 함수 K 사용

@
K(x_ i, x_ j) = \Phi(x_ i) \dot \Phi(x_ j)
@

- 선형 커널 $x_ i ^ T x_ j$
- 다항식 커널 $(x_ i ^ T x_ j)^d$
- RBF 커널 $e^{\gamma \|\|x_ i - x_ j \|\|^2 / (2\sigma^2)}$
- 시그모이드 , 쌍곡 탄젠트 커널 $\text{tanh}(\gamma(x_ i ^ T x_ j) + \theta)$


# 다중 부류 분류

## 이진 분류기를 이용한 다중 부류 분류 (multiclass classification)

### 일대일 방법 one vs one approach, OVO

- $C_p (q)$ : 부류 p와 q를 비교하여 p면 1 아니면 -1
- N개 부류가 있는 문제에 대해 N(N-1)/2 개의 분류기 학습
- 최종 부류 : 가장 빈번하게 나오는 부류

@
C = arg \, max_ p \sum_ q C_ p (q)
@

### 일대나머지 방법 one vs all approach, OVA

- 부류별 분류기 학습
- 각 학습기별로 데이터 x에 대한 $h(x) = \sum \limits_ {i=1}^{N} a_ i t_ i K(x_ i, x) + b$ 계산
- 최종 부류 : 가장 큰 값을 갖는 부류