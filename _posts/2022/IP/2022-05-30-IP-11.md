---
layout: post
title: "IP, 11장 영상처리와 딥러닝"
description: "영상처리, 최경주교수님"
categories: [ImageProcessing]
tags: [2022-1, Image Processing, IP, 최경주]
use_math: true
redirect_from:
  - /IP/11
  - /blog/IP/11
---

* Kramdown table of contents
{:toc .toc} 


# 기계학습

> 어떤 컴퓨터 프로그램이 (분류, 인식) 라는 작업을 수행한다. 이 프로그램의 성능을 (인식률) 라는 척도로 평가했을 때 경험 (학습 집합) 를 통해 성능이 개선된다면 이 프로그램은 학습을 한다고 말할 수 있다.

## 지도 학습

- 학습 단계 : 외부 지도에 따라 배우는 과정
- 테스트 단계 : 현장에서 성능을 평가 하는 과정


지도학습은 부류(class) 정보를 가진 샘플로 구성된 학습 집합 사용

즉 정답을 가지고 학습시킴

### 학습 모델

기계가 학습을 하려면 학습 모델이 있어야함.

$\Theta$ : 학습 모델. 매개변수 집합

> 학습의 목표는 최적의 $\Theta$값을 찾는 것

$\Theta$ 값이 얼마나 좋은지 측정하는 목적함수가 필요함

### 학습 알고리즘 원리

1. 학습 집합을 사용하여 현재 $\Theta$값을 평가한 후, 개선하는 방향을 알아내 갱신
2. 수렴할 때까지 반복
3. 수렴에 도달하면 검증 집합(validation set)을 대상으로 성능 평가
4. 결과가 만족스러우면 학습을 마치고, 그렇지 않으면 매개변수 집합을 변경하고 다시 학습
  - 노드를 추가하거나 학습률, 가중치를 변경

### 일반화 능력

> 학습 과정에서 사용하지 않은 `테스트 집합`으로 학습이 완료된 분류기 성능을 평가

`과적합`이 이루어지면 일반화 능력이 떨어진다


## 비지도 학습

> 부류(class) 정보가 없는 상황의 학습

정답이 없다

## 준지도 학습

> 부류 정보가 있는 샘플과 없는 샘플이 섞여있는 모델

- 부류 정보를 부여하지 못한 샘플이 인터넷에서 많이 발생하기 때문에 최근 부각되고 있음
- 부류 정보가 있는 샘플로 학습한 후, 부류 정보가 없는 샘플의 부류 정보를 추정한다

# 재 샘플링을 이용한 성능 평가

> 학습에 사용할 데이터베이스가 작은 경우, 같은 샘플을 여러번 사용하여 성능 측정의 통계적 신뢰도를 높이는 아이디어

## K-겹 교차 검증

> 샘플 집합을 k개로 등분,     
> k-1개로 학습, 1개로 테스트. 이를 k번 반복

`분류기의 성능은 평균값`

#### 교차 검증을 사용하는 경우

- 모델 선택이 필요 없는 경우
  - 하나뿐인 DB를 훈련과 테스트 집합으로 나누어 사용
- 훈련 집합과 테스트 집합이 별도로 있는데, 모델을 선택하기 위해 검증 집합이 필요한 경우
  - 훈련 집합을 k겹 교차하여 훈련과 검증 목적으로 사용


## 붓스트랩, bootstrap

> 샘플의 중복을 허용         
> 임의의 데이터들을 선택해 성능을 측정한다

훈련 집합의 샘플링과 성능 측정을 여러 번 반복한 후, 평균 성능을 구함

# 신경망, 퍼셉트론, 다층 퍼셉트론

> 연결주의 계산 모형

1. 주어진 샘플에 대해 전방 계산으로 오류를 추정
2. 출력 층에서 시작해 은닉층, 입력층 방향으로 거꾸로 전진하며 오류 전파
  - 오류를 주이는 쪽으로 가중치를 갱신
3. 종료 조건 : 더 이상 성능 향상이 없을 때


# CNN

`Convolutional Neural network`

> 특징 추출과 분류를 동시에 학습

특징 검출기란, 고정된 마스크로 컨볼루션을 수행함으로써 추출

> 주어진 문제에 가장 적합한 마스크를 학습으로 알아내자

- 컨볼루션 층 : 마스크를 학습하는 층
- 가중치 공유
- 풀링

Convolution과 Pooling이 과정을 반복하여 거치게 되면 다음
Input으로 들어갈 Tensor의 크기가 ¼ 로 줄게 되어 계산량이 감소되며, 
이미지 전체를 대표할 수 있는 Global Feature를 얻을 수 있게 됨.

## Convolution layer

- 필터들은 적용되는 위치가 달라도 같은 `가중치 값을 공유한다`
- 픽셀의 특성은 좌표값과는 무관함을 반영

#### Stride

#### Padding

## Polling layer

> 입력 특징맵을 그보다 작은 크기의 특징 맵으로 매핑

- 표현 차원을 줄이고 이미지 왜곡이나 작은 변환에 영향으 받지 않게 함
- 이미지 크기를 줄인 후에 컨볼루션을 수행할 경우 상위 layer로 올라갈 수록 같은 크기의 convolution filter가 하위 layer의 저차원 feature를 조합한 고차원 feature를 학습하는 효과를 가져옴

## Feedforward layer

> Fully connected 계층

분류를 위한 일반 신경망

## Drop Out

> 모든 커넥션을 연결하는게 아니라, 일부만 연결

1. Over fitting을 방지하기 위해 Training procedure 에 Randomness 추가
2. Hidden node를 모두 훈련 시키지 않고 Random하게 훈련에서 제외
3. 단, weight가 무작위로 선택되어 학습되어지므로 학습시간이 더 오래 걸릴 수 있음

- 계층간 연결이 많은 신경망에 비해 오차 신호가 섞여서 상쇄되는 문제가 훨씬 적게 발생
- 방향성 소실 문제 일부 해결
