---
layout: post
title: "기계학습, Hierachy Clustering"
description: "Hierachy Clustering, 계층적 군집 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, Hierachy, Clustering, kaggle]
use_math: true
redirect_from:
  - /kaggle/18
  - /blog/kaggle/18
---

* Kramdown table of contents
{:toc .toc}      


# Hierachy Clustering 
   
> 군집간의 거리(유사도)를 기반으로 계층을 나누어 클러스터링을 하는 알고리즘이며, K Means와는 다르게 군집의 수를 미리 정해주지 않아도 됨            
> 클러스터링 결과 는 일반적으로 덴드로그램 으로 표시한다             

ex ) 종 -> 속 -> 강 -> 목 -> 과 -> 문 -> 계 순으로 묶어가며 하나의 그룹으로 만들어가는 과정

![image](https://user-images.githubusercontent.com/32366711/126897534-0e5a7f15-cbf2-453d-9ed3-97e227bc903d.png)

![image](https://user-images.githubusercontent.com/32366711/126901230-ccf0c013-de20-4e42-a026-02d425e18ac5.png){: width="350"}{: .aligncenter}

## Agglomerative, 병합

> Bottom-Up 방식      
> 비슷한 군집끼리 묶어 가면서 최종 적으로는 하나의 케이스가 될때까지 군집을 묶는 클러스터링 알고리즘           

![image](https://user-images.githubusercontent.com/32366711/128140874-86f358c9-dcc1-4c84-a0ab-c27e00b10d4a.png){: width="600"}{: .aligncenter}

## Divisive, 분할

> Top-Down 방식      
> 일단 하나의 군집으로 묶은 후, 유사하지 않은 군집을 분할해가는 클러스터링 알고리즘
         

## 거리의 기준

### 비계층적 거리 측정법

> 모든 경우에 사용할 수 있는 거리 측정 방법


군집 - 군집 간에 거리는?

- 요소간 거리중 최소거리를 기준으로 ( Single Linkage )     

u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산해 가장 짧은 것으로 선택

@ d(u,v) = min(d(u_ i, v_ j)) @


<br/>

- 요소간 거리중 최대거리를 기준으로 ( Complete Linkage )       

u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산해 가장 긴 것으로 선택

@ d(u,v) = max(d(u_ i, v_ j)) @


<br/>
  
- 요소간 거리의 평균 거리를 기준으로 (Average Linkage, [UPMGA](https://en.wikipedia.org/wiki/UPGMA))      


u의 모든 데이터($u_ i$)와 v의 모든 데이터($v_ i$)간의 거리 조합을 계산하여 평균을 계산

@ d(u,v) = \frac {\sum \limits_ {i, j}^{} {d(u_ i, v_ j)}} {n_ u * n_ v} @

<br/>

- 군집의 중심간 거리를 기준으로 ( Centroid )    

@ d(u,v) = || c_ u - c_ v || @
@ c_ u = \frac {1}{n} \sum \limits_ {i=0}^{n} u_ i @


### 계층적 거리 측정법

> 계층적 군집화에서만 사용할 수 있는 방법으로, 이미 전에 계층적으로 합쳐진 적이 있다고 가정하며            
> 이전에 사용했던 정보를 다시 사용하는 방법           
> 따라서 계산량이 앞선 방법들 보다 적어 효율적이다        

<br/>

군집 u가 군집 s와 t가 합쳐져서 생겼다면 다음의 방식들로 거리를 계산할 수 있다

@ u \leftarrow s + t @


- 중앙값 거리 (median)

군집 u의 중심 $c_ u$은 이전의 두 군집의 중심이라고 볼 수 있음

@ d(u,v) = || c_ u - c_ v || @
@ c_ u = \frac {1}{n} \sum \limits_ {i=0}^{n} u_ i \Rightarrow c_ u  = \frac{1}{2}(c_ s + c_ t)@

따라서 다시 처음부터 모든 데이터로부터 중심을 계산하지 않고,       
위 방법으로 중심을 계산해 비계층적 거리 측정법의 Centroid 방식을 사용할 수 있다           

<br/>

- 가중 거리 (weighted, [WPGMA](https://en.wikipedia.org/wiki/WPGMA))

군집 u와 v간의 거리는 군집 u를 이루는 군집 s, t와 군집 v간의 거리의 평균이라고 볼 수 있다

@ d(u,v) = \frac {1}{2} (d(s,v) + d(t,v)) @

<br/>

- 와드 거리 (Ward)

와드연결법(Ward linkage)은 군집간의 거리에 따라 데이터들을 연결하기 보다는         
군집내 편차들의 제곱합(within group sum of squares)에 근거를 두고 군집들을 병합시키는 방법이다.          
와드연결법은 군집분석의 각 단계에서 데이터들을 하나의 군집으로 묶음으로써 생기는 정보의 손실을          
군집의 평균과 데이터들 사이의 오차제곱합(SSE)으로 아래와 같이 측정한다.              

<br/>

변수의 수가 m인 데이터에서 현 단계에 K 개의 군집이 있고, 각 군집에 $n_i$ 개의 데이터가 있다면
각 군집의 $SSE_i$ 와 전체 군집의 $SSE$는 다음과 같다.

@
SSE_ i = \sum \limits_ {j=1}^{n_ i} \sum \limits_ {k=1}^{m} ||x_ {ijk} - \bar x_ {ik}||^2 \\\ 
SSE = \sum \limits_ {i=1}^{K} SSE_ i = \sum \limits_ {i=1}^{K} \sum \limits_ {j=1}^{n_ i} \sum \limits_ {k=1}^{m} ||x_ {ijk} - \bar x_ {ik}||^2
@

먼저 각각의 데이터들은 그 자체로 군집이며, SSE = 0이 된다

이후 군집을 만들어가는 각 단꼐마다 군집의 모든 가능한 쌍들의 병합이 고려되는데,

각 단계에서 두 군집의 병합으로 인한 SSE의 증가분이 최소가 되도록(에러가 최소가 되도록) 군집을 병합시켜 새로운 군집을 만든다

크기가 각각 $n_ 1, n_ 2$인 두 군집 $G_ 1, G_ 2$을 묶을 때 생기는 SSE의 증가분을 $d(G_ 1, G_ 2)$라고 표기하며, 다음과 같이 계산하며

@
d(G_ 1, G_ 2) = \frac {||\bar x_ 1 - \bar x_ 2||^2}{\frac {1}{n_ 1} + \frac {1}{n_ 2}}
@

이때 이 증가분을 두 군집 $G_ 1, G_ 2$사이의 거리로 정의한다

![image](https://user-images.githubusercontent.com/32366711/128177490-e476cf92-a780-4375-b9e9-e8e9014f9f66.png){: .aligncenter}


#### 평균거리(UPGMA) vs 가중거리(WPGMA)

[Unweighted Pair Group Method with Averaging vs Weighted Pair Group Method with Averaging](https://www.mun.ca/biology/scarr/UPGMA_vs_WPGMA.htm){: target="_ blank"}

가중치 적용 및 비가중치 적용 용어는 최종 결과를 나타내지 계산 과정을 나타내는 것은 아님     
따라서 WPGMA의 단순 평균화는 가중 결과를 생성하며 UPGMA의 비례 평균화는 비가중 결과를 생성한다.     

![image](https://user-images.githubusercontent.com/32366711/128153825-1aef947a-cd42-4c91-92f0-d11c95b3e613.png){: width="400"}{: .aligncenter}



## Dendrogram

> 계층으로 나눈 라벨들을 각각의 거리로 어떻게 나누어졌는지 보기 편하게 그린 플롯


# Practice

[Hierarchical Clustering in Python by George Pipis](https://medium.com/swlh/hierarchical-clustering-in-python-9646cfddee35) 

~~~ python

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn import datasets

iris = datasets.load_iris()
df=pd.DataFrame(iris['data'])

np.unique(iris.target,return_counts=True)

# Import the whiten function
from scipy.cluster.vq import whiten
scaled_data = whiten(df.to_numpy())

~~~

~~~ python

# Import the fcluster and linkage functions
from scipy.cluster.hierarchy import fcluster, linkage

# Use the linkage() function
distance_matrix = linkage(scaled_data, method = 'ward', metric = 'euclidean')

# Import the dendrogram function
from scipy.cluster.hierarchy import dendrogram

# Create a dendrogram
dn = dendrogram(distance_matrix)

# Display the dendogram
plt.show()

~~~

![image](https://user-images.githubusercontent.com/32366711/126991253-4001ad11-a4a1-4d3b-8890-8eef7e9c5063.png)

![image](https://user-images.githubusercontent.com/32366711/126991677-0998ef5a-5756-46f9-b3c4-c8f4cc279192.png)


~~~ python
# Assign cluster labels
df['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')

df['target'] = iris.target
fig, axes = plt.subplots(1, 2, figsize=(16,8))
axes[0].scatter(df[0], df[1], c=df['target'])
axes[1].scatter(df[0], df[1], c=df['cluster_labels'], cmap=plt.cm.Set1)
axes[0].set_title('Actual', fontsize=18)
axes[1].set_title('Hierarchical', fontsize=18)

axes[0].set_xlabel("sepal length")
axes[0].set_ylabel("sepal width")

axes[1].set_xlabel("sepal length")
axes[1].set_ylabel("sepal width")
~~~

![image](https://user-images.githubusercontent.com/32366711/127045519-b1e93b31-ed48-41bc-bb74-8c7acad26395.png)
