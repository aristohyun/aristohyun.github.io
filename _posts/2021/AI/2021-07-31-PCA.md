---
layout: post
title: "기계학습, PCA"
description: "Principal Component Analysis, 주성분 분석"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, PCA, Principal Component Analysis, kaggle]
use_math: true
redirect_from:
  - /kaggle/20
  - /blog/kaggle/20
---

* Kramdown table of contents
{:toc .toc}      


[iris 차원 감소](https://makeit.tistory.com/157)

# 차원 감소 기법

> 차원이 커지면            
> 1. 데이터를 시각적으로 표현하기 어려움,             
> 2. 데이터의 차원이 증가할수록 데이터 포인트 간의 거리 또한 증가하게 되므로, 이러한 데이터를 이용해 머신러닝 알고리즘을 학습 하게되면 모델이 복잡해지게 된다. 따라서, 오버피팅 (overfitting) 위험이 커짐


# PCA, 주성분 분석

> 차원 감소 기법            
> PCA는 데이터의 분산(variance)을 최대한 보존하면서 서로 직교하는 새 기저(축)를 찾아, 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간으로 변환하는 기법                
> 분산이 가장 큰 축을 첫 번째 주성분, 두 번째로 큰 축을 두 번째 주성분으로 놓이도록 새로운 좌표계로 데이터를 선형 변환 함


### 분산이 커야하는 이유

![image](https://user-images.githubusercontent.com/32366711/128202940-4ce0b611-80f1-4af1-9b5a-26f759e1743e.png){: width="400"}{: .aligncenter}

데이터를 최대한 보존한다는 뜻은 거리의 합이 최소가 되도록 한다는 뜻

직선과 데이터간의 거리의 합 $\sum \limits_ {i=1}^{n} b_ i ^2$ 이 최소가 되도록 하려고 함

그러나 데이터와 직선간의 거리는 수직으로 만났을 때를 기준으로 하기에 피타고라스 정리를 사용한다면,

$b^2$ 가 최소가 되도록 한다는 뜻은, $c^2$가 최대가 되도록 한다는 뜻과 같음 ($a^2$는 고정)

즉 $\sum \limits_ {i=1}^{n} c_ i ^2$가 최대가 된다는 것인데

현재 데이터를 평균이 0이 되도록 좌표계를 이동시켰으며, 주성분 직선은 원점을 지나가므로

c의 길이, 원점으로부터 $Z_ i$ 까지의 길이 의 합이 최대로 된어야 한다라는 뜻과 같다 

@\sum \limits_ {i=1}^{n} Z_ i ^2 = \sum \limits_ {i=1}^{n} (Z_ i - \bar Z )^2 @

즉 위의 식은 Z의 분산의 공식[^Dispersion]과 유사하게 되며

Z의 분산이 최대가 되야 한다와 동일한 의미를 지닌다


## 기본 개념

1. 데이터의 각 차원의 평균을 구해서, 평균이 0이 되도록 데이터들을 이동시킨다
2. 원점을 지나며, 데이터들에 가장 적합한 직선, 주성분[^principal]을 구한다
3. 각 직선들 중에서 분산이 가장 큰 축 2개에 데이터들을 정사영시켜 새로운 좌표점을 구하여 차원을 축소시킨다

![image](https://user-images.githubusercontent.com/32366711/128022725-288a9015-c382-4766-8e31-ac790a3e9c71.png){: .aligncenter}


### 적합한 직선

1. 데이터의 공분산 행렬 계산
2. 공분산 행렬로부터 고유값(주성분의 분산값)과 고유벡터(새로운 축, 주성분)를 계산
3. 고유값을 순서대로 정렬하고 그에 해당되는 고유벡터를 나열
4. 정렬된 고유벡터를 토대로 기존 변수를 변환


## 선형 결합

@ Z_ p = \alpha_ p ^ T X = \alpha_ {p1} X_ 1 +  \alpha_ {p2} X_ 2 + \cdots + \alpha_ {pp} X_ p @

- $ X_ 1, X_ 2, \cdots, X_ p$ : 원래 변수
- $ \alpha = [\alpha_ {i1}, \alpha_ {i2}, \cdots, \alpha_ {ip}] $ : i 번째 기저(계수)
- $Z_ 1, Z_ 2, \cdots, Z_ p$: 각 기저로 사영 변환 후의 변수(주성분, PC)


### 라그랑주 승수법

> 기본 가정        
> "제약 조건 g를 만족하는 f의 최솟값 또는 최댓값은 f와 g가 접하는 지점에 존재할 수도 있다          
> 또한 어떠한 함수 f의 최솟값 또는 최댓값은 극점에 존재할 수도 있으며, 다변수 함수의 극점은 전미분 df=0인 지점 중에 존재한다"      

PCA는 주성분 Z의 분산이 최대가 되어야 한다. 이때 Z는 $\alpha ^T X$ 과 같으므로 

@ Var(Z) = Var(\alpha ^T X) = \alpha ^T Var(X) \alpha = \alpha ^T C \alpha @[^aTCa]  (C는 공분산 행렬)

즉 $ \alpha ^T C \alpha $를 최대화 하는 $\alpha$를 찾아야 한다 (이때 $\alpha$의 길이는 1이라는 제한조건이 있음. 단위 벡터이기 때문)

위의 식을 라그랑주 승수법을 도입한 방정식을 세우면[^Lagrange]

@ 
\begin{align\*} 
 L &= f(x) - \lambda g(x) \\\ 
   &= \alpha ^T C \alpha - \lambda( \alpha^ T \alpha - 1 ) \\\ 
 \frac{\partial L}{\partial \alpha} &= C \alpha - \lambda \alpha = 0 \\\ 
 &= (C - \lambda I) \alpha = 0 
\end{align\*} 
@ 


## 고유값, 고유벡터

라그랑주 승수법에 의해[^matrix]               
@ AX = \lambda X \Rightarrow  (A-\lambda I)X = 0 \\\ 
\therefore det|A-\lambda I|=0
@    

어떤 정방행렬 A(여기서는 공분산 행렬 C)에 대해 다음을 만족하는 $\lambda$를 고유값(eigen value)이라고 하며,     
이때의 X를 고유벡터(eigen vector)라고 한다(여기서는 $\alpha$)

PCA에서 이 공분산 행렬의 고유값은 각 주성분의 분산과 같은 의미를 가지며, 고유벡터는 주성분의 방향(새로운 축의 방향)이 된다


## 차원 감소

여기까지의 방법은 기존의 데이터를 고유벡터에 투영시킨 것이며, 아직 차원의 수는 그대로이다

차원의 수를 줄이기 위해서는 구한 주성분중에서 몇개를 선택할것인지 정해야하는데

정하는 방법은

1. 주성분의 분산(고유값)이 유의미하게 낮아지는 Elbow Point에 해당하는 주성분 수를 선택하는 것이며
2. 일정 수준(보통 70% 이상)의 분산비를 보존하는 최소의 주성분을 선택하는 것이다


만일 분산이 가장 큰 주성분 1,2가 3,4와 비슷한 수치를 보인다면           
주성분 1,2를 선택하는 것이 데이터를 정확하게 대표한다고는 할 수 없겠지만,         
군집을 식별하는 데에는 사용할 수 있습니다         

![image](https://user-images.githubusercontent.com/32366711/128162226-27b95050-437d-4ebf-85f2-a31562c8880a.png){: width="500"}


[^principal]: 그 방향으로 데이터들의 분산이 가장 큰 방향벡터
[^Lagrange]: $\frac { \partial (X^T A X) }{\partial X} = X^T(A + A^T)$, $ \frac{\partial L}{\partial \alpha} = \alpha ^T(C + C^T) - \lambda (2 \alpha^T) = 2 \alpha^T C - 2\lambda \alpha^T = 0, (\alpha^T C - \lambda \alpha^T) ^T = C \alpha - \lambda \alpha = 0, (C-\lambda I) \alpha = 0 $
[^matrix]: 1. Ax = 0을 만족하는 솔루션 x의 집합이 존재하는데, 이것을 nullspace라고 부른다. 2. Ax = 0일때만 위의 식을 만족한다면, A는 역행렬이 존재한다.(invertible). 만약 Ax = 0 을 만족하는 해가 무수히 많다면, A는 비가역 행렬이다. 즉, 역행렬이 존재하지 않는다. 
[^aTCa]: $ Var(Z) = Var(\alpha ^T X) = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)^2 = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)(\alpha ^T x_ i)^T = \frac{1}{n} \sum \limits_ {i=1}^{n} (\alpha ^T x_ i)(\alpha ^T x_ i)^T = \frac{1}{n} \sum \limits_ {i=1}^{n} \alpha ^T(x_ i)(x_ i ^T) \alpha  = \alpha ^T Var(X) \alpha$
[^Dispersion]: $Var(X) = \frac {1}{n} \sum \limits_ {i=1}^{n} (X_ i - \bar x)^2$
