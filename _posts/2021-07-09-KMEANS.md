---
layout: post
title: "기계학습, Kmeans Clustering"
description: "KMEANS, k-평균 알고리즘"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, Kmeans Clustering]
use_math: true
redirect_from:
  - /2021/07/09/
---

* Kramdown table of contents
{:toc .toc}           

[참고 사이트](https://muzukphysics.tistory.com/entry/ML-13-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-k-means-Clustering-%ED%8A%B9%EC%A7%95-%EC%9E%A5%EB%8B%A8%EC%A0%90-%EC%A0%81%EC%9A%A9-%EC%98%88%EC%8B%9C-%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5){:target="_ blank"}

# K 평균 알고리즘    

> 주어진 데이터를 k개의 클러스터로 묶는 알고리즘(classification)으로,        
> 각 클러스터와 거리 차이(유클리드 거리)의 분산을 최소화하는 방식으로 동작한다.        
> 즉, 각 데이터는 가장 가까운 그룹의 특성을 따른다       

## 수행 과정

1. 클러스터의 수 K 정의
2. 각 데이터를 클러스터에 할당
3. 새로운 클러스터의 무게중심($ \mu $) 계산
4. 클러스터 재 분류
5. 경계가 변경되지 않을 때 까지 2~4 반복

![kmeans_animation](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Kmeans_animation_withoutWatermark.gif/330px-Kmeans_animation_withoutWatermark.gif)    

## 목표

> 각 집합별 중심점~집합 내 오브젝트간 거리의 제곱합을 최소로 하는 집합 S를 찾는 것
> 목적 함수의 오차를 줄여나가며 지역 최솟값 (local minimum) 을 발견했을 때 알고리즘을 종료함으로써 근사 최적해를 구한다

@
{\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}
@

## 알고리즘

@
V=\sum _ {i=1}^{k}\sum _ {x_ {j}\in S_ {i}}|x_ {j}-\mu _ {i}|^{2}
@ 


### 클러스터 설정

@
S_ {i}^{(t)}=\{x_ {p}:|x_ {p}-\mu _ {i}^{(t)}|^{2}\leq |x_ {p}-\mu_ {j}^{(t)}|^{2}\forall j,1\leq j\leq k\}
@ 


### 클러스터 중심 재조정

@
\mu _ {i}^{(t+1)}={\frac {1}{|S_ {i}^{(t)}|}}\sum \limits _ {x_{j}\in S_ {i}^{(t)}}x_ {j}
@


## K, 클러스터의 수 정의

1. Rule of Thumb
2. Elbow Method
3. 정보 기준 접근법

### Rule of Thumb

데이터 수가 n개 일 때,    
$ k = \sqrt{n/2} $

### Elbow Method

클러스터 수를 순차적으로 늘려가면서 결과를 모니터링 하는 방법    
하나의 클러스터를 추가했을 때, 이전보다 좋은 결과를 나타내지 않으면 이전의 클러스터 수를 최종 클러스터의 수로 설정함

### 정보 기준 접근법

클러스터링 모델에 대해 가능성을 계산하는 방법    
가능도[^1]를 계산할 수 있는 경우에 사용


## 초기화 기법

그렇다면 초기 클러스터는 어떻게 설정할 것인가?     

1. 무작위 분할 기법    
2. Forgy 초기화 기법     
3. MacQueen 기법     
4. Kaufman 기법      


### 무작위 분할 기법     

임의로 클러스터를 K개 만들어 데이터를 배당하는 것    
배당된 데이터들의 평균 값을 초기 $ \mu $값으로 설정하여 재분류를 하며 반복       
초기 클러스터가 데이터들에 대해 고르게 분포되기에 각 초기 클러스터의 무게중심들이 데이터 집합의 중심에 가깝게 위치하는 경향을 띈다      
이러한 특성으로 인해 K-조화 평균, 퍼지 k-평균 에서 선호됨    


### Forgy

임의의 데이터를 K개 선택하여 각 데이터들을 클러스터의 $ \mu $로 설정하는 것    
초기 클러스터가 임의의 k개의 점들에 의해 설정되기 때문에,각 클러스터의 무게중심이 중심으로부터 퍼져있는 경향을 띈다. 
이러한 특성 때문에 EM 알고리즘이나 표준 k-평균 알고리즘에서 선호된다


### MacQueen

Forgy와 마찬가지로 임의의 데이터를 K개 선택하여 각 데이터들을 클러스터의 $ \mu $로 설정    
이후 각 데이터들을 가장 가까운 클러스터를 찾아 데이터를 배당,     
해당 데이터가 클러스터에 배당되면 무게중심($ \mu $로 설정)을 다시 계산하며      
모든 데이터가 클러스트에 배당될때까지 반복       

### Kaufman

전체 데이터 집합중 가장 중심에 위치한 데이터를 첫번째 $ \mu_ 1 $로 설정하며, 이후 선택되지 않은 각 데이터들에 대해 가장 가까운 무게중심 보다 선택되지 않은 데이터 집합에 더 근접하게 위치한 데이터를 또 다른 $ \mu_ 2 $로 설정하는 것을 k번 반복


## 장점

- 적용이 쉬움    
- 데이터에 대한 사전 정보가 필요하지 않으며, 사전에 특정 변수에 대한 역할 정의가 필요하지 않음    
- 오직 데이터간 거리만이 분석에 필요    

## 단점

- 가중치와 적정 거리에 대한 정의가 필요
- 거리를 정의하는 것과 가중치를 결정하는 것이 어려움
- 클러스터의 수가 적합하지 않으면 결과가 좋지 못함
- 모든 데이터를 거리로만 판단하기에, 사전에 주어진 목적이 없어 겨과 해석이 어려울 수 있음


[^1]: 통계학에서 확률 분포의 모수가 확률 변수의 표집값과 일관되는 정도를 나타내는 값. 주어진 표집값에 대한 모수의 가능도는 이 모수를 따르는 분포가 주어진 관측값에 대햐여 부여하는 확률








