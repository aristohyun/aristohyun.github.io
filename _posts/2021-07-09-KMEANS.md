---
layout: post
title: "기계학습, Kmeans Clustering"
description: "KMEANS, k-평균 알고리즘"
categories: [MachineLearning]
tags: [Machine Learning, Unsupervised Learning, Kmeans Clustering]
use_math: true
redirect_from:
  - /2021/07/09/
---

* Kramdown table of contents
{:toc .toc}           

[참고 사이트](https://muzukphysics.tistory.com/entry/ML-13-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-k-means-Clustering-%ED%8A%B9%EC%A7%95-%EC%9E%A5%EB%8B%A8%EC%A0%90-%EC%A0%81%EC%9A%A9-%EC%98%88%EC%8B%9C-%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5){:target="_ blank"}

# K 평균 알고리즘    

> 주어진 데이터를 k개의 클러스터로 묶는 알고리즘(classification)으로,        
> 각 클러스터와 거리 차이(유클리드 거리)의 분산을 최소화하는 방식으로 동작한다.        
> 즉, 각 데이터는 가장 가까운 그룹의 특성을 따른다       

## 수행 과정

1. 클러스터의 수 K 정의
2. 각 데이터를 클러스터에 할당
3. 새로운 클러스터의 무게중심($ \mu $) 계산
4. 클러스터 재 분류
5. 경계가 변경되지 않을 때 까지 2~4 반복

![kmeans_animation](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Kmeans_animation_withoutWatermark.gif/330px-Kmeans_animation_withoutWatermark.gif)    

## 목표

> 각 집합별 중심점~집합 내 오브젝트간 거리의 제곱합을 최소로 하는 집합 S를 찾는 것
> 목적 함수의 오차를 줄여나가며 지역 최솟값 (local minimum) 을 발견했을 때 알고리즘을 종료함으로써 근사 최적해를 구한다

@
{\underset {\mathbf {S} }{\operatorname {arg\, min} }}\sum _ {i=1}^{k}\sum _ {\mathbf {x} \in S_ {i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_ {i}\right\|^{2}
@

## 알고리즘

@
V=\sum _ {i=1}^{k}\sum _ {x_ {j}\in S_ {i}}|x_ {j}-\mu _ {i}|^{2}
@ 


### 클러스터 설정

@
S_ {i}^{(t)}=\{x_ {p}:|x_ {p}-\mu _ {i}^{(t)}|^{2}\leq |x_ {p}-\mu_ {j}^{(t)}|^{2}\forall j,1\leq j\leq k\}
@ 


### 클러스터 중심 재조정

@
\mu _ {i}^{(t+1)}={\frac {1}{|S_ {i}^{(t)}|}}\sum \limits _ {x_{j}\in S_ {i}^{(t)}}x_ {j}
@


## K, 클러스터의 수 정의

1. Rule of Thumb
2. Elbow Method
3. 정보 기준 접근법

### Rule of Thumb

데이터 수가 n개 일 때,    
$ k = \sqrt{n/2} $

### Elbow Method

클러스터 수를 순차적으로 늘려가면서 결과를 모니터링 하는 방법    
하나의 클러스터를 추가했을 때, 이전보다 좋은 결과를 나타내지 않으면 이전의 클러스터 수를 최종 클러스터의 수로 설정함

### 정보 기준 접근법

클러스터링 모델에 대해 가능성을 계산하는 방법    
가능도[^1]를 계산할 수 있는 경우에 사용


## 초기화 기법

그렇다면 초기 클러스터는 어떻게 설정할 것인가?     

1. 무작위 분할 기법    
2. Forgy 초기화 기법     
3. MacQueen 기법     
4. Kaufman 기법      


### 무작위 분할 기법     

임의로 클러스터를 K개 만들어 데이터를 배당하는 것    
배당된 데이터들의 평균 값을 초기 $ \mu $값으로 설정하여 재분류를 하며 반복       
초기 클러스터가 데이터들에 대해 고르게 분포되기에 각 초기 클러스터의 무게중심들이 데이터 집합의 중심에 가깝게 위치하는 경향을 띈다      

~~~ python
def RandomPartition():
def RandomPartition():
    labels = np.array([random.choice(range(k)) for _ in range(len(samples))])
    
    for i in range(k):
        points = [ sepal_length_width[j] for j in range(len(sepal_length_width)) if labels[j] == i ]
        # points의 각 feature, 즉 각 좌표의 평균 지점을 centroid로 지정
        centroids[i] = np.mean(points, axis=0)
    
    plt.scatter(x, y, c=labels, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red')
    plt.show()
    
    label = makeCluster(centroids)
    
    plt.scatter(x, y, c=label, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red')
    plt.show()
    return centroids

~~~

![image](https://user-images.githubusercontent.com/32366711/125301366-e0f09f80-e365-11eb-8395-7f8e57dc1766.png)

![image](https://user-images.githubusercontent.com/32366711/125301392-e77f1700-e365-11eb-8366-59e870fa6fab.png)


### Forgy

임의의 데이터를 K개 선택하여 각 데이터들을 클러스터의 $ \mu $로 설정하는 것    
초기 클러스터가 임의의 k개의 점들에 의해 설정되기 때문에,각 클러스터의 무게중심이 중심으로부터 퍼져있는 경향을 띈다.      

~~~ python
def Forgy():
    # 주어진 데이터 중 k개를 선택
    labels = np.zeros(len(samples))
    cen_index = random.sample(range(len(samples)), 3)
    centroids = np.array([[samples[i][0],samples[i][1]] for i in cen_index])

    labels = np.zeros(len(samples))
    # 각 데이터를 순회하면서 centroids와의 거리를 측정합니다
    for i in range(len(samples)):
        distances = np.zeros(k)	# 초기 거리는 모두 0으로 초기화 해줍니다
        for j in range(k):
            distances[j] = distance(sepal_length_width[i], centroids[j])
            cluster = np.argmin(distances)	# np.argmin은 가장 작은 값의 index를 반환합니다
            labels[i] = cluster
    
    plt.scatter(x, y, c=labels, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red')
    plt.show()
    
    return centroids
~~~

![image](https://user-images.githubusercontent.com/32366711/125299564-39bf3880-e364-11eb-977f-46745f0a5ac5.png)


### MacQueen

Forgy와 마찬가지로 임의의 데이터를 K개 선택하여 각 데이터들을 클러스터의 $ \mu $로 설정    
이후 각 데이터들을 가장 가까운 클러스터를 찾아 데이터를 배당,     
해당 데이터가 클러스터에 배당되면 무게중심($ \mu $로 설정)을 다시 계산하며      
모든 데이터가 클러스트에 배당될때까지 반복       

~~~ python
def MacQueen():
    # 주어진 데이터에서 k개 선택 ( Forgy와 동일 )
    # but, 다른 데이터들을 클러스터에 추가할 때 마다 초기 mu를 재설정함
    clusters = [[] for _ in range(k)]
    labels = np.zeros(len(samples))
    cen_index = random.sample(range(len(samples)), 3)
    centroids = np.array([[samples[i][0],samples[i][1]] for i in cen_index])

    for i in range(len(samples)):
        distances = np.zeros(k)	# 초기 거리는 모두 0으로 초기화 해줍니다
        for j in range(k):
            distances[j] = distance(sepal_length_width[i], centroids[j])
        cluster = np.argmin(distances)	# np.argmin은 가장 작은 값의 index를 반환합니다
        labels[i] = cluster
        clusters[cluster].append(sepal_length_width[i])
        centroids[cluster] = np.mean(clusters[cluster], axis=0)

    plt.scatter(x, y, c=labels, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red')
    plt.show()
    
    return centroids
~~~

![image](https://user-images.githubusercontent.com/32366711/125299633-4b084500-e364-11eb-9a67-fb127a46b3d4.png)


### Kaufman

전체 데이터 집합중 중심에서 가장 가까운 데이터를 첫번째 $ \mu_ 1 $로 설정하며,        
이후 선택되지 않은 각 데이터들에 대해 가장 가까운 무게중심 보다 선택되지 않은 데이터 집합에 더 근접하게 위치한 데이터를 또 다른 $ \mu_ 2 $로 설정하는 것을 k번 반복      

1. Selects the most centrally located object as the first centroid ( $ \mu_ 1 $ )             
2. For every Nonselected objext $ w_ i $   
    2.1 for other Nonselected objext $ w_ j $, calculate $ C_ {ji} = max(D_ {j} - d_ {ji}) $         
      - where $ d_ {ji} $ is the distance between $ w_ i $ and $ w_ j $      
      - $ D_ j is the distance between $w_ j $ and nearest centroid      
    2.2 Calculate $ \sum \limits_ {j}^{} C_ {ji} $    
3. Select $ w_ i $ which macimizes  $ \sum \limits_ {j}^{} C_ {ji} $    
4. while get k centroids repeat 2, 3    

![image](https://user-images.githubusercontent.com/32366711/125279474-f27a7d00-e34e-11eb-8195-1cdfb1d5f449.png)

~~~ python
def nearest_centroid(x_y, centroids):
    distances = []
    for i in range(k):
        if centroids[i][0] == -1 and centroids[i][1] == -1:
            break
        distances.append(distance(x_y, centroids[i]))
    cluster = np.argmin(distances)	# np.argmin은 가장 작은 값의 index를 반환합니다
    return centroids[cluster]
    
def Kaufman():
    # 데이터 집합 중 가장 중심에 위치한 데이터를 첫번째 중심으로 설정한다. 
    # 이후 선택되지 않은 각 데이터들에 대해, 가장 가까운 무게중심 보다 선택되지 않은 데이터 집합에 
    # 더 근접하게 위치한 데이터를 또 다른 중심으로 설정하는 것을 k번 반복
    
    distances = np.zeros(len(samples))
    centroids_index = np.array([-1 for _ in range(k)], int)
    centroids = np.array([[-1,-1] for _ in range(k)], float)
    
    # 1. 전체 데이터의 중심을 구해서, 가장 가까운 데이터 찾기
    middle = np.mean(sepal_length_width, axis=0)
    for i in range(len(samples)):
        distances[i] = distance(sepal_length_width[i], middle)
    nearest_index = np.argmin(distances)
    centroids[0] = sepal_length_width[nearest_index]   # 초기 mu
    centroids_index[0] = nearest_index

    # 2. 다른 데이터들에 D-d 값이 가장 큰 데이터 찾기
    for n in range(1, k):
        C = np.zeros(samples.size)
        for i in range(len(samples)):
            w_i = sepal_length_width[i]
            for j in range(len(samples)):     
                w_j = sepal_length_width[j]
                D_j = distance(nearest_centroid(w_j, centroids), w_j)
                d_ji = distance(w_j, w_i)
                C[i] += max(D_j - d_ji, 0)
        # Select C max
        centroids_index[n] = np.argmax(C)
        centroids[n] = sepal_length_width[centroids_index[n]]
        
    labels = makeCluster(centroids)
        
    plt.scatter(x, y, c=labels, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c='red')
    plt.show()

    return centroids

Kaufman()
~~~

![image](https://user-images.githubusercontent.com/32366711/125299692-58253400-e364-11eb-87cf-8070ceb0bb79.png)


## K mean 장점

- 적용이 쉬움    
- 데이터에 대한 사전 정보가 필요하지 않으며, 사전에 특정 변수에 대한 역할 정의가 필요하지 않음    
- 오직 데이터간 거리만이 분석에 필요    

## K mean 단점

- 가중치와 적정 거리에 대한 정의가 필요
- 거리를 정의하는 것과 가중치를 결정하는 것이 어려움
- 클러스터의 수가 적합하지 않으면 결과가 좋지 못함
- 모든 데이터를 거리로만 판단하기에, 사전에 주어진 목적이 없어 겨과 해석이 어려울 수 있음


[^1]: 통계학에서 확률 분포의 모수가 확률 변수의 표집값과 일관되는 정도를 나타내는 값. 주어진 표집값에 대한 모수의 가능도는 이 모수를 따르는 분포가 주어진 관측값에 대햐여 부여하는 확률








