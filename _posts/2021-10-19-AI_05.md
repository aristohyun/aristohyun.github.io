---
layout: post
title: "AI, 5장 신경망"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/11/15/
---

* Kramdown table of contents
{:toc .toc}  

![image](https://user-images.githubusercontent.com/32366711/137842267-9ff7d417-afc9-4e1e-ac4b-44205b4f1b92.png)


![image](https://user-images.githubusercontent.com/32366711/137843601-0c1e04ed-4172-41f0-90d1-8a2280efabd5.png)

# 신경망

뉴럴네트워크==딥러닝
신경망, neural network, artificial neural network

함수기반 문제, 지식을 함수로, 모델을 함수로

결정트리 -> 함수 사용하지않는 전형적문제
신경망은 함수의 전형적문제

> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야

인간의 두뇌는 신경세포로되어있더라. 신경세포가하는일을수학적으로모델링하고 잘사용하면 지능을구현할수있지않을까


퍼셉트론 - 

함수의 형태를 사람이 만들어주지 않고, 데이터만 주면 스스로 만들어서 문제를 해결한다

그런데 가장 간단한 문제인 XOR문제도 해결하지 못하더라

그런데 하드웨어의 발달등으로 다층퍼셉트론 모델을 찾고, 할 수있게 되었다.
인공신경망에 대한 연구가 다시 활성화되었다

다층퍼셉트론을 수백개씩 쌓게 되면서 모델의 성능이 비약적으로 또 높아졌다

수상돌기로부터 신호를 받고, 신호가 어느정도 결합되면 축색돌기를 동해 전달된다
시냅스를 통해서 다른 수상돌기로 연결이되어 신호(화학물질)를 전달한다

전이값, 포텐셜이 얼마이상이 되면 스파크가 튄다

bias값으로 1을 주면 두쌍의 곱의 합으로 나타낼 ㅅ ㅜ있음

입력 값과 가중치의 내적으로 표현 가능

내적연산의 특징은 두 벡터값이 얼마나 유사한지, 일치하는지를 알 수 있음

퍼셉트론에서 단순한 입력과 가중치의 내적뿐이 아님


# 퍼셉트론 모델

공간을 선형방정식으로 구분하는 것

@
s = \sum \limits _ {i=1}^{d} w_ i x_ i + b = \sum \limits _ {i=0}^{d} w_ i x_ i
@

~~~ python

def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0 
    return activation
~~~

or 연산 and 연산 모두 잘 하지만, 

xor연산은 해결하지 못함

선형 분리 가능 문제(선형방정식, 1차식)에 대해서만 가능



# 다층 퍼셉트론 모델

직선 하날는 안되니까 직선 두개를 그어보자

퍼셉트론을 2개이상 사용해보자

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^T x)
@

입력층       은닉층 (히든레이어)  출력층 
뉴런역할 X    

퍼셉트론은 학습하는걸 알아냈지만,
다층퍼셉트론은 학습하는게 문제다

학습 어떻게하냐

회귀모델에서 오차함수를 정의하고 경사하강법을 통해서 움직이다보면 오차를 최소화할수있다고 했는데,

오차함수를 파라미터에 대해서 편미분할 수 있어야 함

퍼셉트론에서는 이게 안됨

함수를 쓸 때, 활성화함수를 계단모양으로 사용해서 미분할수없었음

미분이 안되는 계단함수를 미분이 되는 시그모이드로 바꿔보자
이를 통해 다층퍼셉트론의 학습이 가능해짐

## 활성화 함수

### 계단 함수

~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0 
~~~

### 시그모이드 함수[^sig]

[^sig]: 계단형에서 살짝 느슨한 시그모이드 함수를 만듦으로써 미분이 가능해졌다. 그러나 미분값이 작아서 학습을 못하는 문제가 있었다. 이를 딥러닝에서 해결했다

@
\sigma(x,a) = \frac{1}{1 + e^{-ax}} \\\ 
\sigma '(x,a) = a \sigma(x,a)(1-\sigma(x,a))
@

미분값이 0.25 1보다 작다               

다층퍼셉트론은 층을 많이 쌓을수록 성능이 좋아질 수 있는데              
시그모이드는 층을 많이 쌓을수록 0에 가까워진다             

학습을 못해서 성능향상이 안된다                    

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x,a=1):
    return a*sigmoid(x,a)*(1-sigmoid(x,a))
~~~

![image](https://user-images.githubusercontent.com/32366711/139608650-9746a52c-90de-496b-a68c-519b4ea137af.png)


### 쌍곡 탄젠트 함수

@
\tanh(x) = \frac{e^{2x}-1}{e^{2x}+1} \\\ 
\tanh'(x) = 1 - \tanh^2(x)
@

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

![image](https://user-images.githubusercontent.com/32366711/139608677-78c46692-745c-4f9d-9747-f73d5c32f9ed.png)

## 다층 퍼셉트론 MLP의 동작

![image](https://user-images.githubusercontent.com/32366711/139608900-359ea52b-9da3-4f88-b60e-60ae5a7654ec.png)

층이 2개           
입력층은 안세고 말하는 경우가 많음 

weight pd, kj

j번째 은닉층의 값 : 입력값 * 가중치의 합

$
zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji}x_ i + u_ {j0} \;\;\; (1 \leq j \leq p) \\\ 
z_ j = f(zsum_ j)
$

$
osum_ k = \sum \limits_ {j=1}^{p} v_ {jk}v_ k + v_ {0k} \;\;\; (1 \leq k \leq p) \\\ 
o_ k = f(osum_ k)
$

전향망 Feed Forward

### MLP의 학습

입력 : ($x_ 1, x_ 2, \cdots, x_ d$)
기대 출력 : $y_ k$
MLP 출력 : $o_ k$

#### 학습 목표

기대 출력과 MLP 출력이 최대한 비슷해지도록 가중치를 변경하는 것            
[^error]
@
E = \frac{1}{2}(o_ k - y_ k)^2
@

[^error]: 오차함수, 손실함수, 목적함수, 에러펑션

경사 하강법 사용 

@
v_ {jk}^{(t+1)} = v_ {jk}^{(t)} - \eta \frac{\partial E}{\partial v_ {jk}} \\\ 
u_ {ij}^{(t+1)} = u_ {ij}^{(t)} - \eta \frac{\partial E}{\partial u_ {ij}} 
@


# 미분

> 함수 f(x)의 변수 x에 대한 순간변화율

@
f'(x) = \frac{df(x)}{dx} = \lim_ {\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
@


## 연쇄 법칙, Chain Rule

@
y = f(g(x)) \\\ 
y = f(u), u = g(x)
@
@
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
@

#### 체인 룰 유도 과정

![image](https://user-images.githubusercontent.com/32366711/139609869-61c18753-1397-496a-977d-3b691ff6759b.png)

## 편미분

partial differentiation

> 다변수 함수에 대하여, 하나의 변수에만 집중하고,            
> 나머지 변수는 상수로 생각하고 미분하는 방법                   

## 다변수 함수의 연쇄 법칙

$
f(x(t),y(t))
$
@
\frac{d \; f(x(t),y(t))}{d \; t} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}
@

![image](https://user-images.githubusercontent.com/32366711/139610152-72c25553-b5f9-4063-b7b3-720243a8acda.png)

$
g(x(t),y(t),z(t))
$
@
\frac{d \; g(x(t),y(t),z(t))}{d \; t} = \frac{\partial g}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial g}{\partial y}\frac{\partial y}{\partial t}+ \frac{\partial g}{\partial z}\frac{\partial z}{\partial t}
@

## 그레디언트

> 함수 f(x,y,z)의 각 변수에 대한 편미분을 성분으로 갖는 <red>벡터</red>

@
\bigtriangledown f(x,y,z) = \begin{bmatrix}
\frac{\partial f}{\partial x} \\\ 
\frac{\partial f}{\partial y} \\\ 
\frac{\partial f}{\partial z}
\end{bmatrix}
@

# 다층 퍼셉트론의 학습

`오차 역전파 알고리즘, Error back propagation algorithm`

![image](https://user-images.githubusercontent.com/32366711/139611170-65d1fa86-83ad-4129-92f4-6d6bb7c49e23.png)

중간 가중치에서의 미분값을 구하려고 보니까
입력값 * 앞의 값의 오차값과 비슷하더라

모든 파라미터에 대해서 편미분값을 가지고 경사하강법을 적용해야함

오차값이 체인룰로 뒤로 넘어올때 가중치만큼 넘어옴

그런데 매번 미분을 할 때 마다 라그랑주 함수를 미분해서 넘어오다보니 0.25씩 계속 줄어서 나중에는 학습에 영향을 미치지 못함


# 분류 문제의 종류

- 이진 분류 : 출력이 2가지. True, False, 출력이 1개일 수도 있음 A, 1-A
- 다부류 분류 : 3개 이상의 부류 중에서 하나 선택
- 다중레이블 분류 : 하나의 대상에 대해서 여러 개의 부류 지정 가능

# 분류 문제의 출력값 표현

정수 인코딩 : 문자열 등에 대해서 정수 번호를 부여해서 표현

원 핫 인코딩 : 1,0,0  0,1,0  0,0,1 

# 오차 함수(손실 함수)

> 기대하는 출력과 모델의 출력 차이를 축적하여 표현하는 함수



## 회귀 문제의 오차 함수

최소제곱법 등

## 이진 분류 문제의 오차 함수

시그모이드 함수 사용
1일 확률이 p(x)일때, 0일 확률은 1-p(x)

가능도

@
p(D;w) = \Pi_ {i=1}^{N} y(x_ i, w)^{t_ i}\{1-y(x_ i, w)\}^{1 - t_ i}
@

그런데 1보다 작은 값을 계속 곱해주면 너무 작은 값이 나오게 됨 (언더플로우)

그래서 로그를 씌워줌. 그런데 그레디언트 디센트 방법을 쓰기위해 최소값을 주도록 하기 위해 - 를 붙여줌


@
-log \; \Pi_ {i=1}^{N} y(x_ i, w)^{t_ i} \{1-y(x_ i, w)\}^{1 - t_ i} \\\ 
= -\sum \limits_ {i=1}^{N} ( t_ i \; log \; y(x_ i, w) + (1-t_ i)log(1-y \; (x_ i, w)) )
@


## 다부류 분류 문제의 오차 함수

출력이 확률인것 처럼 나타내고 싶음

그래서 출력이 0보다 크고, 출력의 합이 1이 되도록 만듬

### 소프트맥스 층

학습 파라미터는 없기에 층이 아니기도 함

1. 출력이 0이상이여야 함 -> 지수함수
2. 합이 1이 되어야 함 -> 전체를 다 더해서 비율로 나타냄

~~~ python
def softmax(x):
  return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

출력값을 구했으니까 손실값을 구하려고 함

참값에 해당하는 확률치

교차엔트로피, 크로스 엔트로피


## 다중레이블 분류 문제의 오차 함수

다중레이블 분류에서는 오차함수로 최소제곱법을 쓸 수 있다

교차엔트로피도 쓸 수 있다
대신 t에서 1의 값이 2


# RBF 

## RBF 함수
베타의 값이 클수록 좁아짐

면적이 1이되게는 안했음

가우시안 분포와 유사하지만 가우시안분포는 아님

## RBF 망(RBF network)

뮤 베타 가중치가 학습대상이 됨

오차제곱합을 쓰면됨
그 오차함수에 대해서 경사하강법을 적용하게되면
주어진입력에 대해 파라미터들을 결정할수있음

파라미터가 많음ㄴ 식이 복잡해짐

# 딥러닝

> 딥러닝 신경망 모델 기반의 기계 학습 기법

deep : 층이 많음 (shallow : 층이 얕음)

층이 많을수록 데이터도 많이 필요, 자원도 많이필요

## 딥러닝 신경망

심층 신경망, deep neural network

### 비교

일반 신경망

데이터에서 특징을 추출, 이건 직접했음. 뭐가 더 좋은 성능을 내는지 직접

딥러닝 신경망
층이 많으니까 그냥 좋은 성능이 나올때 까지 학습. 그럼 필요 특징을 알아서 뽑아내서 사용

## 기울기 소멸 문제

미분값이 0.25이하, 이를 계속 곱하다ㅗ니 그레디언트가 너무작아짐

### 완화

ReLU

## 가중치 초기화

시작하는 위치, 초기화를 잘 시켜야함

### 개선된 가중치 초기화 방법

균등분포 초기화에서는 무작위로 값을 추출해서 초기화

~~~python
sd = np.sqrt(6/(layer_size[i+1]+ layer_size[i])) 
W = np.random.uniform(-sd, sd, layer_size[i]* layer_size[i-1]).reshape(layer_size[i+1], layer_size[i])
~~~

제이비어 초기화는 N(0,1)[^N]에서 무작위로 선택

~~~python
W = np.random.randn(layer_size[i+1], layer_size[i]) * np.sqrt(1/layer_size[i])
~~~

허he 초기화
~~~python
W = np.random.randn(layer_size[i+1], layer_size[i]) * np.sqrt(2/layer_size[i])
~~~
[^N]: 평균0, 표준편차1인 표준 정규분포

제한적 볼츠만 머신 : 비지도 학습법 값을 한층 올려서 계산했다가 내려오는 방식

인접 층간의 가중치를 직교하는 벡터
하나의 뉴런은 입력과 가중치의 내적 = 정사영

각각의 벡터들이 직교가 되게 하자. 서로 다른 특징을 가진 값으로 나누자

열벡터, 행벡터가 직교하게 되어있음

대각행렬을 제외한 값은 0


## 용어

학습주기(epoch, 에포크) 
전체 데이터에 대해서 신경망 모델을 한번의 학습 과정을 완료하는 것

배치(batch)
신경망의 가중치를 한번 수정할 때 사용되는 데이터
    배치 크기(batch size) 
    하나의 배치에 포함되는 데이터의 개수

iteration(반복) 
한 번의 학습주기를 완료하기 위해 수행되는 배치의 처리 회수
iteration 개수 = (전체 데이터 개수)/(배치 크기)


## 과적합 문제

데이터는 내부적으로 불확실성을 가질 수 있기 때문

표본집합은 모집합과 같은 분포를 가진다 라고 가정

![image](https://user-images.githubusercontent.com/32366711/140671030-34469c5a-ee79-457d-b578-08f4f316b9bc.png)

3가지 데이터로 나눔

학습용 데이터
테스트 데이터
검증용데이터 : 과적합이 생기는 순간을 판단

### 규제화(Regularization) 기법

> 오차 함수를 오차 항과 모델 복잡도 항으로 정의

모델이 복잡해 지면 과적합이 될 수 있으므로, 모델 복잡도를 패널티 항으로 추가

오차함수 = 오차항 + a 모델 복잡도 항

a = 하이퍼파라미터

가우시안분포를 쓰는 이유
정확한 하나의 값이 아니라
참값의 확률은 높고, 주변값은 낮다


### 드롭아웃(Dropout)


문제 : 액티베이션 비율이 낮아지니까
최종값에서 드랍아웃 비율만큼 나눠줌

실사용에서는 그대로

### 배치 졍규화(Normalization)

Normalization / Standardization 표준화

보통은 아웃풋가중치부터 순차로 바꿈

## 가중치 학습 기법

경사하강법 : 경사 반대방향
경사가 너무 크면, 반대쪽까지 갈 수 있기 때문에 러닝레이트 곱해줌
미니배치로하면, 값 데이터별 그레이던트 곘ㄴ해서 평균사용

모멘텀 사용 경사 하강법


NAG
모멘텀만큼 이동한 곳에서 그레디언트 계산

Adagrad
가중치 별로 다른 학습율 사용

Adadelta

RMSprop

ADAM
1차모멘텀

2차모멘텀 : 제곱


# CNN Model

`Convolutional Neural Network Models`

> 시각 피질 구조에서 영감을 받아 만들어진 딥러닝 신경망 모델

스프라이드 : 보폭

패딩 : 확장된 배열, 빈칸


컬러이미지는?
하나의 픽셀을 3개의 색상값으로 표현함
컨볼루젼 필터를 씌웠었는데 이건 입력이 3개, 그러면 필터도 3개
빨강필터 초록필터 파랑필터

컨볼루전필터가 몇개냐에 따라 출력채널이 만들어진다

5개의 필터가 주어진다 5개의 채널이 만들어진다

3 * 3 필터 3개 = 27개의 데이터 + 편차항 =28
채널5개 = 28 * 5 = 140

멀티레이어 퍼셉트론 입력 100개, 가중치 100개,
100개의 뉴런 = 만개의 가중치
상대적으로 너무 많음

가중치를 공유하게되서 그렇게 많이 늘지 않음
가중치의 갯수가 적다

가중치방향으로 프로젝션시켰을때 값이 얼만지, 가중치가 가르키는 특징이 얼마나 가지고있는지를 계산하는게 내적이다.

컨볼루젼연산을하게되면, 필터에 의해서 표현되는 벡터, 특징을 얼마나 많이 가지고있는지 계산하는것.

주어진 데이터로 잘 학습할 수 있도록 가중치, 필터가 자동으로 결정되서 필요한특징을 뽑게된다

피쳐 엔지니어링, 입출력에 대한 정보만 주게되면 입출력에 대한 관계를 신경망이 효과적으로 추출하게 된다

## 풀링

> 특징지도, 피쳐맵을 줄이는 역할               
> 일정 크기의 블록을 통합하여 하나의 대푯값으로 대체하는 연산          

메모리 크기와 계산량 감소를 위해

성질을 얼만큼 가지고 있느냐


최대값 풀링, max pooling
미분X -> 최대값에 대해서만 미분하기도 함
평균값 풀링, average pooling
미분O
확률적 풀링, stochastic pooling

## CNN의 구조

완전연결층을 쓰게된다
덴스레이어

순서는 상관없다, 다만 여러개중에 가장 좋은거 선택
분류에는 마지막에 소프트맥스추가

### 가중치 학습

기존 미분방법은 잘안쓴다. 너무오래걸림

자동미분


# 컨볼루션 연산과 영상 분류 CNN 모델

## 컨볼루션 형태

- 단일 채널 컨볼루션
- 다중 채널 2D 컨볼루션
    - 컴퓨터 그래픽스(행렬 연산, 좌표값 계산) 
- 다중 채널 3D 컨볼루션
    - 의료 영상
- 1x1 컨볼루션
    - 가중치 계산, 채널 갯수 조절
- 디컨볼루션 (transposed convolution)
    - 기본적으로 컨볼루션 연산은 크기가 줄게 되어있음
    - 필요에따라 늘리고 싶을 때도 있음
- 팽창 컨볼루션
- 공간 분할 컨볼루션
    - 계산량이 줄어듬 -> 시간절약
- 깊이별 분할 컨볼루션
- 집단 컨볼루션
- 채널섞기 집단 컨볼루션

## 물체 인식 CNN 모델

### LeNet 모델

> Conv - Pll - Conv - Pool - Conv - FC - FC(SM)[^LeNet5]

[^LeNet5]: Pool은 층이 아니기 때문에 5계층 

### ILSVRC 대회

ImageNet 데이터베이스

이미지 분류 문제

#### AlexNet

