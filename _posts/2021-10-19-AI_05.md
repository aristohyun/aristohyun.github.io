---
layout: post
title: "AI, 5장 신경망"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/10/19/
---

* Kramdown table of contents
{:toc .toc}  

![image](https://user-images.githubusercontent.com/32366711/137842267-9ff7d417-afc9-4e1e-ac4b-44205b4f1b92.png)


![image](https://user-images.githubusercontent.com/32366711/137843601-0c1e04ed-4172-41f0-90d1-8a2280efabd5.png)
# 신경망

뉴럴네트워크==딥러닝
신경망, neural network, artificial neural network

함수기반 문제, 지식을 함수로, 모델을 함수로

결정트리 -> 함수 사용하지않는 전형적문제
신경망은 함수의 전형적문제

> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야

인간의 두뇌는 신경세포로되어있더라. 신경세포가하는일을수학적으로모델링하고 잘사용하면 지능을구현할수있지않을까


퍼셉트론 - 

함수의 형태를 사람이 만들어주지 않고, 데이터만 주면 스스로 만들어서 문제를 해결한다

그런데 가장 간단한 문제인 XOR문제도 해결하지 못하더라

그런데 하드웨어의 발달등으로 다층퍼셉트론 모델을 찾고, 할 수있게 되었다.
인공신경망에 대한 연구가 다시 활성화되었다

다층퍼셉트론을 수백개씩 쌓게 되면서 모델의 성능이 비약적으로 또 높아졌다

수상돌기로부터 신호를 받고, 신호가 어느정도 결합되면 축색돌기를 동해 전달된다
시냅스를 통해서 다른 수상돌기로 연결이되어 신호(화학물질)를 전달한다

전이값, 포텐셜이 얼마이상이 되면 스파크가 튄다

bias값으로 1을 주면 두쌍의 곱의 합으로 나타낼 ㅅ ㅜ있음

입력 값과 가중치의 내적으로 표현 가능

내적연산의 특징은 두 벡터값이 얼마나 유사한지, 일치하는지를 알 수 있음

퍼셉트론에서 단순한 입력과 가중치의 내적뿐이 아님


# 퍼셉트론 모델

공간을 선형방정식으로 구분하는 것

@
s = \sum \limits _ {i=1}^{d} w_ i x_ i + b = \sum \limits _ {i=0}^{d} w_ i x_ i
@

~~~ python

def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0 
    return activation
~~~

or 연산 and 연산 모두 잘 하지만, 

xor연산은 해결하지 못함

선형 분리 가능 문제(선형방정식, 1차식)에 대해서만 가능



# 다층 퍼셉트론 모델

직선 하날는 안되니까 직선 두개를 그어보자

퍼셉트론을 2개이상 사용해보자

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^T x)
@

입력층       은닉층 (히든레이어)  출력층 
뉴런역할 X    

퍼셉트론은 학습하는걸 알아냈지만,
다층퍼셉트론은 학습하는게 문제다

학습 어떻게하냐

회귀모델에서 오차함수를 정의하고 경사하강법을 통해서 움직이다보면 오차를 최소화할수있다고 했는데,

오차함수를 파라미터에 대해서 편미분할 수 있어야 함

퍼셉트론에서는 이게 안됨

함수를 쓸 때, 활성화함수를 계단모양으로 사용해서 미분할수없었음

미분이 안되는 계단함수를 미분이 되는 시그모이드로 바꿔보자
이를 통해 다층퍼셉트론의 학습이 가능해짐

## 활성화 함수

### 계단 함수

~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0 
~~~

### 시그모이드 함수[^sig]

[^sig]: 계단형에서 살짝 느슨한 시그모이드 함수를 만듦으로써 미분이 가능해졌다. 그러나 미분값이 작아서 학습을 못하는 문제가 있었다. 이를 딥러닝에서 해결했다

@
\sigma(x,a) = \frac{1}{1 + e^{-ax}} \\\ 
\sigma '(x,a) = a \sigma(x,a)(1-\sigma(x,a))
@

미분값이 0.25 1보다 작다               

다층퍼셉트론은 층을 많이 쌓을수록 성능이 좋아질 수 있는데              
시그모이드는 층을 많이 쌓을수록 0에 가까워진다             

학습을 못해서 성능향상이 안된다                    

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x,a=1):
    return a*sigmoid(x,a)*(1-sigmoid(x,a))
~~~

![image](https://user-images.githubusercontent.com/32366711/139608650-9746a52c-90de-496b-a68c-519b4ea137af.png)


### 쌍곡 탄젠트 함수

@
\tanh(x) = \frac{e^{2x}-1}{e^{2x}+1} \\\ 
\tanh'(x) = 1 - \tanh^2(x)
@

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

![image](https://user-images.githubusercontent.com/32366711/139608677-78c46692-745c-4f9d-9747-f73d5c32f9ed.png)

## 다층 퍼셉트론 MLP의 동작

![image](https://user-images.githubusercontent.com/32366711/139608900-359ea52b-9da3-4f88-b60e-60ae5a7654ec.png)

층이 2개           
입력층은 안세고 말하는 경우가 많음 

weight pd, kj

j번째 은닉층의 값 : 입력값 * 가중치의 합

$
zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji}x_ i + u_ {j0} \;\;\; (1 \leq j \leq p) \\\ 
z_ j = f(zsum_ j)
$

$
osum_ k = \sum \limits_ {j=1}^{p} v_ {jk}v_ k + v_ {0k} \;\;\; (1 \leq k \leq p) \\\ 
o_ k = f(osum_ k)
$

전향망 Feed Forward

### MLP의 학습

입력 : ($x_ 1, x_ 2, \cdots, x_ d$)
기대 출력 : $y_ k$
MLP 출력 : $o_ k$

#### 학습 목표

기대 출력과 MLP 출력이 최대한 비슷해지도록 가중치를 변경하는 것            
[^error]
@
E = \frac{1}{2}(o_ k - y_ k)^2
@

[^error]: 오차함수, 손실함수, 목적함수, 에러펑션

경사 하강법 사용 

@
v_ {jk}^{(t+1)} = v_ {jk}^{(t)} - \eta \frac{\partial E}{\partial v_ {jk}} \\\ 
u_ {ij}^{(t+1)} = u_ {ij}^{(t)} - \eta \frac{\partial E}{\partial u_ {ij}} 
@


# 미분

> 함수 f(x)의 변수 x에 대한 순간변화율

@
f'(x) = \frac{df(x)}{dx} = \lim_ {\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
@


## 연쇄 법칙, Chain Rule

@
y = f(g(x)) \\\ 
y = f(u), u = g(x)
@
@
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
@

#### 체인 룰 유도 과정

![image](https://user-images.githubusercontent.com/32366711/139609869-61c18753-1397-496a-977d-3b691ff6759b.png)

## 편미분

partial differentiation

> 다변수 함수에 대하여, 하나의 변수에만 집중하고,            
> 나머지 변수는 상수로 생각하고 미분하는 방법                   

## 다변수 함수의 연쇄 법칙

$
f(x(t),y(t))
$
@
\frac{d \; f(x(t),y(t))}{d \; t} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}
@

![image](https://user-images.githubusercontent.com/32366711/139610152-72c25553-b5f9-4063-b7b3-720243a8acda.png)

$
g(x(t),y(t),z(t))
$
@
\frac{d \; g(x(t),y(t),z(t))}{d \; t} = \frac{\partial g}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial g}{\partial y}\frac{\partial y}{\partial t}+ \frac{\partial g}{\partial z}\frac{\partial z}{\partial t}
@

## 그레디언트

> 함수 f(x,y,z)의 각 변수에 대한 편미분을 성분으로 갖는 <red>벡터</red>

@
\bigtriangledown f(x,y,z) = \begin{bmatrix}
\frac{\partial f}{\partial x} \\\ 
\frac{\partial f}{\partial y} \\\ 
\frac{\partial f}{\partial z}
\end{bmatrix}
@

# 다층 퍼셉트론의 학습

`오차 역전파 알고리즘, Error back propagation algorithm`

![image](https://user-images.githubusercontent.com/32366711/139611170-65d1fa86-83ad-4129-92f4-6d6bb7c49e23.png)

중간 가중치에서의 미분값을 구하려고 보니까
입력값 * 앞의 값의 오차값과 비슷하더라

모든 파라미터에 대해서 편미분값을 가지고 경사하강법을 적용해야함

오차값이 체인룰로 뒤로 넘어올때 가중치만큼 넘어옴

그런데 매번 미분을 할 때 마다 라그랑주 함수를 미분해서 넘어오다보니 0.25씩 계속 줄어서 나중에는 학습에 영향을 미치지 못함


# 분류 문제의 종류

- 이진 분류 : 출력이 2가지. True, False, 출력이 1개일 수도 있음 A, 1-A
- 다부류 분류 : 3개 이상의 부류 중에서 하나 선택
- 다중레이블 분류 : 하나의 대상에 대해서 여러 개의 부류 지정 가능

# 분류 문제의 출력값 표현

정수 인코딩 : 문자열 등에 대해서 정수 번호를 부여해서 표현

원 핫 인코딩 : 1,0,0  0,1,0  0,0,1 

# 오차 함수(손실 함수)

> 기대하는 출력과 모델의 출력 차이를 축적하여 표현하는 함수



## 회귀 문제의 오차 함수

최소제곱법 등

## 이진 분류 문제의 오차 함수

시그모이드 함수 사용
1일 확률이 p(x)일때, 0일 확률은 1-p(x)

가능도

@
p(D;w) = \Pi \limits_ {i=1}^{N} y(x_ i, w)^{t_ i}\{1-y(x_ i, w)\}^{1 - t_ i}
@

그런데 1보다 작은 값을 계속 곱해주면 너무 작은 값이 나오게 됨 (언더플로우)

그래서 로그를 씌워줌. 그런데 그레디언트 디센트 방법을 쓰기위해 최소값을 주도록 하기 위해 - 를 붙여줌


@
-log \Pi \limits_ {i=1}^{N} y(x_ i, w)^{t_ i} \{1-y(x_ i, w)\}^{1 - t_ i} \\\ 
-\sum \limits_ {i=1}^{N} ( t_ i logy(x_ i, w) + (1-t_ i)log(1-y(x_ i, w)) )
@


## 다부류 분류 문제의 오차 함수

출력이 확률인것 처럼 나타내고 싶음

그래서 출력이 0보다 크고, 출력의 합이 1이 되도록 만듬

### 소프트맥스 층

학습 파라미터는 없기에 층이 아니기도 함

1. 출력이 0이상이여야 함 -> 지수함수
2. 합이 1이 되어야 함 -> 전체를 다 더해서 비율로 나타냄

~~~ python
def softmax(x):
  return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

출력값을 구했으니까 손실값을 구하려고 함

참값에 해당하는 확률치

교차엔트로피, 크로스 엔트로피


## 다중레이블 분류 문제의 오차 함수

다중레이블 분류에서는 오차함수로 최소제곱법을 쓸 수 있다

교차엔트로피도 쓸 수 있다
대신 t에서 1의 값이 2


# RBF 

## RBF 함수
베타의 값이 클수록 좁아짐

면적이 1이되게는 안했음

가우시안 분포와 유사하지만 가우시안분포는 아님

## RBF 망(RBF network)

뮤 베타 가중치가 학습대상이 됨

오차제곱합을 쓰면됨
그 오차함수에 대해서 경사하강법을 적용하게되면
주어진입력에 대해 파라미터들을 결정할수있음

파라미터가 많음ㄴ 식이 복잡해짐

# 딥러닝




