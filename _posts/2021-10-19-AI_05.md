---
layout: post
title: "AI, 5장 신경망"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/10/19/
---

* Kramdown table of contents
{:toc .toc}  

![image](https://user-images.githubusercontent.com/32366711/137842267-9ff7d417-afc9-4e1e-ac4b-44205b4f1b92.png)


![image](https://user-images.githubusercontent.com/32366711/137843601-0c1e04ed-4172-41f0-90d1-8a2280efabd5.png)
# 신경망

뉴럴네트워크==딥러닝
신경망, neural network, artificial neural network

함수기반 문제, 지식을 함수로, 모델을 함수로

결정트리 -> 함수 사용하지않는 전형적문제
신경망은 함수의 전형적문제

> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야

인간의 두뇌는 신경세포로되어있더라. 신경세포가하는일을수학적으로모델링하고 잘사용하면 지능을구현할수있지않을까


퍼셉트론 - 

함수의 형태를 사람이 만들어주지 않고, 데이터만 주면 스스로 만들어서 문제를 해결한다

그런데 가장 간단한 문제인 XOR문제도 해결하지 못하더라

그런데 하드웨어의 발달등으로 다층퍼셉트론 모델을 찾고, 할 수있게 되었다.
인공신경망에 대한 연구가 다시 활성화되었다

다층퍼셉트론을 수백개씩 쌓게 되면서 모델의 성능이 비약적으로 또 높아졌다

수상돌기로부터 신호를 받고, 신호가 어느정도 결합되면 축색돌기를 동해 전달된다
시냅스를 통해서 다른 수상돌기로 연결이되어 신호(화학물질)를 전달한다

전이값, 포텐셜이 얼마이상이 되면 스파크가 튄다

bias값으로 1을 주면 두쌍의 곱의 합으로 나타낼 ㅅ ㅜ있음

입력 값과 가중치의 내적으로 표현 가능

내적연산의 특징은 두 벡터값이 얼마나 유사한지, 일치하는지를 알 수 있음

퍼셉트론에서 단순한 입력과 가중치의 내적뿐이 아님


# 퍼셉트론 모델

공간을 선형방정식으로 구분하는 것

@
s = \sum \limits _ {i=1}^{d} w_ i x_ i + b = \sum \limits _ {i=0}^{d} w_ i x_ i
@

~~~ python

def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0 
    return activation
~~~

or 연산 and 연산 모두 잘 하지만, 

xor연산은 해결하지 못함

선형 분리 가능 문제(선형방정식, 1차식)에 대해서만 가능



# 다층 퍼셉트론 모델

직선 하날는 안되니까 직선 두개를 그어보자

퍼셉트론을 2개이상 사용해보자

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^T x)
@

입력층       은닉층 (히든레이어)  출력층 
뉴런역할 X    

퍼셉트론은 학습하는걸 알아냈지만,
다층퍼셉트론은 학습하는게 문제다

학습 어떻게하냐

회귀모델에서 오차함수를 정의하고 경사하강법을 통해서 움직이다보면 오차를 최소화할수있다고 했는데,

오차함수를 파라미터에 대해서 편미분할 수 있어야 함

퍼셉트론에서는 이게 안됨

함수를 쓸 때, 활성화함수를 계단모양으로 사용해서 미분할수없었음

미분이 안되는 계단함수를 미분이 되는 시그모이드로 바꿔보자
이를 통해 다층퍼셉트론의 학습이 가능해짐

## 활성화 함수

### 계단 함수

~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0 
~~~

### 시그모이드 함수[^sig]

[^sig]: 계단형에서 살짝 느슨한 시그모이드 함수를 만듦으로써 미분이 가능해졌다. 그러나 미분값이 작아서 학습을 못하는 문제가 있었다. 이를 딥러닝에서 해결했다

@
\sigma(x,a) = \frac{1}{1 + e^{-ax}} \\\ 
\sigma '(x,a) = a \sigma(x,a)(1-\sigma(x,a))
@

미분값이 0.25 1보다 작다               

다층퍼셉트론은 층을 많이 쌓을수록 성능이 좋아질 수 있는데              
시그모이드는 층을 많이 쌓을수록 0에 가까워진다             

학습을 못해서 성능향상이 안된다                    

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x,a=1):
    return a*sigmoid(x,a)*(1-sigmoid(x,a))
~~~

![image](https://user-images.githubusercontent.com/32366711/139608650-9746a52c-90de-496b-a68c-519b4ea137af.png)


### 쌍곡 탄젠트 함수

@
\tanh(x) = \frac{e^{2x}-1}{e^{2x}+1} \\\ 
\tanh'(x) = 1 - \tanh^2(x)
@

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

![image](https://user-images.githubusercontent.com/32366711/139608677-78c46692-745c-4f9d-9747-f73d5c32f9ed.png)

## 다층 퍼셉트론 MLP의 동작

![image](https://user-images.githubusercontent.com/32366711/139608900-359ea52b-9da3-4f88-b60e-60ae5a7654ec.png)

층이 2개           
입력층은 안세고 말하는 경우가 많음 

weight pd, kj

j번째 은닉층의 값 : 입력값 * 가중치의 합

$
zsum_ j = \sum \limits_ {i=1}^{d} u_ {ji}x_ i + u_ {j0} \;\;\; (1 \leq j \leq p) \\\ 
z_ j = f(zsum_ j)
$

$
osum_ k = \sum \limits_ {j=1}^{p} v_ {jk}v_ k + v_ {0k} \;\;\; (1 \leq k \leq p) \\\ 
o_ k = f(osum_ k)
$

전향망 Feed Forward

### MLP의 학습

입력 : ($x_ 1, x_ 2, \cdots, x_ d$)
기대 출력 : $y_ k$
MLP 출력 : $o_ k$

#### 학습 목표

기대 출력과 MLP 출력이 최대한 비슷해지도록 가중치를 변경하는 것            
[^error]
@
E = \frac{1}{2}(o_ k - y_ k)^2
@

[^error]: 오차함수, 손실함수, 목적함수, 에러펑션

경사 하강법 사용 

@
v_ {jk}^{(t+1)} = v_ {jk}^{(t)} - \eta \frac{\partial E}{\partial v_ {jk}} \\\ 
u_ {ij}^{(t+1)} = u_ {ij}^{(t)} - \eta \frac{\partial E}{\partial u_ {ij}} 
@


# 미분

> 함수 f(x)의 변수 x에 대한 순간변화율

@
f'(x) = \frac{df(x)}{dx} = \lim_ {\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
@


## 연쇄 법칙, Chain Rule

@
y = f(g(x)) \\\ 
y = f(u), u = g(x)
@
@
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
@

#### 체인 룰 유도 과정

![image](https://user-images.githubusercontent.com/32366711/139609869-61c18753-1397-496a-977d-3b691ff6759b.png)

## 편미분

partial differentiation

> 다변수 함수에 대하여, 하나의 변수에만 집중하고,            
> 나머지 변수는 상수로 생각하고 미분하는 방법                   

## 다변수 함수의 연쇄 법칙

$
f(x(t),y(t))
$
@
\frac{d \; f(x(t),y(t))}{d \; t} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}
@

![image](https://user-images.githubusercontent.com/32366711/139610152-72c25553-b5f9-4063-b7b3-720243a8acda.png)

$
g(x(t),y(t),z(t))
$
@
\frac{d \; g(x(t),y(t),z(t))}{d \; t} = \frac{\partial g}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial g}{\partial y}\frac{\partial y}{\partial t}+ \frac{\partial g}{\partial z}\frac{\partial z}{\partial t}
@

## 그레디언트

> 함수 f(x,y,z)의 각 변수에 대한 편미분을 성분으로 갖는 <red>벡터</red>

@
\bigtriangledown f(x,y,z) = \begin{bmatrix}
\frac{\partial f}{\partial x} \\\ 
\frac{\partial f}{\partial y} \\\ 
\frac{\partial f}{\partial z}
\end{bmatrix}
@

# 다층 퍼셉트론의 학습

# 분류 문제의 종류

# 분류 문제의 출력값 표현

# 오차 함수(손실 함수)

## 회귀 문제의 오차 함수

## 이진 분류 문제의 오차 함수

## 다부류 분류 문제의 오차 함수

### 소프트맥스 층

## 다중레이블 분류 문제의 오차 함수

# RBF 망

## RBF 함수



