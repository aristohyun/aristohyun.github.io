---
layout: post
title: "AI, 5장 신경망"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/11/15/
---

* Kramdown table of contents
{:toc .toc}  

![image](https://user-images.githubusercontent.com/32366711/137842267-9ff7d417-afc9-4e1e-ac4b-44205b4f1b92.png)

![image](https://user-images.githubusercontent.com/32366711/137843601-0c1e04ed-4172-41f0-90d1-8a2280efabd5.png)

# 신경망

> neural network, artificial neural network     
> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야


## 퍼셉트론, Perceptron

> 로젠블랏이 제안한 학습 가능한 신경망 모델

함수의 형태를 사람이 만들어주지 않고, 데이터만 주면 스스로 만들어서 문제를 해결한다

~~~ python
def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0
    return activation
~~~

#### OR 연산

![image](https://user-images.githubusercontent.com/32366711/141779673-99598eeb-8349-4a4b-8124-1126b99cfc6c.png)

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^Tx) = x_ 1 + x_ 2 - 0.5
@

#### AND 연산

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^Tx) = x_ 1 + x_ 2 - 1.5
@


그러나 선형 분리가능 문제(선형방정식, 1차식)에만 사용 가능했음
선형 분리불가 문제는 사용할 수 없음 

특히 가장 간단한 XOR문제도 해결할 수 없었다

그런데 하드웨어의 발달등으로 다층퍼셉트론 모델을 찾고, 할 수 있게 되며
인공신경망에 대한 연구가 다시 활성화되었다

다층퍼셉트론을 수백개씩 쌓게 되면서 모델의 성능이 비약적으로 또 높아졌다

수상돌기로부터 신호를 받고, 신호가 어느정도 결합되면 축색돌기를 동해 전달된다
시냅스를 통해서 다른 수상돌기로 연결이되어 신호(화학물질)를 전달한다

전이값, 포텐셜이 얼마이상이 되면 스파크가 튄다

bias값으로 1을 주면 두쌍의 곱의 합으로 나타낼 수있음

입력 값과 가중치의 내적으로 표현 가능

내적연산의 특징은 두 벡터값이 얼마나 유사한지, 일치하는지를 알 수 있음

퍼셉트론에서 단순한 입력과 가중치의 내적뿐이 아님


## 다층 퍼셉트론 모델, MLP

> 여러 개의 퍼셉트론을 층 구조로 구성한 신경망 모델

직선 하나로 안되니까 직선 두개를 그어보자    
퍼셉트론을 2개이상 사용해보자    

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^T x)
@

### 다층 퍼셉트론의 학습

입력-출력 $(x_ i, y_ i)$의 학습 데이터에 대해서, 기대출력 $(y_ i)$와 다층퍼셉트론의 출력 $f(x_ i)$의 차이,
즉 <red>오차(error)</red>가 <red>최소</red>가 되도록 <blue>가중치 𝒘</blue>를 결정하는 것

`학습을 위해 오차 역전파 알고리즘 사용`

> 활성화 함수를 계단 함수에서 미분 가능한 <red>시그모이드 함수</red>로 대체                    
> 경사 하강법 적용           

회귀모델에서 오차함수를 정의하고 경사하강법을 통해서 움직이다보면 오차를 최소화 할 수있다고 했는데,

오차함수를 파라미터에 대해서 편미분할 수 있어야 함

퍼셉트론에서는 계단함수를 사용했기에 미분할 수 없음

`미분이 안되는 계단함수를 미분이 되는 시그모이드로 바꿔보자`

이를 통해 다층퍼셉트론의 학습이 가능해짐

### 활성화 함수

#### 계단 함수

~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0 
~~~

#### 시그모이드 함수[^sig]

[^sig]: 계단형에서 살짝 느슨한 시그모이드 함수를 만듦으로써 미분이 가능해졌다. 그러나 미분값이 작아서 학습을 못하는 문제가 있었다. 이를 딥러닝에서 해결했다

@
\sigma(x,a) = \frac{1}{1 + e^{-ax}} \\\ 
\sigma '(x,a) = a \sigma(x,a)(1-\sigma(x,a))
@

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x,a=1):
    return a*sigmoid(x,a)*(1-sigmoid(x,a))
~~~

미분값이 0.25 1보다 작다               

다층퍼셉트론은 층을 많이 쌓을수록 성능이 좋아질 수 있는데              
시그모이드는 층을 많이 쌓을수록 0에 가까워진다             

학습을 못해서 성능향상이 안된다                    

![image](https://user-images.githubusercontent.com/32366711/139608650-9746a52c-90de-496b-a68c-519b4ea137af.png)


#### 쌍곡 탄젠트 함수

> 구간 (-1,1)의 출력

@
\tanh(x) = \frac{e^{2x}-1}{e^{2x}+1} \\\ 
\tanh'(x) = 1 - \tanh^2(x)
@

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

![image](https://user-images.githubusercontent.com/32366711/139608677-78c46692-745c-4f9d-9747-f73d5c32f9ed.png)

## 미분

> 함수 f(x)의 변수 x에 대한 순간변화율

@
f'(x) = \frac{df(x)}{dx} = \lim_ {\Delta x \rightarrow 0} \frac{f(x + \Delta x) - f(x)}{\Delta x}
@


### 연쇄 법칙, Chain Rule

@
y = f(g(x)) \\\ 
y = f(u), u = g(x)
@

@
\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
@

#### 체인 룰 유도 과정

![image](https://user-images.githubusercontent.com/32366711/139609869-61c18753-1397-496a-977d-3b691ff6759b.png)

### 편미분, partial differentiation

> 다변수 함수에 대하여, <red>하나의 변수</red>에만 집중하고,            
> 나머지 변수는 <red>상수로 생각</red>하고 미분하는 방법                   

### 다변수 함수의 연쇄 법칙

$
f(x(t),y(t))
$

@
\frac{d \; f(x(t),y(t))}{d \; t} = \frac{\partial f}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial f}{\partial y}\frac{\partial y}{\partial t}
@

$
g(x(t),y(t),z(t))
$
@
\frac{d \; g(x(t),y(t),z(t))}{d \; t} = \frac{\partial g}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial g}{\partial y}\frac{\partial y}{\partial t}+ \frac{\partial g}{\partial z}\frac{\partial z}{\partial t}
@

![image](https://user-images.githubusercontent.com/32366711/139610152-72c25553-b5f9-4063-b7b3-720243a8acda.png)

### 그레디언트

> 함수 f(x,y,z)의 각 변수에 대한 편미분을 성분으로 갖는 <red>벡터</red>                      
> 함수 f(x,y,z)의 값이 가장 커지는 방향과 크기를 나타내는 벡터   

@
\bigtriangledown f(x,y,z) = \begin{bmatrix}
\frac{\partial f}{\partial x} \\\ 
\frac{\partial f}{\partial y} \\\ 
\frac{\partial f}{\partial z}
\end{bmatrix}
@

## 다층 퍼셉트론의 학습

`오차 역전파 알고리즘, Error back propagation algorithm 사용`

![image](https://user-images.githubusercontent.com/32366711/139611170-65d1fa86-83ad-4129-92f4-6d6bb7c49e23.png)

중간 가중치에서의 미분값을 구하려고 보니까
입력값 * 앞의 값의 오차값과 비슷하더라

모든 파라미터에 대해서 편미분값을 가지고 경사하강법을 적용해야함

오차값이 체인룰로 뒤로 넘어올때 가중치만큼 넘어옴

그런데 매번 미분을 할 때 마다 라그랑주 함수를 미분해서 넘어오다보니 0.25씩 계속 줄어서 나중에는 학습에 영향을 미치지 못함


## 분류 문제의 종류

- 이진 분류 : 출력이 2가지. True, False, 출력이 1개일 수도 있음 A, 1-A
- 다부류 분류 : 3개 이상의 부류 중에서 하나 선택
- 다중레이블 분류 : 하나의 대상에 대해서 여러 개의 부류 지정 가능

## 분류 문제의 출력값 표현

### 출력값의 표현

- 문자열 / 기호
- 정수 인코딩 : 문자열 등에 대해서 정수 번호를 부여해서 표현
- 원 핫 인코딩 : 1,0,0  0,1,0  0,0,1 

### 학습 데이터

@
D = {(x_ 1, t_ 1), (x_ 2, t_ 2), \dcots, (x_ N, T_ N)}
@

- i번째 데이터의 입력 : $x_ i$
- i번째 데이터의 입력 :  $t_ i = (t_ {t1}, t_ {t2}, \cdots, t_ {tK}) t_ {ij} \in {0,1}, \sum \limits_ {j=1}^{K} t_ {ij} = 1$ 

## 오차 함수(손실 함수)

> 기대하는 출력과 모델의 출력 차이를 축적하여 표현하는 함수


### 회귀 문제의 오차 함수

`Mean Squared Error`

@
E = \frac{1}{2} \sum \limits_ {i=1}^{N} (y(x_ i, w)- t_ i)^2
@

### 이진 분류 문제의 오차 함수

> 시그모이드 함수 사용
> (0,1) 구간 사이의 값을 출력 == 확률로 해석 가능

1일 확률이 p(x)일때, 0일 확률은 1-p(x)

- 가중치 w에 대한 학습 데이터 D의 <red>가능도</red>(likelihood)
@
p(D;w) = \Pi_ {i=1}^{N} y(x_ i, w)^{t_ i}\{1-y(x_ i, w)\}^{1 - t_ i}
@

그런데 1보다 작은 값을 계속 곱해주면 너무 작은 값이 나오게 됨 (언더플로우)

그래서 로그를 씌워줌. 그러면 덧셈으로 바뀌니까

마이너스는 그레디언트 반대방향으로 가기 위함

@
-log \; \Pi_ {i=1}^{N} y(x_ i, w)^{t_ i} \{1-y(x_ i, w)\}^{1 - t_ i} \\\ 
= -\sum \limits_ {i=1}^{N} ( t_ i \; log \; y(x_ i, w) + (1-t_ i)log(1-y \; (x_ i, w)) )
@

### 다부류 분류 문제의 오차 함수

출력이 확률인것 처럼 나타내고 싶음

그래서 출력이 0보다 크고, 출력의 합이 1이 되도록 만듬

#### 소프트맥스 층

학습 파라미터는 없기에 층이 아니기도 함

1. 출력이 0이상이여야 함 -> 지수함수
2. 합이 1이 되어야 함 -> 전체를 다 더해서 비율로 나타냄

@
y_ k = \frac{e^{Z_ K}}{\sum \limits_ {i=1}^{K} e^{Z_ i}}
@

~~~ python
def softmax(x):
  return np.exp(x) / np.sum(np.exp(x), axis=0)
~~~

- 조건부 확률       
@
p(t_ i | x_ i, w) = \Pi_ {k=1}^{K} y_ k (x_ i, w)^{t_ {ik}}
@

- 전체 데이터 D에 대한 가능도      
@
p(D;w) = \Pi_ {i=1}^{N} \Pi_ {k=1}^{K} y_ k (x_ i, w)^{t_ {ik}}
@

- 데이터의 가능도를 최대로 하는 파라미터 w를 추정하는 것
@
argmax_ w \Pi_ {i=1}^{N} \Pi_ {k=1}^{K} y_ k (x_ i, w)^{t_ {ik}}
@

- 오차함수 E(w) : 가능도의 음의 로그 가능도 (negative log likelihood)
@
E(w) = - log \Pi_ {i=1}^{N} \Pi_ {k=1}^{K} y_ k (x_ i, w)^{t_ {ik}} = - \sum \limits_ {i=1}^{N} \sum \limits_ {k=1}^{K} t_ {ik} log y_ k (x_ i, w)
@

- 오차함수 에 대한 <blue>교차 엔트로피</blue>[^cross]
@
E(w) = - \sum \limits_ {k=1}^{K} t_ {ik} log y_ k (x_ i , w)
@

[^cross]: 출력값을 구했으니까 손실값을 구하려고 함, 참값에 해당하는 확률치

### 다중레이블 분류 문제의 오차 함수

> 최대값을 출력하는 복수 개의 노드

#### MSE, Mean Squared Error

@
E = \frac{1}{2} \sum \limits_ {k=1}^{n} (o_ k - y_ k)^2
@

#### 교차 엔트로피, cross entropy

@
E(w) = - \sum \limits_ {k=1}^{K} t_ {ik} log y_ k (x_ i , w)
@


## RBF

#### RBF 함수

> 기존 벡터 $\mu$와 입력 벡터 $x$의 유사도를 측정하는 함수 

@
\phi (x, \mu) = exp(-\beta || x - \mu ||^2)
@

베타의 값이 클수록 좁아짐

가우시안 분포와 유사하지만 가우시안분포는 아님

![image](https://user-images.githubusercontent.com/32366711/141795436-cb4591ab-5f20-4017-a6ba-4ff996f0b54c.png)

#### RBF 망

> 어떤 함수 $f_ k(x)$를 다음과 같이 RBF 함수[^RBF]들의 선형 결합 형태로 근사시키는 모델

@
f_ k (x) \approx \sum \limits_ {i=1}^{N} w_ {kj} \phi_ i (x, \,mu_ i) + b_ k
@

뮤 베타 가중치가 학습대상이 됨

오차제곱합을 쓰면됨
그 오차함수에 대해서 경사하강법을 적용하게되면
주어진입력에 대해 파라미터들을 결정할수있음

파라미터가 많으면 식이 복잡해짐

- 오차 함수 E
@
E = \frac{1}{2} \sum \limits_ {k=1}^{m} (o_ k - y_ k)^ 2    
@

- 경사 하강법 사용
    - 기준 벡터 $\mu_ j$와 파라미터 $\beta_ j$, 가중치 $w_ {kj}$ 결정

- 부류 별 군집화 결과를 사용한 기준 벡터 $\mu_ j$와 파라미터 $\beta_ j$ 초기화
    - 군집 중심 : 기준(평균) 벡터 $\mu_ j$
    - 분산의 역수 : $\beta_ j$
@
\sigma = \frac{1}{m} \sum \limits_ {i=1}^{m} || x_ i - \mu || \\\ 
\beta = \frac{1}{2\sigma^ 2}
@


# 딥러닝

> 딥러닝 신경망 모델 기반의 기계 학습 기법

deep : 층이 많음 (shallow : 층이 얕음, 일반 신경망)

층이 많을수록 데이터도 많이 필요, 자원도 많이필요

#### 비교

- 일반 신경망
    - 데이터에서 직접 특징을 추출해서 입력으로 사용

- 딥러닝 신경망
    - 층이 많으니까 그냥 좋은 성능이 나올때 까지 학습시킴. 
    - 필요 특징을 알아서 뽑아내서 사용

## 기울기 소멸 문제

> 은닉층이 많은 다층 퍼셉트론 MLP에서     
> 오차가 크게 줄어들어 학습이 되지 않는 현상

미분값이 0.25이하, 이를 계속 곱하다보니 그레디언트가 너무 작아짐

### 완화 방법 

#### ReLU

~~~ pyhton
def ReLU(x):
    return np.maximum(0,x)
~~~

![image](https://user-images.githubusercontent.com/32366711/141798906-eaa15700-994c-4c19-9e60-ed9c5975035c.png)

> 함수를 부분적인 평면 타일들로 근사하는 형태        
> 출력이 0이상인 것들에 의해 계산되는 결과

#### ReLU 변형

- Leaky ReLU[^Leaky]
    - $f(x) = max(\alpha x, x)$ 
- ELU[^ELU]
    - @ f(x) = \begin{cases}
x & \text{ if } x>0 \\\ 
\alpha (exp(x)-1)) & \text{ if otherwise }  
\end{cases} @
- Maxout[^Maxout]
    - $f(x) = max_ {i \in \{1,\cdots,k\}} \{ w_ i ^ T x + b_ i \}$ 
- PReLU[^PReLU]
    - $f(x) = max(\alpha x, x)$ 
    - 파라미터 $\alpha$가 학습됨
- Swish[^Swish]

[^ReLU]: ![image](https://user-images.githubusercontent.com/32366711/141799279-ac787a5f-f271-4f0e-acff-0414ff19fbb6.png){width="300"}

[^Leaky]: ![image](https://user-images.githubusercontent.com/32366711/141799308-97b4e7a8-4cc1-49fc-912e-d6f5ba50a754.png){width="300"}

[^ELU]: ![image](https://user-images.githubusercontent.com/32366711/141799431-2ce1f04e-4a73-41b8-9830-f3fcb4aac766.png){width="300"}

[^Maxout]: ![image](https://user-images.githubusercontent.com/32366711/141799448-9c40924f-07e4-4e3e-95f9-bbc36c433f80.png){width="300"}

[^PReLU]: ![image](https://user-images.githubusercontent.com/32366711/141799511-7dd559f4-acf5-4277-930a-b493e755398e.png){width="300"}

[^Swish]: ![image](https://user-images.githubusercontent.com/32366711/141799539-82c4c25b-bcca-4e71-b8bd-f1651a37cfa3.png){width="300"}

## 가중치 초기화

> 시작하는 위치, 초기화를 잘 시켜야함
> 보통은 0에 가까운 무작위 값 사용

### 개선된 가중치 초기화 방법

#### 균등 분포 초기화

`다음에서 무작위로 선택`

@
U \[ - \sqrt{\frac{6}{n_ i + n_ {i+1}}}, \sqrt{\frac{6}{n_ i + n_ {i+1}}} \]
@

~~~python
sd = np.sqrt(6/(layer_size[i+1]+ layer_size[i])) 
W = np.random.uniform(-sd, sd, layer_size[i]* layer_size[i-1]).reshape(layer_size[i+1], layer_size[i])
~~~

#### 제이비어 초기화

`N(0,1)에서 무작위로 선택`

@
\frac{N(0,1)}{\sqrt {n_ i}}
@

~~~python
W = np.random.randn(layer_size[i+1], layer_size[i]) * np.sqrt(1/layer_size[i])
~~~

#### 허he 초기화

@
\frac{N(0,1)}{\sqrt {n_ i / 2}}
@

~~~python
W = np.random.randn(layer_size[i+1], layer_size[i]) * np.sqrt(2/layer_size[i])
~~~

#### 제한적 볼츠만 머신, RBM

비지도 학습법 값을 한층 올려서 계산했다가 내려오는 방식

#### 인접 층간의 가중치를 직교하는 벡터로 초기화

하나의 뉴런은 입력과 가중치의 내적 = 정사영

각각의 벡터들이 직교가 되게 하자. 서로 다른 특징을 가진 값으로 나누자

`특이값 분해`

열벡터, 행벡터가 직교하게 되어있음

대각행렬을 제외한 값은 0

## 과적합 문제, Overfitting

> 과적합이란, 모델이 학습 데이터에 지나치게 맞추어진 상태

데이터는 내부적으로 불확실성(잡음, 오류)을 가질 수 있기 때문

표본집합은 모집합과 같은 분포를 가진다 라고 가정

![image](https://user-images.githubusercontent.com/32366711/140671030-34469c5a-ee79-457d-b578-08f4f316b9bc.png)

### 학습 데이터 단위에 따른 가중치 갱신 전략

- 확률적 갱신
    - 한번에 하나의 학습 데이터에 대한 그레디언트를 계산하여 가중치 갱신
    - 단점 : 가중치의 변동이 심해 성능 개선이 저하된다
- 배치 갱신
    - 전체 학습 데이터에 대한 그레디언트 평균을 구하여 가중치를 갱신
    - 단점 : 속도가 느림 
- 미니배치 갱신
    - 일정 개수의 학습 데이터, 미니 배치에 대해 그레디언트 평균을 계산하여 가중치 갱신

### 규제화(Regularization) 기법

> 오차 함수를 오차 항과 모델 복잡도 항으로 정의

모델이 복잡해 지면 과적합이 될 수 있으므로, 모델 복잡도를 패널티 항으로 추가

오차함수 = 오차항 + a 모델 복잡도 항

a = 하이퍼 파라미터, 개발자가 직접 지정

모델 복잡도 항 
- ridge : $\sum \limits_ {i=1}^{n} w_ i ^2$
- lasso : $\sum \limits_ {i=1}^{n} | w_ i |$

### 드롭아웃(Dropout)

> 학습할 때 노드들을 무작위로 선택하여       
> 선택된 노드의 뒤로 연결된 가중치 연결선은 없는 것으로 간주

대신 액티베이션 비율이 낮아지니까, 최종값에서 드랍아웃 비율만큼 나눠줌

추론(실사용)에서는 그대로

### 배치 졍규화(Batch Normalization)

- 내부 공변량 이동 문제
    - 이전 층들의 학습에 의해 이들 층의 가중치가 바뀌게 되면, 현재 층에 전달되는 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생한다
    - 학습 속도가 저하되는 문제 발생


> 신경망의 각 층에서 미니배치 B의 각 데이터에 가중치 연산을 적용한 결과인 $x_ i$의 분포를 정규화(normalization)하는 것

1. $x_ i$의 평균 $\mu_ B$가 0이 되고, 표준편차 $\sigma B는 I가 되도록 변환$
2. 크기조정 파라미터 $\gamma$와 이동 파라미터 $\beta$ 적용
3. 변환된 데이터 $y_ i$ 생성

미니배치 평균 : $\mu_ B = \frac{1}{m} \sum \limits_ {i=1}^{m} x_ i$
미니배치 분산 : $\sigma_ B ^2 = \frac{1}{m} \sum \limits_ {i=1}^{m} (x_ i - \mu_ B )^2$

정규화[^norm] : $\hat {x_ {i}} = \frac{x_ i - \mu_ B}{\sqrt{\sigma_ B ^2 + \epsilon }}$

크기조정 및 이동변환 : $y_ i = \gamma \hat x_ i + \beta$

[^norm]: 사실상 표준화, Standardization

## 가중치 학습 기법

#### 경사하강법[^grad]

@
w^{(t+1)} = w^{(t)} - \eta \frac{\partial E(w^{(t)}) }{\partial w}
@

[^grad]: 경사가 너무 크면, 반대쪽까지 갈 수 있기 때문에 러닝레이트 곱해줌. 미니배치로하면, 값 데이터별 그레이던트 계산해서 평균 사용

#### 모멘텀 사용 경사 하강법

@
\begin{align*}
\Delta^{(t)} &= \alpha \Delta^{(t-1)} + \eta \frac{\partial E(w^{(t)}) }{\partial w} \\ 
& w^{(t+1)} = w^{(t)} - \Delta^{(t)}
\end{align*}
@

![image](https://user-images.githubusercontent.com/32366711/141807604-e8f928b1-e65d-4dab-bcbf-87b5665b8058.png){width="300"}


#### NAG, Nesterov Accelerated Gradient

모멘텀만큼 이동한 곳에서 그레디언트 계산

@
\begin{align*}
\Delta^{(t)} &= \alpha \Delta^{(t-1)} + \eta \frac{\partial E(w^{(t)} - \alpha \Delta^{(t-1)}) }{\partial w} \\ 
& w^{(t+1)} = w^{(t)} - \Delta^{(t)}
\end{align*}
@

![image](https://user-images.githubusercontent.com/32366711/141807555-becdc9a8-67bf-48d5-ac4a-12af5fc08bc3.png){width="300"}


#### Adagrad

가중치 별로 다른 학습율 사용

![image](https://user-images.githubusercontent.com/32366711/141807797-ede5b50d-576d-4d70-a6f8-334b5009f019.png)

#### Adadelta

- Adagrad의 확장
- 과거 그레디언트의 영향을 점점 줄이면서 그레디언트 제곱합 계산

![image](https://user-images.githubusercontent.com/32366711/141807934-b302c01d-e163-4460-901f-c8e6eae5b0fa.png)

#### RMSprop

- 가중치별 다른 학습율 사용
- 결합된 그레디언트 제곱의 합의 제곱근을 학습율로 사용

![image](https://user-images.githubusercontent.com/32366711/141808033-8d200760-56f2-43c6-9e1b-53924afcb6c7.png)

#### ADAM

- 가중치별 다른 학습률 사용
- 그레디언트의 1차, 2차 모멘텀 사용

![image](https://user-images.githubusercontent.com/32366711/141808122-cac54508-4abb-49d9-a907-d4ff6ebe28de.png)



# CNN Model

`Convolutional Neural Network Models`

> 시각 피질 구조에서 영감을 받아 만들어진 딥러닝 신경망 모델

컨볼루션 연산을 수행하여 특징 추출 -> 특징을 이용하여 분류

![image](https://user-images.githubusercontent.com/32366711/141984686-ba4733a0-22ab-41ee-8f2e-c557072bce0c.png)

## 컨볼루션, Convolution

> 일정 영역의 값들에 대해 가중치를 적용(필터)하여 하나의 값을 만드는 연산

- 스프라이드 : 보폭
    - 커널을 다음 컨볼루션 연산을 위해 이동시키는 칸 수   
- 패딩 : 확장된 배열, 빈칸
    - 컨볼루션 결과의 크기를 조정하기 위해 입력 배열의 둘레를 확장하고 0으로 채우는 연산

컬러이미지는 하나의 픽셀을 3개의 색상값으로 표현함                            
즉 입력이 3종류, 그러면 특징을 추출할 필터도 3종류가 필요할 것            

컨볼루전 필터가 몇개냐에 따라 출력채널이 만들어진다                                 
5개의 필터가 주어진다 5개의 채널이 만들어진다

가중치방향으로 프로젝션시켰을때 값이 얼만지, 가중치가 가르키는 특징이 얼마나 가지고있는지를 계산하는게 내적이다.

컨볼루젼연산을하게되면, 필터에 의해서 표현되는 벡터, 특징을 얼마나 많이 가지고있는지 계산하는것.

주어진 데이터로 잘 학습할 수 있도록 가중치, 필터가 자동으로 결정되서 필요한 특징을 뽑게된다 - 특이값 분채

피쳐 엔지니어링, 입출력에 대한 정보만 주게되면 입출력에 대한 관계를 신경망이 효과적으로 추출하게 된다

### 특징 지도, feature map

`채널`

> 컨볼루션 필터의 적용 결과로 만들어지는 2차원 행렬
 
## 풀링

> 특징지도, 피쳐 맵을 줄이는 역할               
> 일정 크기의 블록을 통합하여 하나의 대푯값으로 대체하는 연산          

메모리 크기와 계산량 감소를 위함

#### 최대값 풀링, max pooling

미분X -> 최대값에 대해서만 미분하기도 함

#### 평균값 풀링, average pooling

미분 가능

#### 확률적 풀링, stochastic pooling

학습시: 확률적 풀링

$
p_ i = \frac{a_ i}{\sum \limits_ {k \in R_ j}^{} a_ k }
$

추론시 : 확률적 가중합 사용

$
\sum \limits_ {k \in R_ j}^{} p_ i a_ k 
$

## CNN의 구조

- 특징 추출을 위한 컨볼루션 부분
    - Conv : 컨볼루션 연산
    - ReLU : 활성화 함수
    - Pool (선택)
- 추출된 특징을 사용하여 분류/회귀를 수행하는 다층 퍼셉트론 부분
    - 전방향으로 완전 연결층 반복
    - 소프트 맥스[^softmax] 연산 추가

순서는 상관없음, 다만 여러개 해보며 가장 좋은거 선택

[^softmax]: 출력의 값이 0 이상이면서 합은 1

### 가중치 학습

기존 미분 방법은 잘 안쓴다. 너무 오래 걸림

#### 자동 미분, automatic differentiation

Chain Rule에 따라 연쇄 법칙에 의해 미분 계산

# 컨볼루션 연산과 영상 분류 CNN 모델

## [컨볼루션 형태](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)

- 단일 채널 컨볼루션
- 다중 채널 2D 컨볼루션
    - 컴퓨터 그래픽스(행렬 연산, 좌표값 계산) 
- 다중 채널 3D 컨볼루션
    - 의료 영상
- 1x1 컨볼루션
    - 가중치 계산, 채널 갯수 조절
- 디컨볼루션 (transposed convolution)
    - 기본적으로 컨볼루션 연산은 크기가 줄게 되어있음
    - 필요에따라 늘리고 싶을 때도 있음
- 팽창 컨볼루션
- 공간 분할 컨볼루션
    - 계산량이 줄어듬 -> 시간절약
- 깊이별 분할 컨볼루션
- 집단 컨볼루션
- 채널섞기 집단 컨볼루션

## 물체 인식 CNN 모델

### LeNet 모델

> Conv - Pll - Conv - Pool - Conv - FC - FC(SM)[^LeNet5]

입력 : 32 * 32 필기체 숫자 영상
풀링 : 가중치 * (2 * 2 블록의 합) + 편차항
활성화 함수 : 시그모이드

오차율 : 0.95%

[^LeNet5]: Pool은 층이 아니기 때문에 5계층 

### ILSVRC 대회

ImageNet 데이터베이스

이미지 분류 문제

#### AlexNet

> 8계층 컨볼루션 신경망
> Conv-Pool-Norm-Conv-Pool-Norm-Conv-Conv-Conv-Pool-FC-FC-FC(SM)

<img width="369" alt="image" src="https://user-images.githubusercontent.com/32366711/142651592-2750fc59-c52d-4dd1-8a20-66703cf23ba7.png">

- 국소 반응 정규화 연산 층 Norm[^Norm]
- ReLU 함수를 사용한 첫 모델                   
- FC층에 드롭아웃 기법 사용 
- 최대값 풀링 사용


[^Norm]: 인접한 여러 층의 출력값들을 이용하여 출력값 조정
