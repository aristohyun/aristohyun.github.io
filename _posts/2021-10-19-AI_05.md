---
layout: post
title: "AI, 5장 신경망"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/10/19/
---

* Kramdown table of contents
{:toc .toc}  

![image](https://user-images.githubusercontent.com/32366711/137842267-9ff7d417-afc9-4e1e-ac4b-44205b4f1b92.png)


![image](https://user-images.githubusercontent.com/32366711/137843601-0c1e04ed-4172-41f0-90d1-8a2280efabd5.png)
# 신경망

뉴럴네트워크==딥러닝
신경망, neural network, artificial neural network

함수기반 문제, 지식을 함수로, 모델을 함수로

결정트리 -> 함수 사용하지않는 전형적문제
신경망은 함수의 전형적문제

> 인간 두뇌에 대한 계산적 모델을 통해 인공지능을 구현하려는 분야

인간의 두뇌는 신경세포로되어있더라. 신경세포가하는일을수학적으로모델링하고 잘사용하면 지능을구현할수있지않을까


퍼셉트론 - 

함수의 형태를 사람이 만들어주지 않고, 데이터만 주면 스스로 만들어서 문제를 해결한다

그런데 가장 간단한 문제인 XOR문제도 해결하지 못하더라

그런데 하드웨어의 발달등으로 다층퍼셉트론 모델을 찾고, 할 수있게 되었다.
인공신경망에 대한 연구가 다시 활성화되었다

다층퍼셉트론을 수백개씩 쌓게 되면서 모델의 성능이 비약적으로 또 높아졌다

수상돌기로부터 신호를 받고, 신호가 어느정도 결합되면 축색돌기를 동해 전달된다
시냅스를 통해서 다른 수상돌기로 연결이되어 신호(화학물질)를 전달한다

전이값, 포텐셜이 얼마이상이 되면 스파크가 튄다

bias값으로 1을 주면 두쌍의 곱의 합으로 나타낼 ㅅ ㅜ있음

입력 값과 가중치의 내적으로 표현 가능

내적연산의 특징은 두 벡터값이 얼마나 유사한지, 일치하는지를 알 수 있음

퍼셉트론에서 단순한 입력과 가중치의 내적뿐이 아님


# 퍼셉트론 모델

공간을 선형방정식으로 구분하는 것

@
s = \sum \limits _ {i=1}^{d} w_ i x_ i + b = \sum \limits _ {i=0}^{d} w_ i x_ i
@

~~~ python

def Perceptron(inputs):
    sum = np.dot(inputs, weights[1:]) + weights[0]
    if sum > 0:
        activation = 1
    else:
        activation = 0 
    return activation
~~~

or 연산 and 연산 모두 잘 하지만, 

xor연산은 해결하지 못함

선형 분리 가능 문제(선형방정식, 1차식)에 대해서만 가능



# 다층 퍼셉트론 모델

직선 하날는 안되니까 직선 두개를 그어보자

퍼셉트론을 2개이상 사용해보자

@
y = f(s) = f(\sum \limits_ {i=1}^{2} w_ i x_ i + b) = f(w^T x)
@

입력층       은닉층 (히든레이어)  출력층 
뉴런역할 X    

퍼셉트론은 학습하는걸 알아냈지만,
다층퍼셉트론은 학습하는게 문제다

학습 어떻게하냐

회귀모델에서 오차함수를 정의하고 경사하강법을 통해서 움직이다보면 오차를 최소화할수있다고 했는데,

오차함수를 파라미터에 대해서 편미분할 수 있어야 함

퍼셉트론에서는 이게 안됨

함수를 쓸 때, 활성화함수를 계단모양으로 사용해서 미분할수없었음

미분이 안되는 계단함수를 미분이 되는 시그모이드로 바꿔보자
이를 통해 다층퍼셉트론의 학습이 가능해짐

## 활성화 함수

### 계단 함수

~~~ python
def step(x):
    if x > 0:
        return 1
    else:
        return 0 
~~~

### 시그모이드 함수

@
\sigma(x,a) = \frac{1}{1 + e^{-ax}} \\\
\sigma '(x,a) = a \sigma(x,a)(1-\sigma(x,a))
@

미분값이 0.25 1보다 작다
다층퍼셉트론은 층을 많이 쌓을수록 성능이 좋아질 수 있는데

시그모이드는 층을 많이 쌓을수록 0에 가까워진다
성능향상이 안된다

~~~ python
def sigmoid(x, a=1):
    return 1/(1+np.exp(-a*x))

def d_sigmoid(x,a=1):
    return a*sigmoid(x,a)*(1-sigmoid(x,a))
~~~

### 쌍곡 탄젠트 함수

@
\tanh(x) = \frac{e^{2x}-1}{e^{2x}+1} \\\ 
\tanh'(x) = 1 - \tanh^2(x)
@

~~~ python
def tanh(x):
    return (np.exp(2*x)-1)/(np.exp(2*x)+1)

def d_tanh(x):
    return 1.0-tanh(x)*tanh(x)
~~~

# 미분

> 함수 f(x)의 변수 x에 대한 순간변화율

@
f'(x) = \frac{df(x)}{dx} = \lim_ \Delta \rightarrow 0 \frac{f(x + \Delta x) - f(x)}{\Delta x}
@

## 연쇄 법칙, Chain Rule

