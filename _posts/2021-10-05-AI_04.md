---
layout: post
title: "AI, 4장 기계학습"
description: "AI, 이건명 교수님"
categories: [AI]
tags: [2021-2, AI, Artificial Intelligence, 이건명]
use_math: true
redirect_from:
  - /2021/10/05/
---

* Kramdown table of contents
{:toc .toc}  


# 기계학습

> Machine Learning           
> 경험을 통해 작업을 더 효율적으로 처리할 수 있도록          
> 시스템의 구조나 파라미터를 바꾸는 것

컴퓨터가 데이터로부터 특정 문제해결을 위한 지식을 자동으로 추출해서 사용할 수 있게 하는 기술

직접 알고리즘을 짠다면 복잡하거나 성능이 제대로 안나올 수 있지만,             
기계학습을 이용한다면 데이터를 학습시킴에 따라 자동으로 분류 규칙이나 프로그램을 생성함          

## 학습 방법

> 귀납적 학습          
> 사례들을 일반화하여 패턴 또는 모델을 추출하는 것

학습 데이터를 잘 설명할 수 있는 패턴을 찾는 것

#### 오컴의 면도날

어떤 현상의 인과관계를 설명할 때 불필요한 가정을 삼가야 한다         
가능하면 학습 결과를 간단한 형태로 표현하는 것이 좋다

## 기계 학습의 종류

### 지도 학습

> 입력(문제)-출력(답)의 데이터들로부터 새로운 입력에 대한 출력을 결정할 수 있는 패턴 추출

답이 있으니까 정답지를 가지고 채점

### 비지도 학습

> 출력(답)에 대한 정보가 없는 데이터로부터 패턴 추출

답이 없기에 비교하여 채점

### 강화 학습

> 출력에 대한 정확한 정보는 제공하지 않지만,              
> <span> <red>평가정보(reward)</red>는 주어지는 문제에 대해 각 상태에서의 행동을 결정 </span>               
> <span> <red>기대보상(expected reward)</red>이 최대가 되는 정책을 찾는 학습 </span>                


# 지도학습

> 주어진 입출력이 모두 있는 데이터를 학습 데이터로 이용          
> 새로운 입력이 있을 때, 결과를 결정할 수 있도록 하는 방법을 찾아내는 것

## 분류

> 출력이 정해진 부류(class, category) 중의 하나로 결정            

학습 데이터를 잘 분류할 수 있는 함수를 찾는 것            
수학적 함수일 수도 있고, 규칙일 수도 있음        

### 분류기 학습 알고리즘

학습에 사용되지 않은 데이터에 대해서 분류를 잘하는 것이 목표            
일반화 능력이 좋아야 함           

- 결정트리 알고리즘
- KNN(K-근접이웃) 알고리즘
- 다층 퍼셉트론 신경망
- 딥러닝 신경망
- SVM, 서포트 벡터 머신
- AdaBoost, 에이다부스트
- 랜덤 포레스트
- 확률 그래프 모델


### 데이터의 구분

- 학습 데이터
    - 분류기를 학습하는데 사용하는 데이터 집합
    - 학습 데이터가 많을수록 유리
- 테스트 데이터
    - 학습된 모델의 성능을 평가하는데 사용하는 데이터 집합
    - 학습에 사용되지 않은 데이터
    - 출력에 대한 정보 O
- 검증 데이터
    - 학습 과정에서 학습을 중단할 시점을 결정하기 위해 사용하는 데이터 집합

### 과적합, Overfitting

> 학습 데이터에 대해서 지나치게 잘 학습된 상태
            
데이터는 오류나 잡음을 포함할 가능성이 크기 때문에,                         
학습 데이터에 대해 매우 높은 성능을 보이더라도                 
학습되지 않은 데이터에 대해 좋지 않은 성능을 보일 수 있음         

오히려 너무 학습데이터에 맞춰지면 전혀 다른 결과를 보일 수 있음

#### 회피 방법

학습 과정에서 별도의 검증데이터에 대한 성능 평가             
검증 데이터에 대한 오류가 감소하다가 증가하는 시점에 학습 중단          

![image](https://user-images.githubusercontent.com/32366711/137532597-344b605c-8327-4456-8494-50cc3f172b61.png){:width="500"}{:.aligncenter}


### 부적합, Underfitting

> 학습 데이터를 충분히 학습하지 않은 상태

### 분류기 성능 평가

#### 정확도, Accuracy

= 옳게 분류한 데이터 개수 / 전체 데이터 개수

얼마나 정확하게 분류하는가

테스트 데이터에 대한 정확도를 분류기의 정확도로 사용

학습 데이터와 테스트 데이터는 겹치지 않도록 해야함

#### 이진 분류기 성능 평가

[ROC-Curve](https://aristohyun.github.io/blog/2021/07/02/ROC-Curve/){: target="_ blank"}

|실제\\추측 | True | False |
|:---:|:---:|:---:|
|Positive|True Positive| <red>False Negative</red><br/><blue>Type II Error</blue>|   
|Negative|<red> False Positive</red><br /><blue> Type I Error </blue>|True Negative|    

@
\text{민감도} = \frac{TP}{TP+FN} \\\ 
\text{특이도} = \frac{TN}{FP+TN} \\\ 
\text{정밀도} = \frac{TP}{TP + FP} \\\ 
\text{음성 예측도} = \frac{TN}{TN + FN} \\\ 
\text{위 양성률} = \frac{FP}{FP + TN} \\\ 
\text{위 발견율} = \frac{FP}{TP + FP} \\\ 
\text{정확도} = \frac{TP + TN}{TP + FP + TN + FN} \\\ 
\text{F1 측도} = 2\frac{\text{정밀도} * \text{민감도}}{\text{정밀도} + \text{민감도}}
@

#### ROC 곡선

> 부류 판정 임계값에 따른 그래프

- AUC
    - ROC 곡선에서 곡선 아래 부분의 면적
    - 0.5 ~ 1


### 데이터 부족 문제

#### K-fold Cross Validation

학습 데이터를 호율적으로 사용 가능

> 전체 데이터를 K등분 하여, 각 등분을 한번씩 테스트 데이터로 사용하여 성능 평가

![image](https://user-images.githubusercontent.com/32366711/137532901-ab7a4f80-b894-46cf-8cf3-a5067f66649b.png){:width="400"}{:.aligncenter}

### 데이터 불균형 문제

> Imbalanced Class Data            
> 특정 부류에 속하는 학습 데이터의 개수가 다른 부류에 비하여 지나치게 많은 경우

정확도에 의 한 성능 평가는 무의미할 수 있음

- $$<red>가중치</red>를 고려한 정확도 척도 사용               
- 많은 학습 데이터를 갖는 부류에서 <red>재 표본 추출</red>             
- 적은 학습데이터를 갖는 부류에 대해 <red>인공적인 데이터 생성</red>             


#### SMOTE

Synthetic Minority Over-Sampling Technique

빈도가 낮은 부류의 학습 데이터를 인공적으로 만들어내는 방법

1. 임의로 낮은 빈도 부류의 학습 데이터 x 선택
2. x의 KNN이 같은 부류의 데이터 선택
3. KNN중에 무작위로 데이터 y 선택
4. x와 y를 연결하는 직선상의 무작위 위치에 새로운 데이터 생성

![image](https://user-images.githubusercontent.com/32366711/137533370-112e987a-9a4f-44ef-9575-55290250c525.png){:width="400"}{:.aligncenter}


## [회귀](https://aristohyun.github.io/blog/2021/06/25/Linear-Regression/){:target="_ blank"}

> 출력이 연속인 영역의 값 결정

기계학습의 회귀는
학습데이터를 가지고 연속인구간의 값이 나오도록 찾는것

이런 데이터를 가장 잘 근사할 수 있는 함수를 찾는 것

`즉, 미지의 데이터에 대한 값을 추측하는 것`

#### 오차

우리가 수집한 데이터는 항상 오차가 존재함

정규분포를 사용하는 이유가 그런 관점

$\text{(예측값 - 실제값)}^2$의 평균

@
E = \frac{1}{n} \sum \limits_{i=1}^{n} (y_ i - f(x_ i))^2
@

모델의 종류(함수의 종류)에 영향을 받음

### 경사하강법, Gradient descent method

> 오차 함수의 그레디언트 반대 방향으로 조금씩 움직여 가며 최적의 파라미터를 찾으려는 방법          
> 학습 데이터에 부합되는 출력값이 되도록 파라미터를 변경하는 일

@
E = \frac{1}{n} \sum \limits_{i=1}^{n} (y_ i - f(x_ i))^2 \\\ 
f(x) = ax + b \\\ 
\triangledown  E = (\frac{\partial E}{\partial a}, \frac{\partial E}{\partial b}) \\\ 
@
<br/>

각 파라미터별로 그레디언트를 계산하여 조정함

@
a \leftarrow a - \eta \frac{\partial E}{\partial a} \\\ 
b \leftarrow b - \eta \frac{\partial E}{\partial b}
@

### 과적합

> 지나치게 복잡한 모델 사용

모델의 복잡도를 성능 평가에 반영함(패널티)으로써 대응할 수 있음

### 부적합

> 지나치게 단순한 모델 사용

### 로지스틱 회귀

[로지스틱](https://aristohyun.github.io/blog/2021/07/01/Logistic-Regression/){: target="_ blank"}

> 주로 이진분류로 사용하는 회귀 모델

이진 분류는 0과 1의 값을 가져야 하는데,           
회귀모델은 $-\infty ~\infty $의 값을 가짐

따라서 로지스틱 함수를 이용하여 값의 범위를 바꿈

@
f(x) = \frac{1}{1 + e^{-\theta^T x}}
@

#### 가능도, likelihood

모델이 학습 데이터를 생성할 가능성

$
P(X) = \prod \limits_ {i=1}^{N} f(x_ i)^{y_ i}(1 - f(x_ i))^{1-y_ i} \\\ 
Log P = -\frac{1}{N} log P(X) = -\frac{1}{N} log P(X) = -\frac{1}{N} \sum \limits_ {i=1}^{N} (y_ i log f(x_ i) + (1-f(x_ i) log(1-y_ i))
$


### 오차의 편향과 분산 분해

오차의 기대값, $ E(Error) = \text{편향}^2 + \text{분산}$

편향 : 측정값과 예측값들의 평균과의 차이           
분산 : 예측값들의 분산                 


@
\begin{align\*}
E(Error) &= E\[ (f(x) - y)^2 \] \\\ 
 &= E\[ (f(x) -E\[f(x)\] +E\[f(x)\] - y)^2 \] \\\ 
 &= E\[ (E\[f(x)\]-y)^2 + 2(f(x) - E\[f(x)\])(E\[f(x)\] - y) + (f(x) - E\[f(x)\])^2 \] \\\ 
 &= E\[ (E\[f(x)\]-y)^2 \] + E\[ 2(f(x) - E\[f(x)\])(E\[f(x)\] - y)\] + E\[(f(x) - E\[f(x)\])^2 \] \\\ 
 &= (E\[f(x)\]-y)^2 + 2(E\[f(x)\] -y)E\[f(x) -E\[f(x)\] \]) + E\[(f(x) - E\[f(x)\])^2 \] \\\ 
 &= (E\[f(x)\]-y)^2 + E\[(f(x) -E\[f(x)\])^2 \] \\\ 
 &= bias^2 + variance
\end{align\*}
@

편향과 분산은 회귀 함수의 형태에 영향을 받음

#### 단순한 모델

- 큰 편향, 작은 분산
- 실제 데이터와 회귀 함수간 차이가 크며, 학습된 회귀 함수들 간의 차이는 적음

#### 복잡한 모델

- 작은 편향, 큰 분산
- 실제 데이터와 회귀 함수간 차이는 작고, 학습된 회귀 함수들 간의 차이는 큼

#### 트레이드 오프

편향은
- 단순한 모델로 잘못 가정할 때 크게 발생
- 부적합 문제 초래

분산은
- 복잡한 모델 사용에 따라 학습 데이터의 잡음으로 발생
- 과적합 문제 초래

![image](https://user-images.githubusercontent.com/32366711/137538571-e23c6769-9383-40c4-b527-fce0d147ed97.png){:.aligncenter}{:width="400"}


## 추천

> 개인별로 맞춤형 정보를 제공하려는 기술            
> 맞춤형 정보를 제공하여, 정보 검색의 부하를 줄여주는 역할

#### 희소 행렬

비어있는 부분을 채우는 것이 추천

![image](https://user-images.githubusercontent.com/32366711/137538761-29a4fe54-c9a8-4f42-b227-5355fba2792f.png){:.aligncenter}{:width="500"}


### 추천 기법

#### 내용 기반 추천

- 고객이 이전에 높게 평가했던 것과 유사한 내용을 갖는 대상을 추천
- 태그 및 카테고리 활용

#### 협력 필터링

- 사용자간 협력 필터링
    - 유사한 사용자간 집합을 이루어, 집합간 영화 추천
    - 유사한 사용자 A가 1번 영화를 좋아했다면 사용자 B도 좋아할 것
- 항목간 협력 필터링
    - 항목간의 유사도를 구하여 유사 항목 선택

#### 은닉 요소 모델

- 행렬 분해, 특이값 분해에 기반한 방법

![image](https://user-images.githubusercontent.com/32366711/137539080-2f9a569b-3834-4f42-9640-619f05b9e914.png){:.aligncenter}{:width="500"}


## 결정트리 학습

> 트리 형태로 의사결정 지식을 표현한 것
> 기본적으로 분류, 회귀도 가능

- 내부 노드 : 비교 속성
- 간선 : 속성 값
- 단말노드 : 클래스, 대표값

알고리즘을 세우면 다음과 같은 표를 트리로 만들 수 있다

이렇게 트리를 그리면, 추론 또한 가능

![image](https://user-images.githubusercontent.com/32366711/137652738-99209cd8-b5a8-429e-9408-0aec97adfcf1.png)


### 결정트리 학습 알고리즘

> 모든 데이터를 포함한 하나의 노드로 구성된 트리에서 시작
> 그리디 알고리즘[^greedy], 현재 가장 좋아보이는 것으로 해를 구함

반복적인 노드 분할 과정
1. 분할 속성 선택
2. 속성값에 따라 서브트리 생성
3. 데이터를 속성값에 따라 분배

#### 분할 속성 결정

분할의 결과가 가능하면 동질(pure)적인 것으로 만드는게 좋다

정량적인 방법으로 얼마나 순수한가를 평가할 수 있어야 함

**엔트로피**

> 정보이론에서 사용되는 개념  
> 정보의 양이 얼마인가
> 동질적인 정도를 측정하는 척도          

정보는 일어날 일이 낮은 사건일 수록 유용
==> 확률이 낮을수록 큰 값을 가지는 척도를 사용

@
I = - \sum \limits_ {c}^{} p(c)log_ 2 p(c) 
@
-log P 정보 양
E\[f(x)\] = \sum p(x)f(x)
$p(c)$ : 부류 c에 속하는 것의 비율


50%이면 최대값 == 잘섞여있음

#### 정보 이득

@
IG = I - I_{res}
@

- I: A라는 속성, 노드의 엔트로피를 구함
- Ires : 각각의 엔트로피를 또 구함
- IG : 각각의 비율로 가중치로해서 더해줌

가중평균이 클수록 엔트로피값이 낮아짐

정보이득이 클수록 우수한 분할 속성


#### 계산

엔트로피를 측정하게 되면 단위가 있는데, bits

각각의 확률에 대하여 엔트로피를 모두 계산해서 더해줌 I

패턴을 가지고 나누게 되면, 각각의 노드가 생김
각각에 대해서 부모 노드의 속성으로 엔트로피를 계산해서 I를 구해줌

패턴이후에 나눴을 때 엔트로피 I는 Ires

I - Ires = IG pattern

이걸 모든 가능한 경우를 계산해줌

패턴, 아웃라인, 닷(부류가 아닌 속성만)

그러면 가장 IG값이 큰것을 선택

그상태에서 또 반복

부류를 찾고싶은 것

#### Example

<img width="107" alt="image" src="https://user-images.githubusercontent.com/32366711/145703846-fab8686e-74c1-4e95-916b-df2ed3068f24.png">

□ 가 선택될 확률 : $\frac{9}{14}$

△ 가 선택될 확률 : $\frac{5}{14}$

$ I = -\sum \limits_ {c}{} p(c) log_ 2 p(c) $
$
\begin{align\* }
I &= -p(\square)log_ 2 p(\square) - \p(\bigtriangleup)log_ 2 p(\bigtriangleup)  \\\ 
 &= -\frac{9}{14}log_ 2 \frac{9}{14} - \frac{5}{14}log_ 2 \frac{5}{14} \\\ 
 &= 0.940
\end{align\* }
$

**Pattern 기준 분할** 

![image](https://user-images.githubusercontent.com/32366711/145704881-27eb8e08-8ddb-4541-93e7-90adc43bdd95.png)

$
I_ {horizontal} = - \frac{2}{5}log_ 2 \frac{2}{5} - \frac{3}{5}log_ 2 \frac{3}{5} = 0.971
$

$
I_ {diagonal} = - \frac{0}{4}log_ 2 \frac{0}{4} - \frac{4}{4}log_ 2 \frac{4}{4} = 0
$

$
I_ {vertical} = - \frac{3}{5}log_ 2 \frac{3}{5} - \frac{2}{5}log_ 2 \frac{2}{5} = 0.971
$

$
I_ {res} (Pattern) = \sum p(v)I(v) = \frac{5}{14} * 0.971 + \frac{4}{14} * 0 + \frac{5}{14} * 0.971 = 0.694
$

$
IG(Pattern) = I - I_ {res}(Pattern) = 0.94 - 0.694 = 0.246
$

이렇게 속성별 IG(정보이득)를 구하여 가장 큰 것을 선택  

#### 단점

속성값이 많을 수록 선호
이름 등

개선 척도
- 정보이득비
- 지니지수

#### 정보이득비 척도

@
GainRatio(A) = \frac{IG(A)}{I(A)} = \frac{I - I_ {res}(A)}{I(A)}
@

$
I(A) = -\sum \limits_ {v}{} p(v) log_ 2 p(v)
$

I(A)로 나눠줌
속성값이 많으면 패널티를 줄 수 있음

위의 예에서, 기존 I는 모양을 구분하기 위함이였지만
I(A), 전체에서 패턴에 대한 값을 구하는 것

#### 지니 지수, Gini index

@
Gini = \sum \limits_ {i \neq j}{} p(i)p(j)
@

$
Gini(A) = \sum \lmits_ {v}{} p(v) \sum \lmits_ {i \neq j}{} p(i|v)p(j|v)
$

**지니 지수 이득**

$
GiniGain(A) = Gini - Gini(A)
$

A의 v일때 i, j일 확률(ex. 패턴의 수평일 때, 삼각형, 네모일 확률) 

$
Gini(Pattern) = \frac{5}{14} * (\frac{3}{5} * \frac{2}{5}) + \frac{4}{14} * (\frac{0}{4} * \frac{4}{4}) + \frac{5}{14} * (\frac{2}{5} * \frac{3}{5}) = 0.171
$
$
GiniGain(Pattern) = 0.23 - 0.171 = 0.058
$

[^greedy]: 최적해를 구하는 데에 사용되는 근사적인 방법으로, 여러 경우 중 하나를 결정해야 할 때마다 그 순간에 최적이라고 생각 되는것을 선택해 나가는 방식이다.


### 결정트리 알고리즘

분할 속성을 선택하는 방식이 중요
-> 그리디 알고리즘 사용

- ID3 알고리즘
  - 속성이 범주형 값일 때 사용하는 결정트리 학슴
- C4.5 알고리즘
  - 범주형 + 수치형속성을 갖는 데이터
  - 수치형 : 기준값을 주고 크다작다, 혹은 구간별로 10kg, 150~160cm
  - 값을 정렬해서 속성값을 고려해서 나눔
- C5.0 알고리즘
- CART 알고리즘
  - 수치형 속성을 갖는 데이터에 대해 적용

### 결정트리를 이용한 회귀

> 단말노드로 가는 방법이 중요
> 값을 어떻게 나눌 것인가

평균 + 표준편차(분산) 

@
SDR(A) = SD - SD(A)
@

SD : 나누기 전 표준편차[^SD]
SD(A) : 나눈 후 각각의 표준편차의 가중 평균

표준편차 SDR(A)가 작을수록 쓰겠다

<img width="286" alt="image" src="https://user-images.githubusercontent.com/32366711/145705485-86cb270e-9cc8-4df2-965b-64fce2a63dac.png">


[^SD]: $\sqrt{\frac{1}{N} \sum \limits_ {i=1}{N} (x_ i - m)^2 }$

## 앙상블 학습

`무작위로 선택된 많은 사람의 답변을 모은 것이 전문가의 답보다 낫다`

> 여러개의 모델을 만들어서 하나로 사용하여 성능을 높이겠다               


![image](https://user-images.githubusercontent.com/32366711/137660166-ff65af8f-50c6-4bbe-b0b5-0446f14e79a9.png)

### 붓스트랩, bootstrap

> 주어진 학습 데이터 집합에서 복원추출하여 다수의 학습 데이터 집합을 만들어 내는 기법

같은 데이터로 같은 알고리즘으로 학습 시키면 같은 모델이 만들어질때가 있음

다른종류의 데이터가 필요하다

복원추출하여 학습데이터 집합을 여러개 만듬


## 배깅 알고리즘

> bagging, bootstrap aggregating

- 부트스트랩을 통해서 여러개의 학습 데이터를 만듬
- 각각의 학습 데이터를 통해 학습을 시킴
- 이를 바탕으로 가중치 투표

<img width="417" alt="image" src="https://user-images.githubusercontent.com/32366711/145705695-959d0cf5-106a-4904-84e4-445129f9ec8f.png">


#### 랜덤 포레스트

> 분류기로 결정트리를 사용하는 배깅 기법

Random 무자위로 선택한 속성중에서 분할 속성을 선택
Forest 여러 결정트리로 구성

트리를 여러개 만드는데 모든 가능한 분할 속성을선택하는게 아니라
랜덤으로 속성을 선택하여 만든다

그러면 서로다른 모양의 트리가 여러개가 나온다

#### 배깅에 의한 회귀

- 다수의 학습데이터 집합을 생성하여
- 각 학습 데이터별로 회귀모델 생성
- 각 회귀모델의 평균 값으로 최종 회귀 생성

평균적으로 빨강색이 나온다
하나를 선택해서 나온것보단 더 좋은 성능을 보일 것

<img width="270" alt="image" src="https://user-images.githubusercontent.com/32366711/145705715-41371979-d315-4b34-a9ca-f2c2436051b5.png">


## 부스팅 알고리즘

예측 모델을 순치적으로 만들어 가는 앙상블 모델 생성

오차에 따라 학습데이터에 가중치 또는 값을 변경해가면서 예측 모델 생성

가중치 값을 키우거나 줄여서 값을 추정한다

![image](https://user-images.githubusercontent.com/32366711/137661400-3694c3b8-2322-4d37-8f84-cc97d80d2741.png)

#### AdaBoost

모든 가중치가 처음엔 1/n

학습 오류율을 계산을 하는데, 0.5 미만이면 50% 이상 이여만 허용

잘못 판정한 학습 데이터의 가중치는 증가

제대로 판정한 학습 데이터의 가중치는 축소

누적할수록 정확도가 높아짐










